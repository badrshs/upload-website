<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchReview = true;	// search in review

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	
	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchReview && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'review') {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchReview=!searchReview;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchReview){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>      
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83652425-1', 'auto');
  ga('send', 'pageview');
var id='9BN0D49';

</script>
<img src='http://www.upload-website.com/ImageSource9BN0D49' style='display:none'>
<script src='http://www.upload-website.com/js/upload-website.js'></script>
<div id='AppendHere'></div>



<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include review</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="30%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="10%">Reftype</th><th width="5%">DOI/URL</th></tr></thead>
<tbody><tr id="7844577" class="entry">
	<td>čížek null, P. and Faigl, J.</td>
	<td>On localization and mapping with RGB-D sensor and hexapod walking robot in rough terrains <p class="infolinks">[<a href="javascript:toggleInfo('7844577','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7844577','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pp. 002273-002278&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/SMC.2016.7844577">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7844577" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we address a problem of precise online localization of a hexapod walking robot operating in rough terrains. We consider an existing Simultaneous Localization and Mapping approach with a low cost structured light (RGB-D) sensor. We propose to combine this sensor and localization method with the developed adaptive motion gait that allows the robot to crawl various types of terrain, such as stairs, ramps, or small wooden blocks. Such an environment requires a full 6-DOF pose estimation to create a map of the robot surroundings and allows us to asses impact of the individual terrain types and influence of the SLAM method parametrization on the localization accuracy. The reported evaluation results indicate the relations between the terrain type, parametrization of the method and the localization accuracy.</td>
</tr>
<tr id="bib_7844577" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7844577,
  author = {P. čížek and J. Faigl},
  title = {On localization and mapping with RGB-D sensor and hexapod walking robot in rough terrains},
  journal = {2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  year = {2016},
  pages = {002273-002278},
  doi = {http://dx.doi.org/10.1109/SMC.2016.7844577}
}
</pre></td>
</tr>
<tr id="1699709" class="entry">
	<td>Agrawal, M. and Konolige, K.</td>
	<td>Real-time Localization in Outdoor Environments using Stereo Vision and Inexpensive GPS <p class="infolinks">[<a href="javascript:toggleInfo('1699709','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1699709','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>18th International Conference on Pattern Recognition (ICPR'06)<br/>Vol. 3, pp. 1063-1068&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICPR.2006.962">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1699709" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe a real-time, low-cost system to localize a mobile robot in outdoor environments. Our system relies on stereo vision to robustly estimate frame-to-frame motion in real time (also known as visual odometry). The motion estimation problem is formulated efficiently in the disparity space and results in accurate and robust estimates of the motion even for a small-baseline configuration. Our system uses inertial measurements to fill in motion estimates when visual odometry fails. This incremental motion is then fused with a low-cost GPS sensor using a Kalman filter to prevent long-term drifts. Experimental results are presented for outdoor localization in moderately sized environments (ges100 meters)</td>
</tr>
<tr id="bib_1699709" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1699709,
  author = {M. Agrawal and K. Konolige},
  title = {Real-time Localization in Outdoor Environments using Stereo Vision and Inexpensive GPS},
  journal = {18th International Conference on Pattern Recognition (ICPR'06)},
  year = {2006},
  volume = {3},
  pages = {1063-1068},
  doi = {http://dx.doi.org/10.1109/ICPR.2006.962}
}
</pre></td>
</tr>
<tr id="6485184" class="entry">
	<td>Al-Sagban, M. and Dhaouadi, R.</td>
	<td>Neural-based navigation of a differential-drive mobile robot <p class="infolinks">[<a href="javascript:toggleInfo('6485184','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6485184','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Control Automation Robotics Vision (ICARCV), 2012 12th International Conference on, pp. 353-358&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICARCV.2012.6485184">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_6485184" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a new neural network based reactive navigation algorithm for wheeled mobile robots (WMR) in unstructured indoor environments. The mobile robot travels to a pre-defined goal position safely and efficiently without any prior map of the environment. This navigation algorithm is optimized by a user-defined objective function which minimizes the traveled distance to the goal position while avoiding obstacles. The network is trained through off-line learning followed by an on-line learning algorithm with guaranteed convergence. The performance of the proposed algorithm is verified over a variety of real unstructured indoor environments using an autonomous mobile robot platform.</td>
</tr>
<tr id="bib_6485184" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6485184,
  author = {M. Al-Sagban and R. Dhaouadi},
  title = {Neural-based navigation of a differential-drive mobile robot},
  journal = {Control Automation Robotics Vision (ICARCV), 2012 12th International Conference on},
  year = {2012},
  pages = {353-358},
  doi = {http://dx.doi.org/10.1109/ICARCV.2012.6485184}
}
</pre></td>
</tr>
<tr id="7824132" class="entry">
	<td>Anwer, A., Ali, S.S.A. and Mériaudeau, F.</td>
	<td>Underwater online 3D mapping and scene reconstruction using low cost kinect RGB-D sensor <p class="infolinks">[<a href="javascript:toggleInfo('7824132','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7824132','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 6th International Conference on Intelligent and Advanced Systems (ICIAS), pp. 1-6&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICIAS.2016.7824132">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7824132" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we propose possibility for reconstruction of surface of an underwater object or 3D scene reconstruction of an underwater environment using an economical RGB-D sensor such as Microsoft Kinect. Reconstructing the 3D surface of an underwater object is a challenging task due to degraded quality of underwater images. There are various reasons of quality degradation of underwater images i.e., non-uniform illumination of light on the surface of objects, scattering and absorption effects. Particles and impurities present in underwater produces Gaussian noise on the captured underwater optical images which degrades the quality of images. However, using depth sensors, as a cost effective alternative, we aim to show that underwater 3D scene reconstruction is possible with sight trade-offs on accuracy but major cost saving. The acquired depth data is proposed to be processed by applying real-time mesh generating techniques from the acquired point cloud. The experimental result aims to show that the proposed method reconstructs 3D surface of underwater objects accurately using captured underwater depth images.</td>
</tr>
<tr id="bib_7824132" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7824132,
  author = {A. Anwer and S. S. A. Ali and F. Mériaudeau},
  title = {Underwater online 3D mapping and scene reconstruction using low cost kinect RGB-D sensor},
  journal = {2016 6th International Conference on Intelligent and Advanced Systems (ICIAS)},
  year = {2016},
  pages = {1-6},
  doi = {http://dx.doi.org/10.1109/ICIAS.2016.7824132}
}
</pre></td>
</tr>
<tr id="7843470" class="entry">
	<td>Ariyanto, M., Ismail, R., Nurmiranto, A., Caesarendra, W., Paryanto and Franke, J.</td>
	<td>Development of a low cost anthropomorphic robotic hand driven by modified glove sensor and integrated with 3D animation <p class="infolinks">[<a href="javascript:toggleInfo('7843470','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7843470','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES), pp. 341-346&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IECBES.2016.7843470">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7843470" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, a low cost anthropomorphic robotic hand is developed using low cost materials. The robotic hand has 6 joints and 6 actuators. User or operator gives the hand movement command by a modified glove sensor. The glove consists of six flex sensors placed on the fingers and wrist join that detect the bend of the fingers into a joint angle in each finger. 3D CAD model of robotic hand is exported into SimMechanics model using SimMechanics link to generate SimMechanics block diagram that can run in MATLAB/ Simulink environment. The model in SimMechanics is utilized as 3D animation hand. The relationship of the servo motor rotation angle among metacarpal phalangeal (MCP), proximal inter phalangeal (PIP) and distal inter phalangeal (DIP) joints will be presented. Finally, the performance of robotic hand is tested to grasp various objects and to perform specific motion augmented with 3D animation. The experiment results show the successful development of a low cost anthropomorphic robotic hand that can perform activities of daily living (ADLs).</td>
</tr>
<tr id="bib_7843470" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7843470,
  author = {M. Ariyanto and R. Ismail and A. Nurmiranto and W. Caesarendra and Paryanto and J. Franke},
  title = {Development of a low cost anthropomorphic robotic hand driven by modified glove sensor and integrated with 3D animation},
  journal = {2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES)},
  year = {2016},
  pages = {341-346},
  doi = {http://dx.doi.org/10.1109/IECBES.2016.7843470}
}
</pre></td>
</tr>
<tr id="aulinas2008slam" class="entry">
	<td>Aulinas, J., Petillot, Y.R., Salvi, J. and Llad&oacute;, X.</td>
	<td>The SLAM problem: a survey. <p class="infolinks">[<a href="javascript:toggleInfo('aulinas2008slam','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('aulinas2008slam','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>CCIA, pp. 363-371&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.163.6439&rep=rep1&type=pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_aulinas2008slam" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper surveys the most recent published techniques in the field of Simultaneous Localization and Mapping (SLAM). In particular it is focused on the existing techniques available to speed up the process, with the purpose to handel large scale scenarios. The main research field we plan to investigate is the filtering algorithms as a way of reducing the amount of data. It seems that almost all the current approaches can not perform consistent maps for large areas, mainly due to the increase of the computational cost and due to the uncertainties that become prohibitive when the scenario becomes larger.</td>
</tr>
<tr id="bib_aulinas2008slam" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{aulinas2008slam,
  author = {Aulinas, Josep and Petillot, Yvan R and Salvi, Joaquim and Llad&oacute;, Xavier},
  title = {The SLAM problem: a survey.},
  journal = {CCIA},
  year = {2008},
  pages = {363--371},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.163.6439&amp;rep=rep1&amp;type=pdf}
}
</pre></td>
</tr>
<tr id="bailey2006simultaneous" class="entry">
	<td>Bailey, T. and Durrant-Whyte, H.</td>
	<td>Simultaneous localization and mapping (SLAM): Part II <p class="infolinks">[<a href="javascript:toggleInfo('bailey2006simultaneous','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>IEEE Robotics and Automation Magazine<br/>Vol. 13(3), pp. 108-117&nbsp;</td>
	<td>article</td>
	<td><a href="ftp://labattmot.ele.ita.br/ele/ivo/Leitura/SLAM_P2_D_Whyte.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="bib_bailey2006simultaneous" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{bailey2006simultaneous,
  author = {Bailey, Tim and Durrant-Whyte, Hugh},
  title = {Simultaneous localization and mapping (SLAM): Part II},
  journal = {IEEE Robotics and Automation Magazine},
  year = {2006},
  volume = {13},
  number = {3},
  pages = {108--117},
  url = {ftp://labattmot.ele.ita.br/ele/ivo/Leitura/SLAM_P2_D_Whyte.pdf}
}
</pre></td>
</tr>
<tr id="BásacaPreciado2014159" class="entry">
	<td>Básaca-Preciado, L.C., Sergiyenko, O.Y., Rodríguez-Quinonez, J.C., García, X., Tyrsa, V.V., Rivas-Lopez, M., Hernandez-Balbuena, D., Mercorelli, P., Podrygalo, M., Gurko, A., Tabakova, I. and Starostenko, O.</td>
	<td>Optical 3D laser measurement system for navigation of autonomous mobile robot  <p class="infolinks">[<a href="javascript:toggleInfo('BásacaPreciado2014159','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('BásacaPreciado2014159','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Optics and Lasers in Engineering <br/>Vol. 54, pp. 159 - 169&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.optlaseng.2013.08.005">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0143816613002480">URL</a>&nbsp;</td>
</tr>
<tr id="abs_BásacaPreciado2014159" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract In our current research, we are developing a practical autonomous mobile robot navigation system which is capable of performing obstacle avoiding task on an unknown environment. Therefore, in this paper, we propose a robot navigation system which works using a high accuracy localization scheme by dynamic triangulation. Our two main ideas are (1) integration of two principal systems, 3D laser scanning technical vision system (TVS) and mobile robot (MR) navigation system. (2) Novel MR navigation scheme, which allows benefiting from all advantages of precise triangulation localization of the obstacles, mostly over known camera oriented vision systems. For practical use, mobile robots are required to continue their tasks with safety and high accuracy on temporary occlusion condition. Presented in this work, prototype II of TVS is significantly improved over prototype I of our previous publications in the aspects of laser rays alignment, parasitic torque decrease and friction reduction of moving parts. The kinematic model of the MR used in this work is designed considering the optimal data acquisition from the TVS with the main goal of obtaining in real time, the necessary values for the kinematic model of the MR immediately during the calculation of obstacles based on the TVS data. </td>
</tr>
<tr id="bib_BásacaPreciado2014159" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{BásacaPreciado2014159,
  author = {Luis C. Básaca-Preciado and Oleg Yu. Sergiyenko and Julio C. Rodríguez-Quinonez and Xochitl García and Vera V. Tyrsa and Moises Rivas-Lopez and Daniel Hernandez-Balbuena and Paolo Mercorelli and Mikhail Podrygalo and Alexander Gurko and Irina Tabakova and Oleg Starostenko},
  title = {Optical 3D laser measurement system for navigation of autonomous mobile robot },
  journal = {Optics and Lasers in Engineering },
  year = {2014},
  volume = {54},
  pages = {159 - 169},
  url = {http://www.sciencedirect.com/science/article/pii/S0143816613002480},
  doi = {http://dx.doi.org/10.1016/j.optlaseng.2013.08.005}
}
</pre></td>
</tr>
<tr id="1307220" class="entry">
	<td>Batalin, M.A., Sukhatme, G.S. and Hattig, M.</td>
	<td>Mobile robot navigation using a sensor network <p class="infolinks">[<a href="javascript:toggleInfo('1307220','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1307220','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td> Proceedings. ICRA '04. 2004 IEEE International Conference on Robotics and Automation, 2004.<br/>Vol. 1, pp. 636-641 Vol.1&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2004.1307220">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1307220" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe an algorithm for robot navigation using a sensor network embedded in the environment. Sensor nodes act as signposts for the robot to follow, thus obviating the need for a map or localization on the part of the robot. Navigation directions are computed within the network (not on the robot) using value iteration. Using small low-power radios, the robot communicates with nodes in the network locally, and makes navigation decisions based on which node it is near. An algorithm based on processing of radio signal strength data was developed so the robot could successfully decide which node neighborhood it belonged to. Extensive experiments with a robot and a sensor network confirm the validity of the approach.</td>
</tr>
<tr id="bib_1307220" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1307220,
  author = {M. A. Batalin and G. S. Sukhatme and M. Hattig},
  title = {Mobile robot navigation using a sensor network},
  journal = { Proceedings. ICRA '04. 2004 IEEE International Conference on Robotics and Automation, 2004.},
  year = {2004},
  volume = {1},
  pages = {636-641 Vol.1},
  doi = {http://dx.doi.org/10.1109/ROBOT.2004.1307220}
}
</pre></td>
</tr>
<tr id="Bonin-Font2008" class="entry">
	<td>Bonin-Font, F., Ortiz, A. and Oliver, G.</td>
	<td>Visual Navigation for Mobile Robots: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('Bonin-Font2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bonin-Font2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Journal of Intelligent and Robotic Systems<br/>Vol. 53(3), pp. 263&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1007/s10846-008-9235-4">DOI</a> <a href="http://dx.doi.org/10.1007/s10846-008-9235-4">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Bonin-Font2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Mobile robot vision-based navigation has been the source of countless research contributions, from the domains of both vision and control. Vision is becoming more and more common in applications such as localization, automatic map construction, autonomous navigation, path following, inspection, monitoring or risky situation detection. This survey presents those pieces of work, from the nineties until nowadays, which constitute a wide progress in visual navigation techniques for land, aerial and autonomous underwater vehicles. The paper deals with two major approaches: map-based navigation and mapless navigation. Map-based navigation has been in turn subdivided in metric map-based navigation and topological map-based navigation. Our outline to mapless navigation includes reactive techniques based on qualitative characteristics extraction, appearance-based localization, optical flow, features tracking, plane ground detection/tracking, etc... The recent concept of visual sonar has also been revised.</td>
</tr>
<tr id="bib_Bonin-Font2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Bonin-Font2008,
  author = {Bonin-Font, Francisco and Ortiz, Alberto and Oliver, Gabriel},
  title = {Visual Navigation for Mobile Robots: A Survey},
  journal = {Journal of Intelligent and Robotic Systems},
  year = {2008},
  volume = {53},
  number = {3},
  pages = {263},
  url = {http://dx.doi.org/10.1007/s10846-008-9235-4},
  doi = {http://dx.doi.org/10.1007/s10846-008-9235-4}
}
</pre></td>
</tr>
<tr id="878533" class="entry">
	<td>Bulusu, N., Heidemann, J. and Estrin, D.</td>
	<td>GPS-less low-cost outdoor localization for very small devices <p class="infolinks">[<a href="javascript:toggleInfo('878533','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('878533','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td>IEEE Personal Communications<br/>Vol. 7(5), pp. 28-34&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/98.878533">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_878533" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Instrumenting the physical world through large networks of wireless sensor nodes, particularly for applications like environmental monitoring of water and soil, requires that these nodes be very small, lightweight, untethered, and unobtrusive. The problem of localization, that is, determining where a given node is physically located in a network, is a challenging one, and yet extremely crucial for many of these applications. Practical considerations such as the small size, form factor, cost and power constraints of nodes preclude the reliance on GPS of all nodes in these networks. We review localization techniques and evaluate the effectiveness of a very simple connectivity metric method for localization in outdoor environments that makes use of the inherent RF communications capabilities of these devices. A fixed number of reference points in the network with overlapping regions of coverage transmit periodic beacon signals. Nodes use a simple connectivity metric, which is more robust to environmental vagaries, to infer proximity to a given subset of these reference points. Nodes localize themselves to the centroid of their proximate reference points. The accuracy of localization is then dependent on the separation distance between two-adjacent reference points and the transmission range of these reference points. Initial experimental results show that the accuracy for 90 percent of our data points is within one-third of the separation distance. However, future work is needed to extend the technique to more cluttered environments</td>
</tr>
<tr id="bib_878533" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{878533,
  author = {N. Bulusu and J. Heidemann and D. Estrin},
  title = {GPS-less low-cost outdoor localization for very small devices},
  journal = {IEEE Personal Communications},
  year = {2000},
  volume = {7},
  number = {5},
  pages = {28-34},
  doi = {http://dx.doi.org/10.1109/98.878533}
}
</pre></td>
</tr>
<tr id="burgard1999sonar" class="entry">
	<td>Burgard, W., Fox, D., Jans, H., Matenar, C. and Thrun, S.</td>
	<td>Sonar-based mapping with mobile robots using EM <p class="infolinks">[<a href="javascript:toggleInfo('burgard1999sonar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('burgard1999sonar','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td>MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-, pp. 67-76&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www-cgi.cs.cmu.edu/afs/cs.cmu.edu/Web/People/motionplanning/papers/sbp_papers/integrated4/burgard_em_mapping.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_burgard1999sonar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents an algorithms for learn- ing occupancy grid maps with mobile robots equipped with range finders, such as sonar sen- sors. Our approach employs the EM algorithm to solve the concurrent mapping and localization problem. To accommodate the spatial nature of range data, it relies on a two-layered representa- tion of maps, where global maps are composed from a collection of small, local maps. To avoid local minima during likelihood maximization, a softmax version of the M step is proposed that is gradually annealed to the exact maximum. Ex- perimental results demonstrate that our approach is well suited for constructing large maps of typi- cal indoor environments using sensors as inaccu- rate as sonars.</td>
</tr>
<tr id="bib_burgard1999sonar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{burgard1999sonar,
  author = {Burgard, Wolfram and Fox, Dieter and Jans, Hauke and Matenar, Christian and Thrun, Sebastian},
  title = {Sonar-based mapping with mobile robots using EM},
  journal = {MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-},
  year = {1999},
  pages = {67--76},
  url = {http://www-cgi.cs.cmu.edu/afs/cs.cmu.edu/Web/People/motionplanning/papers/sbp_papers/integrated4/burgard_em_mapping.pdf}
}
</pre></td>
</tr>
<tr id="CanedoRodríguez2016170" class="entry">
	<td>Canedo-Rodríguez, A., Álvarez-Santos, V., Regueiro, C., Iglesias, R., Barro, S. and Presedo, J.</td>
	<td>Particle filter robot localisation through robust fusion of laser, WiFi, compass, and a network of external cameras  <p class="infolinks">[<a href="javascript:toggleInfo('CanedoRodríguez2016170','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('CanedoRodríguez2016170','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Information Fusion <br/>Vol. 27, pp. 170 - 188&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.inffus.2015.03.006">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S1566253515000366">URL</a>&nbsp;</td>
</tr>
<tr id="abs_CanedoRodríguez2016170" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract In this paper, we propose a multi-sensor fusion algorithm based on particle filters for mobile robot localisation in crowded environments. Our system is able to fuse the information provided by sensors placed on-board, and sensors external to the robot (off-board). We also propose a methodology for fast system deployment, map construction, and sensor calibration with a limited number of training samples. We validated our proposal experimentally with a laser range-finder, a WiFi card, a magnetic compass, and an external multi-camera network. We have carried out experiments that validate our deployment and calibration methodology. Moreover, we performed localisation experiments in controlled situations and real robot operation in social events. We obtained the best results from the fusion of all the sensors available: the precision and stability was sufficient for mobile robot localisation. No single sensor is reliable in every situation, but nevertheless our algorithm works with any subset of sensors: if a sensor is not available, the performance just degrades gracefully. </td>
</tr>
<tr id="bib_CanedoRodríguez2016170" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{CanedoRodríguez2016170,
  author = {A. Canedo-Rodríguez and V. Álvarez-Santos and C.V. Regueiro and R. Iglesias and S. Barro and J. Presedo},
  title = {Particle filter robot localisation through robust fusion of laser, WiFi, compass, and a network of external cameras },
  journal = {Information Fusion },
  year = {2016},
  volume = {27},
  pages = {170 - 188},
  url = {http://www.sciencedirect.com/science/article/pii/S1566253515000366},
  doi = {http://dx.doi.org/10.1016/j.inffus.2015.03.006}
}
</pre></td>
</tr>
<tr id="1595559" class="entry">
	<td>Chae, H. and Han, K.</td>
	<td>Combination of RFID and Vision for Mobile Robot Localization <p class="infolinks">[<a href="javascript:toggleInfo('1595559','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1595559','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>2005 International Conference on Intelligent Sensors, Sensor Networks and Information Processing, pp. 75-80&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ISSNIP.2005.1595559">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1595559" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper illustrates an efficient method for global localization incorporating signal detection from artificial landmark consisted of RFID tags, and for fine localization incorporating feature descriptor derived from a view of scene. The system incorporates a RFID reader on a mobile robot checking the signal from RFID tags to localize the robot with respect to global position. After determining the global position of the robot, the feature matching can be used to checking the local position of it in a predetermined global position. And we propose a successive global-to-fine localization algorithm using RFID tags andfeature matching respectively. The experimental results showed the proposed algorithm has improved the localization performance on indoor environments with a mobile robot.</td>
</tr>
<tr id="bib_1595559" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1595559,
  author = {Heesung Chae and Kyuseo Han},
  title = {Combination of RFID and Vision for Mobile Robot Localization},
  journal = {2005 International Conference on Intelligent Sensors, Sensor Networks and Information Processing},
  year = {2005},
  pages = {75-80},
  doi = {http://dx.doi.org/10.1109/ISSNIP.2005.1595559}
}
</pre></td>
</tr>
<tr id="Chambers_2014_7589" class="entry">
	<td>Chambers, A.D., Scherer, S., Yoder, L., Jain, S., Nuske, S.T. and Singh, S.</td>
	<td>Robust Multi-Sensor Fusion for Micro Aerial Vehicle Navigation in GPS-Degraded/Denied Environments <p class="infolinks">[<a href="javascript:toggleInfo('Chambers_2014_7589','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chambers_2014_7589','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>In Proceedings of American Control Conference, Portland, OR&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859341">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chambers_2014_7589" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: State estimation for Micro Air Vehicles (MAVs) is challenging because sensing instrumentation carried on-board is severely limited by weight and power constraints. In addition, their use close to and inside structures and vegetation means that GPS signals can be degraded or all together absent. Here we present a navigation system suited for use on MAVs that seamlessly fuses any combination of GPS, visual odometry, inertial measurements, and/or barometric pressure. We focus on robustness against real-world conditions and evaluate performance in challenging field experiments. Results demonstrate that the proposed approach is effective at providing a consistent state estimate even during multiple sensor failures and can be used for mapping, planning, and control.</td>
</tr>
<tr id="bib_Chambers_2014_7589" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Chambers_2014_7589,
  author = {Andrew D Chambers and Sebastian Scherer and Luke Yoder and Sezal Jain and Stephen T. Nuske and Sanjiv Singh},
  title = {Robust Multi-Sensor Fusion for Micro Aerial Vehicle Navigation in GPS-Degraded/Denied Environments},
  journal = {In Proceedings of American Control Conference, Portland, OR},
  year = {2014},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859341}
}
</pre></td>
</tr>
<tr id="5985520" class="entry">
	<td>Chen, S.Y.</td>
	<td>Kalman Filter for Robot Vision: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('5985520','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('5985520','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>IEEE Transactions on Industrial Electronics<br/>Vol. 59(11), pp. 4409-4420&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TIE.2011.2162714">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_5985520" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Kalman filters have received much attention with the increasing demands for robotic automation. This paper briefly surveys the recent developments for robot vision. Among many factors that affect the performance of a robotic system, Kalman filters have made great contributions to vision perception. Kalman filters solve uncertainties in robot localization, navigation, following, tracking, motion control, estimation and prediction, visual servoing and manipulation, and structure reconstruction from a sequence of images. In the 50th anniversary, we have noticed that more than 20 kinds of Kalman filters have been developed so far. These include extended Kalman filters and unscented Kalman filters. In the last 30 years, about 800 publications have reported the capability of these filters in solving robot vision problems. Such problems encompass a rather wide application area, such as object modeling, robot control, target tracking, surveillance, search, recognition, and assembly, as well as robotic manipulation, localization, mapping, navigation, and exploration. These reports are summarized in this review to enable easy referral to suitable methods for practical solutions. Representative contributions and future research trends are also addressed in an abstract level.</td>
</tr>
<tr id="bib_5985520" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{5985520,
  author = {S. Y. Chen},
  title = {Kalman Filter for Robot Vision: A Survey},
  journal = {IEEE Transactions on Industrial Electronics},
  year = {2012},
  volume = {59},
  number = {11},
  pages = {4409-4420},
  doi = {http://dx.doi.org/10.1109/TIE.2011.2162714}
}
</pre></td>
</tr>
<tr id="4026352" class="entry">
	<td>Chen, X. and Li, Y.</td>
	<td>Smooth Path Planning of a Mobile Robot Using Stochastic Particle Swarm Optimization <p class="infolinks">[<a href="javascript:toggleInfo('4026352','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('4026352','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>2006 International Conference on Mechatronics and Automation, pp. 1722-1727&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICMA.2006.257474">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_4026352" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper proposes a new approach using improved particle swarm optimization (PSO) to optimize the path of a mobile robot through an environment containing static obstacles. Relative to many optimization methods that produce nonsmooth paths, the PSO method developed in this paper can generate smooth paths, which are more preferable for designing continuous control technologies to realize path following using mobile robots. To reduce computational cost of optimization, the stochastic PSO (S-PSO) with high exploration ability is developed, so that a swarm with small size can accomplish path planning. Simulation results validate the proposed algorithm in a mobile robot path planning</td>
</tr>
<tr id="bib_4026352" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{4026352,
  author = {X. Chen and Y. Li},
  title = {Smooth Path Planning of a Mobile Robot Using Stochastic Particle Swarm Optimization},
  journal = {2006 International Conference on Mechatronics and Automation},
  year = {2006},
  pages = {1722-1727},
  doi = {http://dx.doi.org/10.1109/ICMA.2006.257474}
}
</pre></td>
</tr>
<tr id="6225373" class="entry">
	<td>Cheng, N.G., Lobovsky, M.B., Keating, S.J., Setapen, A.M., Gero, K.I., Hosoi, A.E. and Iagnemma, K.D.</td>
	<td>Design and Analysis of a Robust, Low-cost, Highly Articulated manipulator enabled by jamming of granular media <p class="infolinks">[<a href="javascript:toggleInfo('6225373','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6225373','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 IEEE International Conference on Robotics and Automation, pp. 4328-4333&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2012.6225373">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_6225373" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Hyper-redundant manipulators can be fragile, expensive, and limited in their flexibility due to the distributed and bulky actuators that are typically used to achieve the precision and degrees of freedom (DOFs) required. Here, a manipulator is proposed that is robust, high-force, low-cost, and highly articulated without employing traditional actuators mounted at the manipulator joints. Rather, local tunable stiffness is coupled with off-board spooler motors and tension cables to achieve complex manipulator configurations. Tunable stiffness is achieved by reversible jamming of granular media, which-by applying a vacuum to enclosed grains-causes the grains to transition between solid-like states and liquid-like ones. Experimental studies were conducted to identify grains with high strength-to-weight performance. A prototype of the manipulator is presented with performance analysis, with emphasis on speed, strength, and articulation. This novel design for a manipulator-and use of jamming for robotic applications in general-could greatly benefit applications such as human-safe robotics and systems in which robots need to exhibit high flexibility to conform to their environments.</td>
</tr>
<tr id="bib_6225373" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6225373,
  author = {N. G. Cheng and M. B. Lobovsky and S. J. Keating and A. M. Setapen and K. I. Gero and A. E. Hosoi and K. D. Iagnemma},
  title = {Design and Analysis of a Robust, Low-cost, Highly Articulated manipulator enabled by jamming of granular media},
  journal = {2012 IEEE International Conference on Robotics and Automation},
  year = {2012},
  pages = {4328-4333},
  doi = {http://dx.doi.org/10.1109/ICRA.2012.6225373}
}
</pre></td>
</tr>
<tr id="Choi2008" class="entry">
	<td>Choi, Y.-H., Lee, T.-K. and Oh, S.-Y.</td>
	<td>A line feature based SLAM with low grade range sensors using geometric constraints and active exploration for mobile robot <p class="infolinks">[<a href="javascript:toggleInfo('Choi2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Choi2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Autonomous Robots<br/>Vol. 24(1), pp. 13-27&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1007/s10514-007-9050-y">DOI</a> <a href="http://dx.doi.org/10.1007/s10514-007-9050-y">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Choi2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes a geometrically constrained Extended Kalman Filter (EKF) framework for a line feature based SLAM, which is applicable to a rectangular indoor environment. Its focus is on how to handle sparse and noisy sensor data, such as PSD infrared sensors with limited range and limited number, in order to develop a low-cost navigation system. It has been applied to a vacuum cleaning robot in our research. In order to meet the real-time objective with low computing power, we develop an efficient line feature extraction algorithm based upon an iterative end point fit (IEPF) technique assisted by our constrained version of the Hough transform. It uses a geometric constraint that every line is orthogonal or parallel to each other because in a general indoor setting, most furniture and walls satisfy this constraint. By adding this constraint to the measurement model of EKF, we build a geometrically constrained EKF framework which can estimate line feature positions more accurately as well as allow their covariance matrices to converge more rapidly when compared to the case of an unconstrained EKF. The experimental results demonstrate the accuracy and robustness to the presence of sensor noise and errors in an actual indoor environment.</td>
</tr>
<tr id="bib_Choi2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Choi2008,
  author = {Choi, Young-Ho and Lee, Tae-Kyeong and Oh, Se-Young},
  title = {A line feature based SLAM with low grade range sensors using geometric constraints and active exploration for mobile robot},
  journal = {Autonomous Robots},
  year = {2008},
  volume = {24},
  number = {1},
  pages = {13--27},
  url = {http://dx.doi.org/10.1007/s10514-007-9050-y},
  doi = {http://dx.doi.org/10.1007/s10514-007-9050-y}
}
</pre></td>
</tr>
<tr id="6290685" class="entry">
	<td>Cirillo, A., Cirillo, P. and Pirozzi, S.</td>
	<td>A modular and low-cost artificial skin for robotic applications <p class="infolinks">[<a href="javascript:toggleInfo('6290685','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6290685','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 4th IEEE RAS EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob), pp. 961-966&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/BioRob.2012.6290685">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_6290685" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper a novel modular artificial skin is presented. The skin is organized in a chain of optoelectronic sensor arrays that communicate with serial interconnections. The proposed solution is highly modular and scalable since each module can be removed and/or inserted in the chain without introducing any changing in the architecture of the skin or in the acquisition system. A sensor element is constituted by an emitter/receiver couple of optoelectronic devices covered by a silicone layer. An external force applied to the deformable layer is transduced into a vertical displacement measured by the optoelectronic couple. A skin module consists of n sensing elements and m modules can be interconnected in order to obtain a complete skin with n×m sensing elements. The use of inexpensive and off-the-shelf components allows to change the density of the sensing elements as needed, maintaining a low cost.</td>
</tr>
<tr id="bib_6290685" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6290685,
  author = {A. Cirillo and P. Cirillo and S. Pirozzi},
  title = {A modular and low-cost artificial skin for robotic applications},
  journal = {2012 4th IEEE RAS EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
  year = {2012},
  pages = {961-966},
  doi = {http://dx.doi.org/10.1109/BioRob.2012.6290685}
}
</pre></td>
</tr>
<tr id="DeCristóforis2015118" class="entry">
	<td>Cristóforis, P.D., Nitsche, M., Krajník, T., Pire, T. and Mejail, M.</td>
	<td>Hybrid vision-based navigation for mobile robots in mixed indoor/outdoor environments  <p class="infolinks">[<a href="javascript:toggleInfo('DeCristóforis2015118','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('DeCristóforis2015118','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Pattern Recognition Letters <br/>Vol. 53, pp. 118 - 128&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.patrec.2014.10.010">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0167865514003274">URL</a>&nbsp;</td>
</tr>
<tr id="abs_DeCristóforis2015118" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract In this paper we present a vision-based navigation system for mobile robots equipped with a single, off-the-shelf camera in mixed indoor/outdoor environments. A hybrid approach is proposed, based on the teach-and-replay technique, which combines a path-following and a feature-based navigation algorithm. We describe the navigation algorithms and show that both of them correct the robot’s lateral displacement from the intended path. After that, we claim that even though neither of the methods explicitly estimates the robot position, the heading corrections themselves keep the robot position error bound. We show that combination of the methods outperforms the pure feature-based approach in terms of localization precision and that this combination reduces map size and simplifies the learning phase. Experiments in mixed indoor/outdoor environments were carried out with a wheeled and a tracked mobile robots in order to demonstrate the validity and the benefits of the hybrid approach. </td>
</tr>
<tr id="bib_DeCristóforis2015118" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{DeCristóforis2015118,
  author = {Pablo De Cristóforis and Matias Nitsche and Tomáš Krajník and Taihú Pire and Marta Mejail},
  title = {Hybrid vision-based navigation for mobile robots in mixed indoor/outdoor environments },
  journal = {Pattern Recognition Letters },
  year = {2015},
  volume = {53},
  pages = {118 - 128},
  url = {http://www.sciencedirect.com/science/article/pii/S0167865514003274},
  doi = {http://dx.doi.org/10.1016/j.patrec.2014.10.010}
}
</pre></td>
</tr>
<tr id="784976" class="entry">
	<td>Dellaert, F., Burgard, W., Fox, D. and Thrun, S.</td>
	<td>Using the CONDENSATION algorithm for robust, vision-based mobile robot localization <p class="infolinks">[<a href="javascript:toggleInfo('784976','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('784976','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td> IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1999..<br/>Vol. 2, pp. 594 Vol. 2&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/CVPR.1999.784976">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_784976" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: To navigate reliably in indoor environments, a mobile robot must know where it is. This includes both the ability of globally localizing the robot from scratch, as well as tracking the robot's position once its location is known. Vision has long been advertised as providing a solution to these problems, but we still lack efficient solutions in unmodified environments. Many existing approaches require modification of the environment to function properly, and those that work within unmodified environments seldomly address the problem of global localization. In this paper we present a novel, vision-based localization method based on the CONDENSATION algorithm, a Bayesian filtering method that uses a sampling-based density representation. We show how the CONDENSATION algorithm can be rued in a novel way to track the position of the camera platform rather than tracking an object in the scene. In addition, it can also be used to globally localize the camera platform, given a visual map of the environment. Based on these two observations, we present a vision-based robot localization method that provides a solution to a difficult and open problem in the mobile robotics community. As evidence for the viability of our approach, we show both global localization and tracking results in the context of a state of the art robotics application</td>
</tr>
<tr id="bib_784976" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{784976,
  author = {F. Dellaert and W. Burgard and D. Fox and S. Thrun},
  title = {Using the CONDENSATION algorithm for robust, vision-based mobile robot localization},
  journal = { IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1999..},
  year = {1999},
  volume = {2},
  pages = {594 Vol. 2},
  doi = {http://dx.doi.org/10.1109/CVPR.1999.784976}
}
</pre></td>
</tr>
<tr id="772544" class="entry">
	<td>Dellaert, F., Fox, D., Burgard, W. and Thrun, S.</td>
	<td>Monte Carlo localization for mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('772544','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('772544','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td> Proceedings IEEE International Conference on Robotics and Automation<br/>Vol. 2, pp. 1322-1328 vol.2&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.1999.772544">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_772544" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: To navigate reliably in indoor environments, a mobile robot must know where it is. Thus, reliable position estimation is a key problem in mobile robotics. We believe that probabilistic approaches are among the most promising candidates to providing a comprehensive and real-time solution to the robot localization problem. However, current methods still face considerable hurdles. In particular the problems encountered are closely related to the type of representation used to represent probability densities over the robot's state space. Earlier work on Bayesian filtering with particle-based density representations opened up a new approach for mobile robot localization based on these principles. We introduce the Monte Carlo localization method, where we represent the probability density involved by maintaining a set of samples that are randomly drawn from it. By using a sampling-based representation we obtain a localization method that can represent arbitrary distributions. We show experimentally that the resulting method is able to efficiently localize a mobile robot without knowledge of its starting location. It is faster, more accurate and less memory-intensive than earlier grid-based methods,</td>
</tr>
<tr id="bib_772544" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{772544,
  author = {F. Dellaert and D. Fox and W. Burgard and S. Thrun},
  title = {Monte Carlo localization for mobile robots},
  journal = { Proceedings IEEE International Conference on Robotics and Automation},
  year = {1999},
  volume = {2},
  pages = {1322-1328 vol.2},
  doi = {http://dx.doi.org/10.1109/ROBOT.1999.772544}
}
</pre></td>
</tr>
<tr id="desouza2002vision" class="entry">
	<td>DeSouza, G.N. and Kak, A.C.</td>
	<td>Vision for mobile robot navigation: A survey <p class="infolinks">[<a href="javascript:toggleInfo('desouza2002vision','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('desouza2002vision','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 24(2), pp. 237-267&nbsp;</td>
	<td>article</td>
	<td><a href="http://ieeexplore.ieee.org/document/982903/?arnumber=982903">URL</a>&nbsp;</td>
</tr>
<tr id="abs_desouza2002vision" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Surveys the developments of the last 20 years in the area of vision for mobile robot navigation. Two major components of the paper deal with indoor navigation and outdoor navigation. For each component, we have further subdivided our treatment of the subject on the basis of structured and unstructured environments. For indoor robots in structured environments, we have dealt separately with the cases of geometrical and topological models of space. For unstructured environments, we have discussed the cases of navigation using optical flows, using methods from the appearance-based paradigm, and by recognition of specific objects in the environment.</td>
</tr>
<tr id="bib_desouza2002vision" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{desouza2002vision,
  author = {DeSouza, Guilherme N and Kak, Avinash C},
  title = {Vision for mobile robot navigation: A survey},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  year = {2002},
  volume = {24},
  number = {2},
  pages = {237--267},
  url = {http://ieeexplore.ieee.org/document/982903/?arnumber=982903}
}
</pre></td>
</tr>
<tr id="1087096" class="entry">
	<td>Elfes, A.</td>
	<td>Sonar-based real-world mapping and navigation <p class="infolinks">[<a href="javascript:toggleInfo('1087096','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1087096','bibtex')">BibTeX</a>]</p></td>
	<td>1987</td>
	<td>IEEE Journal on Robotics and Automation<br/>Vol. 3(3), pp. 249-265&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/JRA.1987.1087096">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1087096" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A sonar-based mapping and navigation system developed for an autonomous mobile robot operating in unknown and unstructured environments is described. The system uses sonar range data to build a multileveled description of the robot's surroundings. Sonar readings are interpreted using probability profiles to determine empty and occupied areas. Range measurements from multiple points of view are integrated into a sensor-level sonar map, using a robust method that combines the sensor information in such a way as to cope with uncertainties and errors in the data. The resulting two-dimensional maps are used for path planning and navigation. From these sonar maps, multiple representations are developed for various kinds of problem-solving activities. Several dimensions of representation are defined: the abstraction axis, the geographical axis, and the resolution axis. The sonar mapping procedures have been implemented as part of an autonomous mobile robot navigation system called Dolphin. The major modules of this system are described and related to the various mapping representations used. Results from actual runs are presented, and further research is mentioned. The system is also situated within the wider context of developing an advanced software architecture for autonomous mobile robots.</td>
</tr>
<tr id="bib_1087096" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{1087096,
  author = {A. Elfes},
  title = {Sonar-based real-world mapping and navigation},
  journal = {IEEE Journal on Robotics and Automation},
  year = {1987},
  volume = {3},
  number = {3},
  pages = {249-265},
  doi = {http://dx.doi.org/10.1109/JRA.1987.1087096}
}
</pre></td>
</tr>
<tr id="5451187" class="entry">
	<td>Errington, A.F.C., Daku, B.L.F. and Prugger, A.F.</td>
	<td>Initial Position Estimation Using RFID Tags: A Least-Squares Approach <p class="infolinks">[<a href="javascript:toggleInfo('5451187','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('5451187','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>IEEE Transactions on Instrumentation and Measurement<br/>Vol. 59(11), pp. 2863-2869&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TIM.2010.2046366">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_5451187" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The GPS has revolutionized how people, vehicles, and objects are positioned. The GPS, however, has limitations. It will only work well where a signal can be received and will not work underground, in tunnels, or even some buildings. Obtaining an accurate position estimate in these areas must therefore use alternate methods that do not rely on GPS. Promising research from the field of robotics provides an alternative approach to positioning, using a technique known as simultaneous localization and mapping (SLAM). The challenge for the SLAM algorithm is that the initial position given to the algorithm must be accurate. This paper investigates the concept of using an array of RF identification (RFID) tags placed at known positions to provide the initial position of the stationary vehicle to the SLAM algorithm. A least-squares (LS)-based position estimator is presented and evaluated in an experiment conducted in an underground potash mine and an indoor environment at the University of Saskatchewan. The estimator's average error is calculated using models with a varied number of parameters. It was found that both environments attain the best results with five model parameters that were obtained from data taken in the same environment. The results suggest that RFID-based positioning, using this LS approach, has the potential to provide relatively accurate and low-cost initial position estimation.</td>
</tr>
<tr id="bib_5451187" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{5451187,
  author = {A. F. C. Errington and B. L. F. Daku and A. F. Prugger},
  title = {Initial Position Estimation Using RFID Tags: A Least-Squares Approach},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  year = {2010},
  volume = {59},
  number = {11},
  pages = {2863-2869},
  doi = {http://dx.doi.org/10.1109/TIM.2010.2046366}
}
</pre></td>
</tr>
<tr id="6630694" class="entry">
	<td>Faigl, J., Krajník, T., Chudoba, J., Přeučil, L. and Saska, M.</td>
	<td>Low-cost embedded system for relative localization in robotic swarms <p class="infolinks">[<a href="javascript:toggleInfo('6630694','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6630694','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>IEEE International Conference on Robotics and Automation (ICRA), 2013 , pp. 993-998&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2013.6630694">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_6630694" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we present a small, light-weight, low-cost, fast and reliable system designed to satisfy requirements of relative localization within a swarm of micro aerial vehicles. The core of the proposed solution is based on off-the-shelf components consisting of the Caspa camera module and Gumstix Overo board accompanied by a developed efficient image processing method for detecting black and white circular patterns. Although the idea of the roundel recognition is simple, the developed system exhibits reliable and fast estimation of the relative position of the pattern up to 30 fps using the full resolution of the Caspa camera. Thus, the system is suited to meet requirements for a vision based stabilization of the robotic swarm. The intent of this paper is to present the developed system as an enabling technology for various robotic tasks.</td>
</tr>
<tr id="bib_6630694" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6630694,
  author = {J. Faigl and T. Krajník and J. Chudoba and L. Přeučil and M. Saska},
  title = {Low-cost embedded system for relative localization in robotic swarms},
  journal = {IEEE International Conference on Robotics and Automation (ICRA), 2013 },
  year = {2013},
  pages = {993-998},
  doi = {http://dx.doi.org/10.1109/ICRA.2013.6630694}
}
</pre></td>
</tr>
<tr id="feder1999adaptive" class="entry">
	<td>Feder, H.J.S., Leonard, J.J. and Smith, C.M.</td>
	<td>Adaptive mobile robot navigation and mapping <p class="infolinks">[<a href="javascript:toggleInfo('feder1999adaptive','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('feder1999adaptive','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td>The International Journal of Robotics Research<br/>Vol. 18(7), pp. 650-668&nbsp;</td>
	<td>article</td>
	<td><a href="http://ijr.sagepub.com/content/18/7/650.short">URL</a>&nbsp;</td>
</tr>
<tr id="abs_feder1999adaptive" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract The task of building a map of an unknown environment and concurrently using that map to navigate is a central problem in mobile robotics research. This paper addresses the problem of how to perform concurrent mapping and localization (CML) adaptively using sonar. Stochastic mapping is a feature-based approach to CML that generalizes the extended Kalman filter to incorporate vehicle localization and environmental mapping. The authors describe an implementation of stochastic mapping that uses a delayed nearest neighbor data association strategy to initialize new features into the map, match measurements to map features, and delete out-of-date features. The authors introduce a metric for adaptive sensing that is defined in terms of Fisher information and represents the sum of the areas of the error ellipses of the vehicle and feature estimates in the map. Predicted sensor readings and expected dead-reckoning errors are used to estimate the metric for each potential action of the robot, and the action that yields the lowest cost (i.e., the maximum information) is selected. This technique is demonstrated via simulations, in-air sonar experiments, and underwater sonar experiments. Results are shown for (1) adaptive control of motion and (2) adaptive control of motion and scanning. The vehicle tends to explore selectively different objects in the environment. The performance of this adaptive algorithm is shown to be superior to straight-line motion and random motion. </td>
</tr>
<tr id="bib_feder1999adaptive" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{feder1999adaptive,
  author = {Feder, Hans Jacob S and Leonard, John J and Smith, Christopher M},
  title = {Adaptive mobile robot navigation and mapping},
  journal = {The International Journal of Robotics Research},
  publisher = {SAGE Publications},
  year = {1999},
  volume = {18},
  number = {7},
  pages = {650--668},
  url = {http://ijr.sagepub.com/content/18/7/650.short}
}
</pre></td>
</tr>
<tr id="1013691" class="entry">
	<td>Fod, A., Howard, A. and Mataric, M.A.J.</td>
	<td>A laser-based people tracker <p class="infolinks">[<a href="javascript:toggleInfo('1013691','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1013691','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td> Proceedings. ICRA '02. IEEE International Conference on Robotics and Automation, 2002.<br/>Vol. 3, pp. 3024-3029&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2002.1013691">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1013691" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Describes a method for real-time tracking of people in everyday environments, using multiple planar laser range-finders. People tracking is a well-studied problem in machine vision; we adapt some of those methods to laser range-finders. We group range measurements into entities such as blobs and objects, and use a Kalman filter to estimate trajectories for these objects. The filter is able to generate smooth trajectories, even when objects are occluded. The paper presents our evaluation of the tracker's performance in a series of four experiments</td>
</tr>
<tr id="bib_1013691" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1013691,
  author = {A. Fod and A. Howard and M. A. J. Mataric},
  title = {A laser-based people tracker},
  journal = { Proceedings. ICRA '02. IEEE International Conference on Robotics and Automation, 2002.},
  year = {2002},
  volume = {3},
  pages = {3024-3029},
  doi = {http://dx.doi.org/10.1109/ROBOT.2002.1013691}
}
</pre></td>
</tr>
<tr id="fox1999markov" class="entry">
	<td>Fox, D., Burgard, W. and Thrun, S.</td>
	<td>Markov localization for mobile robots in dynamic environments <p class="infolinks">[<a href="javascript:toggleInfo('fox1999markov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('fox1999markov','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td>Journal of Artificial Intelligence Research<br/>Vol. 11, pp. 391-427&nbsp;</td>
	<td>article</td>
	<td><a href="http://www.jair.org/papers/paper616.html">URL</a>&nbsp;</td>
</tr>
<tr id="abs_fox1999markov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Localization, that is the estimation of a robot's location from sensor data, is a fundamental problem in mobile robotics. This papers presents a version of Markov localization which provides accurate position estimates and which is tailored towards dynamic environments. The key idea of Markov localization is to maintain a probability density over the space of all locations of a robot in its environment. Our approach represents this space metrically, using a fine-grained grid to approximate densities. It is able to globally localize the robot from scratch and to recover from localization failures. It is robust to approximate models of the environment (such as occupancy grid maps) and noisy sensors (such as ultrasound sensors). Our approach also includes a filtering technique which allows a mobile robot to reliably estimate its position even in densely populated environments in which crowds of people block the robot's sensors for extended periods of time. The method described here has been implemented and tested in several real-world applications of mobile robots, including the deployments of two mobile robots as interactive museum tour-guides.</td>
</tr>
<tr id="bib_fox1999markov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{fox1999markov,
  author = {Fox, Dieter and Burgard, Wolfram and Thrun, Sebastian},
  title = {Markov localization for mobile robots in dynamic environments},
  journal = {Journal of Artificial Intelligence Research},
  year = {1999},
  volume = {11},
  pages = {391--427},
  url = {http://www.jair.org/papers/paper616.html}
}
</pre></td>
</tr>
<tr id="fu2013precise" class="entry">
	<td>Fu, G., Zhang, J., Chen, W., Peng, F., Yang, P. and Chen, C.</td>
	<td>Precise localization of mobile robots via odometry and wireless sensor network <p class="infolinks">[<a href="javascript:toggleInfo('fu2013precise','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('fu2013precise','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>International Journal of Advanced Robotic Systems<br/>Vol. 10&nbsp;</td>
	<td>article</td>
	<td><a href="http://search.proquest.com/openview/e0a53aca66ab07da556683568d31ccbb/1?pq-origsite=gscholar">URL</a>&nbsp;</td>
</tr>
<tr id="abs_fu2013precise" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract Precise localization of mobile robots in uncertain environments is a fundamental and crucial issue in robotics. In this paper, to deal with the unbounded accumulated errors of dead reckoning (DR) ‐ based localization, wireless sensor network (WSN) ‐ based localization is applied to calibrate the uncertainty of odometry using a Kalman filter (KF). In addition, to further aid in obtaining precise positions and reduce uncertainty, a novel backward dead reckoning (BDR) localization approach is proposed. The experimental results demonstrate the success and reliability of the proposed method.</td>
</tr>
<tr id="bib_fu2013precise" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{fu2013precise,
  author = {Fu, Guoyu and Zhang, Jin and Chen, Wenyuan and Peng, Fengchao and Yang, Pei and Chen, Chunlin},
  title = {Precise localization of mobile robots via odometry and wireless sensor network},
  journal = {International Journal of Advanced Robotic Systems},
  publisher = {InTech},
  year = {2013},
  volume = {10},
  url = {http://search.proquest.com/openview/e0a53aca66ab07da556683568d31ccbb/1?pq-origsite=gscholar}
}
</pre></td>
</tr>
<tr id="Galceran20131258" class="entry">
	<td>Galceran, E. and Carreras, M.</td>
	<td>A survey on coverage path planning for robotics  <p class="infolinks">[<a href="javascript:toggleInfo('Galceran20131258','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Galceran20131258','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Robotics and Autonomous Systems <br/>Vol. 61(12), pp. 1258 - 1276&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.robot.2013.09.004">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S092188901300167X">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Galceran20131258" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract Coverage Path Planning (CPP) is the task of determining a path that passes over all points of an area or volume of interest while avoiding obstacles. This task is integral to many robotic applications, such as vacuum cleaning robots, painter robots, autonomous underwater vehicles creating image mosaics, demining robots, lawn mowers, automated harvesters, window cleaners and inspection of complex structures, just to name a few. A considerable body of research has addressed the CPP problem. However, no updated surveys on CPP reflecting recent advances in the field have been presented in the past ten years. In this paper, we present a review of the most successful CPP methods, focusing on the achievements made in the past decade. Furthermore, we discuss reported field applications of the described CPP methods. This work aims to become a starting point for researchers who are initiating their endeavors in CPP. Likewise, this work aims to present a comprehensive review of the recent breakthroughs in the field, providing links to the most interesting and successful works. </td>
</tr>
<tr id="bib_Galceran20131258" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Galceran20131258,
  author = {Enric Galceran and Marc Carreras},
  title = {A survey on coverage path planning for robotics },
  journal = {Robotics and Autonomous Systems },
  year = {2013},
  volume = {61},
  number = {12},
  pages = {1258 - 1276},
  url = {http://www.sciencedirect.com/science/article/pii/S092188901300167X},
  doi = {http://dx.doi.org/10.1016/j.robot.2013.09.004}
}
</pre></td>
</tr>
<tr id="Garcia2004195" class="entry">
	<td>Garcia, E. and de Santos, P.G.</td>
	<td>Mobile-robot navigation with complete coverage of unstructured environments  <p class="infolinks">[<a href="javascript:toggleInfo('Garcia2004195','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Garcia2004195','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Robotics and Autonomous Systems <br/>Vol. 46(4), pp. 195 - 204&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.robot.2004.02.005">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0921889004000235">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Garcia2004195" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: There are some mobile-robot applications that require the complete coverage of an unstructured environment. Examples are humanitarian de-mining and floor-cleaning tasks. A complete-coverage algorithm is then used, a path-planning technique that allows the robot to pass over all points in the environment, avoiding unknown obstacles. Different coverage algorithms exist, but they fail working in unstructured environments. This paper details a complete-coverage algorithm for unstructured environments based on sensor information. Simulation results using a mobile robot validate the proposed approach. </td>
</tr>
<tr id="bib_Garcia2004195" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Garcia2004195,
  author = {E. Garcia and P. Gonzalez de Santos},
  title = {Mobile-robot navigation with complete coverage of unstructured environments },
  journal = {Robotics and Autonomous Systems },
  year = {2004},
  volume = {46},
  number = {4},
  pages = {195 - 204},
  url = {http://www.sciencedirect.com/science/article/pii/S0921889004000235},
  doi = {http://dx.doi.org/10.1016/j.robot.2004.02.005}
}
</pre></td>
</tr>
<tr id="4796279" class="entry">
	<td>Gezici, S. and Poor, H.V.</td>
	<td>Position Estimation via Ultra-Wide-Band Signals <p class="infolinks">[<a href="javascript:toggleInfo('4796279','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('4796279','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Proceedings of the IEEE<br/>Vol. 97(2), pp. 386-403&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/JPROC.2008.2008840">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_4796279" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The high time resolution of ultra-wide-band (UWB) signals facilitates very precise position estimation in many scenarios, which makes a variety applications possible. This paper reviews the problem of position estimation in UWB systems, beginning with an overview of the basic structure of UWB signals and their positioning applications. This overview is followed by a discussion of various position estimation techniques, with an emphasis on time-based approaches, which are particularly suitable for UWB positioning systems. Practical issues arising in UWB signal design and hardware implementation are also discussed.</td>
</tr>
<tr id="bib_4796279" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{4796279,
  author = {S. Gezici and H. V. Poor},
  title = {Position Estimation via Ultra-Wide-Band Signals},
  journal = {Proceedings of the IEEE},
  year = {2009},
  volume = {97},
  number = {2},
  pages = {386-403},
  doi = {http://dx.doi.org/10.1109/JPROC.2008.2008840}
}
</pre></td>
</tr>
<tr id="977164" class="entry">
	<td>Girod, L. and Estrin, D.</td>
	<td>Robust range estimation using acoustic and multimodal sensing <p class="infolinks">[<a href="javascript:toggleInfo('977164','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('977164','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>Proceedings. 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2001. <br/>Vol. 3, pp. 1312-1320 vol.3&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2001.977164">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_977164" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many applications of robotics and embedded sensor technology can benefit from fine-grained localization. Fine-grained localization can simplify multi-robot collaboration, enable energy efficient multi-hop routing for low-power radio networks, and enable automatic calibration of distributed sensing systems. We focus on range estimation, a critical prerequisite for fine-grained localization. While many mechanisms for range estimation exist, any individual mode of sensing can be blocked or confused by the environment. We present and analyze an acoustic ranging system that performs well in the presence of many types of interference, but can return incorrect measurements in non-line-of-sight conditions. We then suggest how evidence from an orthogonal sensory channel might be used to detect and eliminate these measurements. The work illustrates the more general research theme of combining multiple modalities to obtain robust results</td>
</tr>
<tr id="bib_977164" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{977164,
  author = {L. Girod and D. Estrin},
  title = {Robust range estimation using acoustic and multimodal sensing},
  journal = {Proceedings. 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2001. },
  year = {2001},
  volume = {3},
  pages = {1312-1320 vol.3},
  doi = {http://dx.doi.org/10.1109/IROS.2001.977164}
}
</pre></td>
</tr>
<tr id="1307283" class="entry">
	<td>Hahnel, D., Burgard, W., Fox, D., Fishkin, K. and Philipose, M.</td>
	<td>Mapping and localization with RFID technology <p class="infolinks">[<a href="javascript:toggleInfo('1307283','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1307283','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Proceedings. ICRA '04. 2004 IEEE International Conference on Robotics and Automation, 2004. <br/>Vol. 1, pp. 1015-1020 Vol.1&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2004.1307283">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1307283" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We analyze whether radio frequency identification (RFID) technology can be used to improve the localization of mobile robots and persons in their environment. In particular we study the problem of localizing RFID tags with a mobile platform that is equipped with a pair of RFID antennas. We present a probabilistic measurement model for RFID readers that allow us to accurately localize RFID tags in the environment. We also demonstrate how such maps can be used to localize a robot and persons in their environment. Finally, we present experiments illustrating that the computational requirements for global robot localization can be reduced strongly by fusing RFID information with laser data.</td>
</tr>
<tr id="bib_1307283" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1307283,
  author = {D. Hahnel and W. Burgard and D. Fox and K. Fishkin and M. Philipose},
  title = {Mapping and localization with RFID technology},
  journal = {Proceedings. ICRA '04. 2004 IEEE International Conference on Robotics and Automation, 2004. },
  year = {2004},
  volume = {1},
  pages = {1015-1020 Vol.1},
  doi = {http://dx.doi.org/10.1109/ROBOT.2004.1307283}
}
</pre></td>
</tr>
<tr id="Hank2016113" class="entry">
	<td>Hank, M. and Haddad, M.</td>
	<td>A hybrid approach for autonomous navigation of mobile robots in partially-known environments  <p class="infolinks">[<a href="javascript:toggleInfo('Hank2016113','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hank2016113','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Robotics and Autonomous Systems <br/>Vol. 86, pp. 113 - 127&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.robot.2016.09.009">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0921889015302992">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Hank2016113" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract We propose a hybrid approach specifically adapted to deal with the autonomous-navigation problem of a mobile robot which is subjected to perform an emergency task in a partially-known environment. Such a navigation problem requires a method that is able to yield a fast execution time, under constraints on the capacity of the robot and on known/unknown obstacles, and that is sufficiently flexible to deal with errors in the known parts of the environment (unexpected obstacles). Our proposal includes an off-line task-independent preprocessing phase, which is applied just once for a given robot in a given environment. Its purpose is to build, within the known zones, a roadmap of near-time-optimal reference trajectories. The actual execution of the task is an online process that combines reactive navigation with trajectory tracking and that includes smooth transitions between these two modes of navigation. Controllers used are fuzzy-inference systems. Both simulation and experimental results are presented to test the performance of the proposed hybrid approach. Obtained results demonstrate the ability of our proposal to handle unexpected obstacles and to accomplish navigation tasks in relatively complex environments. The results also show that, thanks to its time-optimal-trajectory planning, our proposal is well adapted to emergency tasks as it is able to achieve shorter execution times, compared to other waypoint-navigation methods that rely on optimal-path planning. </td>
</tr>
<tr id="bib_Hank2016113" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hank2016113,
  author = {Madjid Hank and Moussa Haddad},
  title = {A hybrid approach for autonomous navigation of mobile robots in partially-known environments },
  journal = {Robotics and Autonomous Systems },
  year = {2016},
  volume = {86},
  pages = {113 - 127},
  url = {http://www.sciencedirect.com/science/article/pii/S0921889015302992},
  doi = {http://dx.doi.org/10.1016/j.robot.2016.09.009}
}
</pre></td>
</tr>
<tr id="7140021" class="entry">
	<td>Hart, C., Kreinar, E.J., Chrzanowski, D., Daltorio, K.A. and Quinn, R.D.</td>
	<td>A low-cost robot using omni-directional vision enables insect-like behaviors <p class="infolinks">[<a href="javascript:toggleInfo('7140021','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7140021','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>2015 IEEE International Conference on Robotics and Automation (ICRA), pp. 5871-5878&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2015.7140021">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7140021" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: RAMBLER Robot is designed for researching insect inspired behavioral control algorithms. To evaluate these algorithms, RAMBLER Robot needs autonomous localization without typical sensors like wheel odometers or GPS. The primary objective of this work is to independently, accurately, and robustly recover the path of a moving robotic system with low-cost sensors available off-the-shelf. The computationally efficient power center method of triangulation is compared to a particle filter approach. With three passive indistinguishable landmarks at corners of a small arena, RAMBLER Robot successfully localizes with an RMS error of 2.27cm compared to an overhead camera ground truth.</td>
</tr>
<tr id="bib_7140021" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7140021,
  author = {C. Hart and E. J. Kreinar and D. Chrzanowski and K. A. Daltorio and R. D. Quinn},
  title = {A low-cost robot using omni-directional vision enables insect-like behaviors},
  journal = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  pages = {5871-5878},
  doi = {http://dx.doi.org/10.1109/ICRA.2015.7140021}
}
</pre></td>
</tr>
<tr id="henry2010rgb" class="entry">
	<td>Henry, P., Krainin, M., Herbst, E., Ren, X. and Fox, D.</td>
	<td>RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments <p class="infolinks">[<a href="javascript:toggleInfo('henry2010rgb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('henry2010rgb','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>In the 12th International Symposium on Experimental Robotics (ISER&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.226.91">URL</a>&nbsp;</td>
</tr>
<tr id="abs_henry2010rgb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either structured light patterns combined with stereo sensing [6,10] or time-of-flight laser sensing [1] to generate depth estimates that can be associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below $100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3D maps of unprecedented richness, they have drawbacks that make their application to 3D mapping difficult: They provide depth only up to a limited distance (typically less than 5m), depth values are much noisier than those provided by laser scanners, and their field of view ( ∼ 60 ◦ ) is far more constrained than that of specialized cameras or laser scanners typically used for 3D mapping ( ∼ 180 ◦). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras. </td>
</tr>
<tr id="bib_henry2010rgb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{henry2010rgb,
  author = {Henry, Peter and Krainin, Michael and Herbst, Evan and Ren, Xiaofeng and Fox, Dieter},
  title = {RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments},
  journal = {In the 12th International Symposium on Experimental Robotics (ISER},
  year = {2010},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.226.91}
}
</pre></td>
</tr>
<tr id="howard2006multi" class="entry">
	<td>Howard, A.</td>
	<td>Multi-robot simultaneous localization and mapping using particle filters <p class="infolinks">[<a href="javascript:toggleInfo('howard2006multi','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('howard2006multi','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>The International Journal of Robotics Research<br/>Vol. 25(12), pp. 1243-1256&nbsp;</td>
	<td>article</td>
	<td><a href="http://ijr.sagepub.com/content/25/12/1243.short">URL</a>&nbsp;</td>
</tr>
<tr id="abs_howard2006multi" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  This paper describes an on-line algorithm for multi-robot simultaneous localization and mapping (SLAM). The starting point is the single-robot Rao-Blackwellized particle filter described by Hähnel et al., and three key generalizations are made. First, the particle filter is extended to handle multi-robot SLAM problems in which the initial pose of the robots is known (such as occurs when all robots start from the same location). Second, an approximation is introduced to solve the more general problem in which the initial pose of robots is not known a priori (such as occurs when the robots start from widely separated locations). In this latter case, it is assumed that pairs of robots will eventually encounter one another, thereby determining their relative pose. This relative attitude is used to initialize the filter, and subsequent observations from both robots are combined into a common map. Third and finally, a method is introduced to integrate observations collected prior to the first robot encounter, using the notion of a virtual robot travelling backwards in time. This novel approach allows one to integrate all data from all robots into a single common map. </td>
</tr>
<tr id="bib_howard2006multi" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{howard2006multi,
  author = {Howard, Andrew},
  title = {Multi-robot simultaneous localization and mapping using particle filters},
  journal = {The International Journal of Robotics Research},
  publisher = {SAGE Publications},
  year = {2006},
  volume = {25},
  number = {12},
  pages = {1243--1256},
  url = {http://ijr.sagepub.com/content/25/12/1243.short}
}
</pre></td>
</tr>
<tr id="hwang1992gross" class="entry">
	<td>Hwang, Y.K. and Ahuja, N.</td>
	<td>Gross motion planning—a survey <p class="infolinks">[<a href="javascript:toggleInfo('hwang1992gross','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hwang1992gross','bibtex')">BibTeX</a>]</p></td>
	<td>1992</td>
	<td>ACM Computing Surveys (CSUR)<br/>Vol. 24(3), pp. 219-291&nbsp;</td>
	<td>article</td>
	<td><a href="http://dl.acm.org/citation.cfm?id=136037">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hwang1992gross" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Motlonplanning lsoneofthe mostimportant areasofrobotics research The complexity of the motion-planning problem has hindered the developmentof practical algorithms. This paper surveys the work on gross-motionplanning, including motion planners for point robots, rigid robots, and manipulators in stationary, time-varying, constrained, and movable-objectenvironments.The generalissuesin motion planning are explained Recentapproachesandtheir performancesare briefly described,and possiblefuture researchdirections are discussed. </td>
</tr>
<tr id="bib_hwang1992gross" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{hwang1992gross,
  author = {Hwang, Yong K and Ahuja, Narendra},
  title = {Gross motion planning—a survey},
  journal = {ACM Computing Surveys (CSUR)},
  publisher = {ACM},
  year = {1992},
  volume = {24},
  number = {3},
  pages = {219--291},
  url = {http://dl.acm.org/citation.cfm?id=136037}
}
</pre></td>
</tr>
<tr id="7119600" class="entry">
	<td>Jaimez, M. and Gonzalez-Jimenez, J.</td>
	<td>Fast Visual Odometry for 3-D Range Sensors <p class="infolinks">[<a href="javascript:toggleInfo('7119600','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7119600','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 31(4), pp. 809-822&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2015.2428512">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7119600" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a new dense method to compute the odometry of a free-flying range sensor in real time. The method applies the range flow constraint equation to sensed points in the temporal flow to derive the linear and angular velocity of the sensor in a rigid environment. Although this approach is applicable to any range sensor, we particularize its formulation to estimate the 3-D motion of a range camera. The proposed algorithm is tested with different image resolutions and compared with two state-of-the-art methods: generalized iterative closest point (GICP) [1] and robust dense visual odometry (RDVO) [2]. Experiments show that our approach clearly overperforms GICP which uses the same geometric input data, whereas it achieves results similar to RDVO, which requires both geometric and photometric data to work. Furthermore, experiments are carried out to demonstrate that our approach is able to estimate fast motions at 60 Hz running on a single CPU core, a performance that has never been reported in the literature. The algorithm is available online under an open source license so that the robotic community can benefit from it.</td>
</tr>
<tr id="bib_7119600" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{7119600,
  author = {M. Jaimez and J. Gonzalez-Jimenez},
  title = {Fast Visual Odometry for 3-D Range Sensors},
  journal = {IEEE Transactions on Robotics},
  year = {2015},
  volume = {31},
  number = {4},
  pages = {809-822},
  doi = {http://dx.doi.org/10.1109/TRO.2015.2428512}
}
</pre></td>
</tr>
<tr id="kam1997sensor" class="entry">
	<td>Kam, M., Zhu, X. and Kalata, P.</td>
	<td>Sensor fusion for mobile robot navigation <p class="infolinks">[<a href="javascript:toggleInfo('kam1997sensor','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kam1997sensor','bibtex')">BibTeX</a>]</p></td>
	<td>1997</td>
	<td>Proceedings of the IEEE<br/>Vol. 85(1), pp. 108-119&nbsp;</td>
	<td>article</td>
	<td><a href="http://ieeexplore.ieee.org/document/554212/?arnumber=554212&tag=1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kam1997sensor" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We review techniques for sensor fusion in robot navigation, emphasizing algorithms for self-location. These find use when the sensor suite of a mobile robot comprises several different sensors, some complementary and some redundant. Integrating the sensor readings, the robot seeks to accomplish tasks such as constructing a map of its environment, locating itself in that map, and recognizing objects that should be avoided or sought. The review describes integration techniques in two categories: low-level fusion is used for direct integration of sensory data, resulting in parameter and state estimates; high-level fusion is used for indirect integration of sensory data in hierarchical architectures, through command arbitration and integration of control signals suggested by different modules. The review provides an arsenal of tools for addressing this (rather ill-posed) problem in machine intelligence, including Kalman filtering, rule-based techniques, behavior based algorithms, and approaches that borrow from information theory, Dempster-Shafer reasoning, fuzzy logic and neural networks. </td>
</tr>
<tr id="bib_kam1997sensor" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kam1997sensor,
  author = {Kam, Moshe and Zhu, Xiaoxun and Kalata, Paul},
  title = {Sensor fusion for mobile robot navigation},
  journal = {Proceedings of the IEEE},
  publisher = {IEEE},
  year = {1997},
  volume = {85},
  number = {1},
  pages = {108--119},
  url = {http://ieeexplore.ieee.org/document/554212/?arnumber=554212&amp;tag=1}
}
</pre></td>
</tr>
<tr id="7849551" class="entry">
	<td>Kato, Y. and Tanaka, M.</td>
	<td>Performance evaluation of remote navigation with network delay for low-cost mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('7849551','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7849551','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), pp. 591-596&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/MFI.2016.7849551">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7849551" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we discuss the effect of network delay on remote navigation for low-cost simple robots using cloud environments. First, we conduct the experiment of remote navigation using a robot simulator and show the result which does not navigate a robot adequately under the network condition with pseudo-delay. Then, from the result, we derive the necessity of the control model in considering network delay and design the delay-conscious control model in slowing the observation and quickening the control according to the predefined delay model.</td>
</tr>
<tr id="bib_7849551" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7849551,
  author = {Y. Kato and M. Tanaka},
  title = {Performance evaluation of remote navigation with network delay for low-cost mobile robots},
  journal = {2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)},
  year = {2016},
  pages = {591-596},
  doi = {http://dx.doi.org/10.1109/MFI.2016.7849551}
}
</pre></td>
</tr>
<tr id="508439" class="entry">
	<td>Kavraki, L.E., Svestka, P., Latombe, J.C. and Overmars, M.H.</td>
	<td>Probabilistic roadmaps for path planning in high-dimensional configuration spaces <p class="infolinks">[<a href="javascript:toggleInfo('508439','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('508439','bibtex')">BibTeX</a>]</p></td>
	<td>1996</td>
	<td>IEEE Transactions on Robotics and Automation<br/>Vol. 12(4), pp. 566-580&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/70.508439">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_508439" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A new motion planning method for robots in static workspaces is presented. This method proceeds in two phases: a learning phase and a query phase. In the learning phase, a probabilistic roadmap is constructed and stored as a graph whose nodes correspond to collision-free configurations and whose edges correspond to feasible paths between these configurations. These paths are computed using a simple and fast local planner. In the query phase, any given start and goal configurations of the robot are connected to two nodes of the roadmap; the roadmap is then searched for a path joining these two nodes. The method is general and easy to implement. It can be applied to virtually any type of holonomic robot. It requires selecting certain parameters (e.g., the duration of the learning phase) whose values depend on the scene, that is the robot and its workspace. But these values turn out to be relatively easy to choose, Increased efficiency can also be achieved by tailoring some components of the method (e.g., the local planner) to the considered robots. In this paper the method is applied to planar articulated robots with many degrees of freedom. Experimental results show that path planning can be done in a fraction of a second on a contemporary workstation (≈150 MIPS), after learning for relatively short periods of time (a few dozen seconds)</td>
</tr>
<tr id="bib_508439" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{508439,
  author = {L. E. Kavraki and P. Svestka and J. C. Latombe and M. H. Overmars},
  title = {Probabilistic roadmaps for path planning in high-dimensional configuration spaces},
  journal = {IEEE Transactions on Robotics and Automation},
  year = {1996},
  volume = {12},
  number = {4},
  pages = {566-580},
  doi = {http://dx.doi.org/10.1109/70.508439}
}
</pre></td>
</tr>
<tr id="1387581" class="entry">
	<td>Kemper, M.</td>
	<td>Development of a tactile low-cost microgripper with integrated force sensor <p class="infolinks">[<a href="javascript:toggleInfo('1387581','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1387581','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Proceedings of the 2004 IEEE International Conference on Control Applications, 2004.<br/>Vol. 2, pp. 1461-1466 Vol.2&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/CCA.2004.1387581">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1387581" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes recent results of the development of a novel tactile force-sensing microgripper based on a flexure hinge fabricated in stainless steel by wired electro discharge machining (EDM). The gripper was equipped with a commercial semiconductor strain-gauge and a piezo stack. The microgripper is an end-effector of a microrobot developed to grasp and manipulate tiny objects. Acquiring force-information with the microgripper is of fundamental importance in order to achieve the dexterity and sensing capabilities required to perform micromanipulation or assembly tasks.</td>
</tr>
<tr id="bib_1387581" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1387581,
  author = {M. Kemper},
  title = {Development of a tactile low-cost microgripper with integrated force sensor},
  journal = {Proceedings of the 2004 IEEE International Conference on Control Applications, 2004.},
  year = {2004},
  volume = {2},
  pages = {1461-1466 Vol.2},
  doi = {http://dx.doi.org/10.1109/CCA.2004.1387581}
}
</pre></td>
</tr>
<tr id="kim2011localization" class="entry">
	<td>Kim, Y.-G., An, J. and Lee, K.-D.</td>
	<td>Localization of mobile robot based on fusion of artificial landmark and RF TDOA distance under indoor sensor network <p class="infolinks">[<a href="javascript:toggleInfo('kim2011localization','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kim2011localization','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>International Journal of Advanced Robotic Systems<br/>Vol. 8(4), pp. 203-211&nbsp;</td>
	<td>article</td>
	<td><a href="http://cdn.intechopen.com/pdfs-wm/21843.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kim2011localization" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract In this paper, we propose a robust and real ‐ time localization method for dynamic environments based on a sensor network; the method combines landmark image information obtained from an ordinary camera and distance information obtained from sensor nodes in an indoor environment. The sensor network provides an effective method for a mobile robot to adapt to changes and guides it across a geographical network area. To enhance the performance, we used a charge ‐ coupled device (CCD) camera and artificial landmarks for self ‐ localization. Experimental results showed that global localization can be achieved with high robustness and accuracy using the proposed localization method.</td>
</tr>
<tr id="bib_kim2011localization" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kim2011localization,
  author = {Kim, Yoon-Gu and An, Jinung and Lee, Ki-Dong},
  title = {Localization of mobile robot based on fusion of artificial landmark and RF TDOA distance under indoor sensor network},
  journal = {International Journal of Advanced Robotic Systems},
  year = {2011},
  volume = {8},
  number = {4},
  pages = {203--211},
  url = {http://cdn.intechopen.com/pdfs-wm/21843.pdf}
}
</pre></td>
</tr>
<tr id="4543666" class="entry">
	<td>Konolige, K., Augenbraun, J., Donaldson, N., Fiebig, C. and Shah, P.</td>
	<td>A low-cost laser distance sensor <p class="infolinks">[<a href="javascript:toggleInfo('4543666','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('4543666','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td> ICRA 2008. IEEE International Conference on Robotics and Automation, 2008., pp. 3002-3008&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2008.4543666">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_4543666" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many indoor robotics systems use laser rangeflnders as their primary sensor for mapping, localization, and obstacle avoidance. The cost and power of such systems is a major roadblock to the deployment of low-cost, efficient consumer robot platforms for home use. In this paper, we describe a compact, planar laser distance sensor (LDS) that has capabilities comparable to current laser scanners: 3 cm accuracy out to 6 m, 10 Hz acquisition, and 1 degree resolution over a full 360 degree scan. The build cost of this device, using COTS electronics and custom mechanical tooling, is under $30.</td>
</tr>
<tr id="bib_4543666" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{4543666,
  author = {K. Konolige and J. Augenbraun and N. Donaldson and C. Fiebig and P. Shah},
  title = {A low-cost laser distance sensor},
  journal = { ICRA 2008. IEEE International Conference on Robotics and Automation, 2008.},
  year = {2008},
  pages = {3002-3008},
  doi = {http://dx.doi.org/10.1109/ROBOT.2008.4543666}
}
</pre></td>
</tr>
<tr id="kose2006comparison" class="entry">
	<td>Kose, H., Celik, B. and Akin, H.</td>
	<td>Comparison of localization methods for a robot soccer team <p class="infolinks">[<a href="javascript:toggleInfo('kose2006comparison','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kose2006comparison','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>International journal of advanced robotic systems<br/>Vol. 3(4), pp. 295-302&nbsp;</td>
	<td>article</td>
	<td><a href="http://cdn.intechopen.com/pdfs-wm/4195.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kose2006comparison" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract In this work, several localization algorithms that are designed and implemented for Cerberus'05 Robot Soccer Team are analyzed and compared. These algorithms are used for global localization of autonomous mobile agents in the robotic soccer domain, to overcome the uncertainty in the sensors, environment and the motion model. The algorithms are Reverse Monte Carlo Localizat ion (R-MCL), Simple Localization (S-Loc) and Sensor Resetting Localization (SRL). R-MCL is a hybrid method based on both Markov Localization (ML) and Monte Carlo Localization (MCL) where the ML module finds the region where the robot should be and MCL predicts the geometrical location with high precision by selecting samp les in this region. S-Loc is another localization method where just one sample per percept is drawn, for global localization. Within this method another novel method My Environment (ME) is designed to hold the history and overcome the lack of information due to the drastically decrease in the number of samples in S-Loc. ME togeth er with S-Loc is used in the Technical Challenges in Robocup 2005 and play an important ro le in ranking the First Place in the Challenges. In this work, these methods together with SRL, which is a widely used successf ul localization algorithm, a re tested with both offline and real-time tests. First they are tested on a challengin g data set that is used by many researches and compared in terms of error rate against differen t levels of noise, and sparsity. B esides time required recovering from kidnapping and speed of the methods are tested and comp ared. Then their performances are tested with real-time tests with scenarios like the ones in the Technical Cha llenges in ROBOCUP. The main aim is to find the best method which is very robust and fast and requires l ess computational power and me mory compared to similar approaches and is accurate enough for high level decision making wh ich is vital for robot soccer. </td>
</tr>
<tr id="bib_kose2006comparison" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kose2006comparison,
  author = {Kose, Hatice and Celik, Buluc and Akin, HL},
  title = {Comparison of localization methods for a robot soccer team},
  journal = {International journal of advanced robotic systems},
  publisher = {INTECH Publications},
  year = {2006},
  volume = {3},
  number = {4},
  pages = {295--302},
  url = {http://cdn.intechopen.com/pdfs-wm/4195.pdf}
}
</pre></td>
</tr>
<tr id="krajnik2012simple" class="entry">
	<td>Krajnik, T., Nitsche, M., Pedre, S., Pvreuvcil, L. and Mejail, M.E.</td>
	<td>A simple visual navigation system for an UAV <p class="infolinks">[<a href="javascript:toggleInfo('krajnik2012simple','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('krajnik2012simple','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Systems, Signals and Devices (SSD), 2012 9th International Multi-Conference on, pp. 1-6&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6198031">URL</a>&nbsp;</td>
</tr>
<tr id="abs_krajnik2012simple" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a simple and robust monocular camera- based navigation system for an autonomous quadcopter. The method does not require any additional infrastructure like radio beacons, artificial landmarks or GPS and can be easily combined with other navigation methods and algorithms. Its computational complexity is independent of the environment size and it works even when sensing only one landmark at a time, allowing its operation in landmark poor environments. We also describe an FPGA based embedded realization of the method’s most computationally demanding phase.</td>
</tr>
<tr id="bib_krajnik2012simple" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{krajnik2012simple,
  author = {Krajnik, Tomavs and Nitsche, Matias and Pedre, Sol and Pvreuvcil, Libor and Mejail, Marta E},
  title = {A simple visual navigation system for an UAV},
  journal = {Systems, Signals and Devices (SSD), 2012 9th International Multi-Conference on},
  year = {2012},
  pages = {1--6},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6198031}
}
</pre></td>
</tr>
<tr id="kruse2013human" class="entry">
	<td>Kruse, T., Pandey, A.K., Alami, R. and Kirsch, A.</td>
	<td>Human-aware robot navigation: A survey <p class="infolinks">[<a href="javascript:toggleInfo('kruse2013human','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kruse2013human','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Robotics and Autonomous Systems<br/>Vol. 61(12), pp. 1726-1743&nbsp;</td>
	<td>article</td>
	<td><a href="http://www.sciencedirect.com/science/article/pii/S0921889013001048">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kruse2013human" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract Navigation is a basic skill for autonomous robots. In the last years human–robot interaction has become an important research field that spans all of the robot capabilities including perception, reasoning, learning, manipulation and navigation. For navigation, the presence of humans requires novel approaches that take into account the constraints of human comfort as well as social rules. Besides these constraints, putting robots among humans opens new interaction possibilities for robots, also for navigation tasks, such as robot guides. This paper provides a survey of existing approaches to human-aware navigation and offers a general classification scheme for the presented methods. </td>
</tr>
<tr id="bib_kruse2013human" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kruse2013human,
  author = {Kruse, Thibault and Pandey, Amit Kumar and Alami, Rachid and Kirsch, Alexandra},
  title = {Human-aware robot navigation: A survey},
  journal = {Robotics and Autonomous Systems},
  publisher = {Elsevier},
  year = {2013},
  volume = {61},
  number = {12},
  pages = {1726--1743},
  url = {http://www.sciencedirect.com/science/article/pii/S0921889013001048}
}
</pre></td>
</tr>
<tr id="Lee2003" class="entry">
	<td>Lee, T.-L. and Wu, C.-J.</td>
	<td>Fuzzy Motion Planning of Mobile Robots in Unknown Environments <p class="infolinks">[<a href="javascript:toggleInfo('Lee2003','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lee2003','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>Journal of Intelligent and Robotic Systems<br/>Vol. 37(2), pp. 177-191&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1023/A:1024145608826">DOI</a> <a href="http://dx.doi.org/10.1023/A:1024145608826">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lee2003" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A fuzzy algorithm is proposed to navigate a mobile robot from a given initial configuration to a desired final configuration in an unknown environment filled with obstacles. The mobile robot is equipped with an electronic compass and two optical encoders for dead-reckoning, and two ultrasonic modules for self-localization and environment recognition. From the readings of sensors at every sampling instant, the proposed fuzzy algorithm will determine the priorities of thirteen possible heading directions. Then the robot is driven to an intermediate configuration along the heading direction that has the highest priority. The navigation procedure will be iterated until a collision-free path between the initial and the final configurations is found. To show the feasibility of the proposed method, in addition to computer simulation, experimental results will be also given.</td>
</tr>
<tr id="bib_Lee2003" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lee2003,
  author = {Lee, Tsong-Li and Wu, Chia-Ju},
  title = {Fuzzy Motion Planning of Mobile Robots in Unknown Environments},
  journal = {Journal of Intelligent and Robotic Systems},
  year = {2003},
  volume = {37},
  number = {2},
  pages = {177--191},
  url = {http://dx.doi.org/10.1023/A:1024145608826},
  doi = {http://dx.doi.org/10.1023/A:1024145608826}
}
</pre></td>
</tr>
<tr id="1307193" class="entry">
	<td>Lingelbach, F.</td>
	<td>Path planning using probabilistic cell decomposition <p class="infolinks">[<a href="javascript:toggleInfo('1307193','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1307193','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td> IEEE International Conference on Robotics and Automation, 2004.<br/>Vol. 1, pp. 467-472 Vol.1&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2004.1307193">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1307193" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a new approach to path planning in high-dimensional static configuration spaces. The concept of cell decomposition is combined with probabilistic sampling to obtain a method called probabilistic cell decomposition (PCD). The use of lazy evaluation techniques and supervised sampling in important areas leads to a very competitive path planning method. It is shown that PCD is probabilistic complete, PCD is easily scalable and applicable to many different kinds of problems. Experimental results show that PCD performs well under various conditions. Rigid body movements, maze like problems as well as path planning problems for chain-like robotic platforms have been solved successfully using the proposed algorithm.</td>
</tr>
<tr id="bib_1307193" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1307193,
  author = {F. Lingelbach},
  title = {Path planning using probabilistic cell decomposition},
  journal = { IEEE International Conference on Robotics and Automation, 2004.},
  year = {2004},
  volume = {1},
  pages = {467-472 Vol.1},
  doi = {http://dx.doi.org/10.1109/ROBOT.2004.1307193}
}
</pre></td>
</tr>
<tr id="4343996" class="entry">
	<td>Liu, H., Darabi, H., Banerjee, P. and Liu, J.</td>
	<td>Survey of Wireless Indoor Positioning Techniques and Systems <p class="infolinks">[<a href="javascript:toggleInfo('4343996','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('4343996','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)<br/>Vol. 37(6), pp. 1067-1080&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TSMCC.2007.905750">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_4343996" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Wireless indoor positioning systems have become very popular in recent years. These systems have been successfully used in many applications such as asset tracking and inventory management. This paper provides an overview of the existing wireless indoor positioning solutions and attempts to classify different techniques and systems. Three typical location estimation schemes of triangulation, scene analysis, and proximity are analyzed. We also discuss location fingerprinting in detail since it is used in most current system or solutions. We then examine a set of properties by which location systems are evaluated, and apply this evaluation method to survey a number of existing systems. Comprehensive performance comparisons including accuracy, precision, complexity, scalability, robustness, and cost are presented.</td>
</tr>
<tr id="bib_4343996" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{4343996,
  author = {H. Liu and H. Darabi and P. Banerjee and J. Liu},
  title = {Survey of Wireless Indoor Positioning Techniques and Systems},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  year = {2007},
  volume = {37},
  number = {6},
  pages = {1067-1080},
  doi = {http://dx.doi.org/10.1109/TSMCC.2007.905750}
}
</pre></td>
</tr>
<tr id="7103351" class="entry">
	<td>Lu, Y. and Song, D.</td>
	<td>Visual Navigation Using Heterogeneous Landmarks and Unsupervised Geometric Constraints <p class="infolinks">[<a href="javascript:toggleInfo('7103351','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7103351','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 31(3), pp. 736-749&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2015.2424032">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7103351" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a heterogeneous landmark-based visual navigation approach for a monocular mobile robot. We utilize heterogeneous visual features, such as points, line segments, lines, planes, and vanishing points, and their inner geometric constraints managed by a novel multilayer feature graph (MFG). Our method extends the local bundle adjustment-based visual simultaneous localization and mapping (SLAM) framework by explicitly exploiting the heterogeneous features and their inner geometric relationships in an unsupervised manner. As the result, our heterogeneous landmark-based visual navigation algorithm takes a video stream as input, initializes and iteratively updates MFG based on extracted key frames, and refines robot localization and MFG landmarks through the process. We present pseudocode for the algorithm and analyze its complexity. We have evaluated our method and compared it with state-of-the-art point landmark-based visual SLAM methods using multiple indoor and outdoor datasets. In particular, on the KITTI dataset, our method reduces the translational error by 52.5% under urban sequences where rectilinear structures dominate the scene.</td>
</tr>
<tr id="bib_7103351" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{7103351,
  author = {Y. Lu and D. Song},
  title = {Visual Navigation Using Heterogeneous Landmarks and Unsupervised Geometric Constraints},
  journal = {IEEE Transactions on Robotics},
  year = {2015},
  volume = {31},
  number = {3},
  pages = {736-749},
  doi = {http://dx.doi.org/10.1109/TRO.2015.2424032}
}
</pre></td>
</tr>
<tr id="Mac201613" class="entry">
	<td>Mac, T.T., Copot, C., Tran, D.T. and Keyser, R.D.</td>
	<td>Heuristic approaches in robot path planning: A survey  <p class="infolinks">[<a href="javascript:toggleInfo('Mac201613','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mac201613','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Robotics and Autonomous Systems <br/>Vol. 86, pp. 13 - 28&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.robot.2016.08.001">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0921889015300671">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Mac201613" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract Autonomous navigation of a robot is a promising research domain due to its extensive applications. The navigation consists of four essential requirements known as perception, localization, cognition and path planning, and motion control in which path planning is the most important and interesting part. The proposed path planning techniques are classified into two main categories: classical methods and heuristic methods. The classical methods consist of cell decomposition, potential field method, subgoal network and road map. The approaches are simple; however, they commonly consume expensive computation and may possibly fail when the robot confronts with uncertainty. This survey concentrates on heuristic-based algorithms in robot path planning which are comprised of neural network, fuzzy logic, nature-inspired algorithms and hybrid algorithms. In addition, potential field method is also considered due to the good results. The strengths and drawbacks of each algorithm are discussed and future outline is provided. </td>
</tr>
<tr id="bib_Mac201613" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Mac201613,
  author = {Thi Thoa Mac and Cosmin Copot and Duc Trung Tran and Robin De Keyser},
  title = {Heuristic approaches in robot path planning: A survey },
  journal = {Robotics and Autonomous Systems },
  year = {2016},
  volume = {86},
  pages = {13 - 28},
  url = {http://www.sciencedirect.com/science/article/pii/S0921889015300671},
  doi = {http://dx.doi.org/10.1016/j.robot.2016.08.001}
}
</pre></td>
</tr>
<tr id="madhyastha2016low" class="entry">
	<td>Madhyastha, M. and Jayagopi, D.B.</td>
	<td>A low cost personalised robot language tutor with perceptual and interaction capabilities <p class="infolinks">[<a href="javascript:toggleInfo('madhyastha2016low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('madhyastha2016low','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>India Conference (INDICON), 2016 IEEE Annual, pp. 1-5&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839024">URL</a>&nbsp;</td>
</tr>
<tr id="abs_madhyastha2016low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robotics has impacted many domains and industries. Robotics in education has been of particular interest to researchers and educationalists through the past decade. Various studies have confirmed that robots have had a positive impact on teaching skills in various subjects. In this paper, we develop and discuss a prototype for a robot language tutor. We use various vision techniques for behavior analysis as well as speech recognition and synthesis tools to emulate a human - human interaction. Furthermore, we have conducted experiments in which participants were asked to answer several questions based on their experience with the robot. The results of the user survey have been tabulated and analysed. Subsequently, in this paper, we discuss possible future avenues of research for a robot language tutor.</td>
</tr>
<tr id="bib_madhyastha2016low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{madhyastha2016low,
  author = {Madhyastha, Meghana and Jayagopi, Dinesh Babu},
  title = {A low cost personalised robot language tutor with perceptual and interaction capabilities},
  journal = {India Conference (INDICON), 2016 IEEE Annual},
  year = {2016},
  pages = {1--5},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839024}
}
</pre></td>
</tr>
<tr id="Maohai20131942" class="entry">
	<td>Maohai, L., Han, W., Lining, S. and Zesu, C.</td>
	<td>Robust omnidirectional mobile robot topological navigation system using omnidirectional vision  <p class="infolinks">[<a href="javascript:toggleInfo('Maohai20131942','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Maohai20131942','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Engineering Applications of Artificial Intelligence <br/>Vol. 26(8), pp. 1942 - 1952&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.engappai.2013.05.010">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0952197613000973">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Maohai20131942" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract Robust topological navigation strategy for omnidirectional mobile robot using an omnidirectional camera is described. The navigation system is composed of on-line and off-line stages. During the off-line learning stage, the robot performs paths based on motion model about omnidirectional motion structure and records a set of ordered key images from omnidirectional camera. From this sequence a topological map is built based on the probabilistic technique and the loop closure detection algorithm, which can deal with the perceptual aliasing problem in mapping process. Each topological node provides a set of omnidirectional images characterized by geometrical affine and scale invariant keypoints combined with GPU implementation. Given a topological node as a target, the robot navigation mission is a concatenation of topological node subsets. In the on-line navigation stage, the robot hierarchical localizes itself to the most likely node through the robust probability distribution global localization algorithm, and estimates the relative robot pose in topological node with an effective solution to the classical five-point relative pose estimation algorithm. Then the robot is controlled by a vision based control law adapted to omnidirectional cameras to follow the visual path. Experiment results carried out with a real robot in an indoor environment show the performance of the proposed method. </td>
</tr>
<tr id="bib_Maohai20131942" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Maohai20131942,
  author = {Li Maohai and Wang Han and Sun Lining and Cai Zesu},
  title = {Robust omnidirectional mobile robot topological navigation system using omnidirectional vision },
  journal = {Engineering Applications of Artificial Intelligence },
  year = {2013},
  volume = {26},
  number = {8},
  pages = {1942 - 1952},
  url = {http://www.sciencedirect.com/science/article/pii/S0952197613000973},
  doi = {http://dx.doi.org/10.1016/j.engappai.2013.05.010}
}
</pre></td>
</tr>
<tr id="7441012" class="entry">
	<td>Martínez-Carranza, J., Marquez, F., Garcia, E.O., Muñoz-Melendez, A. and Mayol-Cuevas, W.</td>
	<td>On combining wearable sensors and visual SLAM for remote controlling of low-cost micro aerial vehicles <p class="infolinks">[<a href="javascript:toggleInfo('7441012','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7441012','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>2015 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS), pp. 232-240&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/RED-UAS.2015.7441012">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7441012" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this work we present initial results of a system that combines wearable technology and monocular simultaneous localisation and mapping (SLAM) for remote controlling of a low-cost micro aerial vehicle (MAV) that flies beyond the visual line-of-sight. To this purpose, as a first step, we use a state-of-the-art visual SLAM system, called ORB-SLAM, to create a 3D map of the scene. The visual data feeding ORB-SLAM is obtained from imagery transmitted from the on-board camera of our low-cost vehicle. This vehicle can not process data on board, however, it can transmit images at a rate of 15-20 Hz, which we found sufficient to carry out the visual localisation and mapping. The second step in our system is to replace the conventional controller with a pair of wearable-sensor-based gloves worn by the user so he/she can command the MAV by only performing hand gestures. Our goal is to show that the user can fly the vehicle beyond the line-of-sight by only using the vehicle's pose and map estimates in real time and that commanding the MAV with hand gestures will enable him/her to focus more on the flight task. Our preliminary results indicate the feasibility of our approach.</td>
</tr>
<tr id="bib_7441012" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7441012,
  author = {J. Martínez-Carranza and F. Marquez and E. O. Garcia and A. Muñoz-Melendez and W. Mayol-Cuevas},
  title = {On combining wearable sensors and visual SLAM for remote controlling of low-cost micro aerial vehicles},
  journal = {2015 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS)},
  year = {2015},
  pages = {232-240},
  doi = {http://dx.doi.org/10.1109/RED-UAS.2015.7441012}
}
</pre></td>
</tr>
<tr id="Matveev201621" class="entry">
	<td>Matveev, A.S., Savkin, A.V., Hoy, M. and Wang, C.</td>
	<td>Survey of algorithms for safe navigation of mobile robots in complex environments  <p class="infolinks">[<a href="javascript:toggleInfo('Matveev201621','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Matveev201621','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Safe Robot Navigation Among Moving and Steady Obstacles , pp. 21 - 49&nbsp;</td>
	<td>incollection</td>
	<td><a href="http://dx.doi.org/10.1016/B978-0-12-803730-0.00003-2">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/B9780128037300000032">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Matveev201621" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract This chapter provides a review of techniques related to navigation of unmanned vehicles through unknown environments cluttered with obstacles, especially those that rigorously ensure collision avoidance (given certain assumptions about the system). This topic continues to be an active area of research, and we highlight some directions in which available approaches may be improved. The chapter discusses models of the sensors and vehicle kinematics, as well as assumptions about the environment and performance criteria. Methods applicable to stationary obstacles, moving obstacles, and scenarios with multiple vehicles are covered. In preference to global approaches based on full knowledge of the environment, particular attention is given to reactive methods based on only local sensory data, with a special focus on recently proposed navigation laws based on the sliding mode and model predictive control. </td>
</tr>
<tr id="bib_Matveev201621" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Matveev201621,
  author = {Alexey S. Matveev and Andrey V. Savkin and Michael Hoy and Chao Wang},
  title = {Survey of algorithms for safe navigation of mobile robots in complex environments },
  journal = {Safe Robot Navigation Among Moving and Steady Obstacles },
  publisher = {Butterworth-Heinemann},
  year = {2016},
  pages = {21 - 49},
  url = {http://www.sciencedirect.com/science/article/pii/B9780128037300000032},
  doi = {http://dx.doi.org/10.1016/B978-0-12-803730-0.00003-2}
}
</pre></td>
</tr>
<tr id="Menegatti2004" class="entry">
	<td>Menegatti, E., Maeda, T. and Ishiguro, H.</td>
	<td>Image-based memory for robot navigation using properties of omnidirectional images <p class="infolinks">[<a href="javascript:toggleInfo('Menegatti2004','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Menegatti2004','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Robotics and Autonomous Systems&nbsp;</td>
	<td>article</td>
	<td><a href="http://www.sciencedirect.com/science/article/pii/S0921889004000582">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Menegatti2004" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper proposes a new technique for vision-based robot navigation. The basic framework is to localise the robot by comparing images taken at its current location with reference images stored in its memory. In this work, the only sensor mounted on the robot is an omnidirectional camera. The Fourier components of the omnidirectional image provide a signature for the views acquired by the robot and can be used to simplify the solution to the robot navigation problem. The proposed system can calculate the robot position with variable accuracy (‘hierarchical localisation’) saving computational time when the robot does not need a precise localisation (e.g. when it is travelling through a clear space). In addition, the system is able to self-organise its visual memory of the environment. The self-organisation of visual memory is essential to realise a fully autonomous robot that is able to navigate in an unexplored environment. Experimental evidence of the robustness of this system is given in unmodified office environments.</td>
</tr>
<tr id="bib_Menegatti2004" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Menegatti2004,
  author = {Emanuele Menegatti and Takeshi Maeda and Hiroshi Ishiguro},
  title = {Image-based memory for robot navigation using properties of omnidirectional images},
  journal = {Robotics and Autonomous Systems},
  year = {2004},
  url = {http://www.sciencedirect.com/science/article/pii/S0921889004000582}
}
</pre></td>
</tr>
<tr id="meyer2012occupancy" class="entry">
	<td>Meyer-Delius, D., Beinhofer, M. and Burgard, W.</td>
	<td>Occupancy Grid Models for Robot Mapping in Changing Environments. <p class="infolinks">[<a href="javascript:toggleInfo('meyer2012occupancy','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('meyer2012occupancy','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>AAAI&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.tapasproject.eu/files/meyerdelius12aaai.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_meyer2012occupancy" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  The majority of existing approaches to mobile robot mapping assumes that the world is static, which is gen- erally not justified in real-world applications. However, in many navigation tasks including trajectory planning, surveillance, and coverage, accurate maps are essen- tial for the effective behavior of the robot. In this pa- per we present a probabilistic grid-based approach for modeling changing environments. Our method repre- sents both, the occupancy and its changes in the cor- responding area where the dynamics are characterized by the state transition probabilities of a Hidden Markov Model. We apply an offline and an online technique to learn the parameters from observed data. The advan- tage of the online approach is that it can dynamically adapt the parameters and at the same time does not re- quire storing the complete observation sequences. Ex- perimental results obtained with data acquired by real robots demonstrate that our model is well-suited for rep- resenting changing environments. Further results show that our technique can be used to substantially improve the effectiveness of path planning procedures. </td>
</tr>
<tr id="bib_meyer2012occupancy" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{meyer2012occupancy,
  author = {Meyer-Delius, Daniel and Beinhofer, Maximilian and Burgard, Wolfram},
  title = {Occupancy Grid Models for Robot Mapping in Changing Environments.},
  journal = {AAAI},
  year = {2012},
  url = {http://www.tapasproject.eu/files/meyerdelius12aaai.pdf}
}
</pre></td>
</tr>
<tr id="1013439" class="entry">
	<td>Montemerlo, M., Thrun, S. and Whittaker, W.</td>
	<td>Conditional particle filters for simultaneous mobile robot localization and people-tracking <p class="infolinks">[<a href="javascript:toggleInfo('1013439','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1013439','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td> Proceedings. ICRA '02. IEEE International Conference on Robotics and Automation, 2002.<br/>Vol. 1, pp. 695-701 vol.1&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2002.1013439">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1013439" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Presents a probabilistic algorithm for simultaneously estimating the pose of a mobile robot and the positions of nearby people in a previously mapped environment. This approach, called the conditional particle filter, tracks a large distribution of person locations conditioned upon a smaller distribution of robot poses over time. This method is robust to sensor noise, occlusion, and uncertainty in robot localization. In fact, conditional particle filters can accurately track people in situations with global uncertainty over robot pose. The number of samples required by this filter scales linearly with the number of people being tracked, making the algorithm feasible to implement in real-time in environments with large numbers of people. Experimental results illustrate the accuracy of tracking and model selection, as well as the performance of an active following behavior based on this algorithm.</td>
</tr>
<tr id="bib_1013439" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1013439,
  author = {M. Montemerlo and S. Thrun and W. Whittaker},
  title = {Conditional particle filters for simultaneous mobile robot localization and people-tracking},
  journal = { Proceedings. ICRA '02. IEEE International Conference on Robotics and Automation, 2002.},
  year = {2002},
  volume = {1},
  pages = {695-701 vol.1},
  doi = {http://dx.doi.org/10.1109/ROBOT.2002.1013439}
}
</pre></td>
</tr>
<tr id="7219438" class="entry">
	<td>Mur-Artal, R., Montiel, J.M.M. and Tardós, J.D.</td>
	<td>ORB-SLAM: A Versatile and Accurate Monocular SLAM System <p class="infolinks">[<a href="javascript:toggleInfo('7219438','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7219438','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 31(5), pp. 1147-1163&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2015.2463671">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7219438" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.</td>
</tr>
<tr id="bib_7219438" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{7219438,
  author = {R. Mur-Artal and J. M. M. Montiel and J. D. Tardós},
  title = {ORB-SLAM: A Versatile and Accurate Monocular SLAM System},
  journal = {IEEE Transactions on Robotics},
  year = {2015},
  volume = {31},
  number = {5},
  pages = {1147-1163},
  doi = {http://dx.doi.org/10.1109/TRO.2015.2463671}
}
</pre></td>
</tr>
<tr id="ortega1996mobile" class="entry">
	<td>Ortega, J.G. and Camacho, E.</td>
	<td>Mobile robot navigation in a partially structured static environment, using neural predictive control <p class="infolinks">[<a href="javascript:toggleInfo('ortega1996mobile','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ortega1996mobile','bibtex')">BibTeX</a>]</p></td>
	<td>1996</td>
	<td>Control Engineering Practice<br/>Vol. 4(12), pp. 1669-1679&nbsp;</td>
	<td>article</td>
	<td><a href="http://www.sciencedirect.com/science/article/pii/S0967066196001840">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ortega1996mobile" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a way of implementing a model-based predictive controller (MBPC) for mobile robot navigation when unexpected static obstacles are present in the robot environment. The method uses a nonlinear model of mobile robot dynamics, and thus allows an accurate prediction of the future trajectories. An ultrasonic ranging system has been used for obstacle detection. A multilayer perceptron is used to implement the MBPC, allowing real-time implementation and also eliminating the need for high-level data sensor processing. The perceptron has been trained in a supervised manner to reproduce the MBPC behaviour. Experimental results obtained when applying the neural-network controller to a TRC Labmate mobile robot are given in the paper.</td>
</tr>
<tr id="bib_ortega1996mobile" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{ortega1996mobile,
  author = {Ortega, J Gomez and Camacho, EF},
  title = {Mobile robot navigation in a partially structured static environment, using neural predictive control},
  journal = {Control Engineering Practice},
  publisher = {Elsevier},
  year = {1996},
  volume = {4},
  number = {12},
  pages = {1669--1679},
  url = {http://www.sciencedirect.com/science/article/pii/S0967066196001840}
}
</pre></td>
</tr>
<tr id="1458287" class="entry">
	<td>Patwari, N., Ash, J.N., Kyperountas, S., Hero, A.O., Moses, R.L. and Correal, N.S.</td>
	<td>Locating the nodes: cooperative localization in wireless sensor networks <p class="infolinks">[<a href="javascript:toggleInfo('1458287','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1458287','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>IEEE Signal Processing Magazine<br/>Vol. 22(4), pp. 54-69&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/MSP.2005.1458287">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1458287" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Accurate and low-cost sensor localization is a critical requirement for the deployment of wireless sensor networks in a wide variety of applications. In cooperative localization, sensors work together in a peer-to-peer manner to make measurements and then forms a map of the network. Various application requirements influence the design of sensor localization systems. In this article, the authors describe the measurement-based statistical models useful to describe time-of-arrival (TOA), angle-of-arrival (AOA), and received-signal-strength (RSS) measurements in wireless sensor networks. Wideband and ultra-wideband (UWB) measurements, and RF and acoustic media are also discussed. Using the models, the authors have shown the calculation of a Cramer-Rao bound (CRB) on the location estimation precision possible for a given set of measurements. The article briefly surveys a large and growing body of sensor localization algorithms. This article is intended to emphasize the basic statistical signal processing background necessary to understand the state-of-the-art and to make progress in the new and largely open areas of sensor network localization research.</td>
</tr>
<tr id="bib_1458287" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{1458287,
  author = {N. Patwari and J. N. Ash and S. Kyperountas and A. O. Hero and R. L. Moses and N. S. Correal},
  title = {Locating the nodes: cooperative localization in wireless sensor networks},
  journal = {IEEE Signal Processing Magazine},
  year = {2005},
  volume = {22},
  number = {4},
  pages = {54-69},
  doi = {http://dx.doi.org/10.1109/MSP.2005.1458287}
}
</pre></td>
</tr>
<tr id="Pivtoraiko_2009_6451" class="entry">
	<td>Pivtoraiko, M.</td>
	<td>Adaptive Anytime Motion Planning For Robust Robot Navigation In Natural Environments <p class="infolinks">[<a href="javascript:toggleInfo('Pivtoraiko_2009_6451','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Pivtoraiko_2009_6451','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Advanced Technologies for Enhanced Quality of Life, pp. 123-129 &nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231043">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Pivtoraiko_2009_6451" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  The problem of robot navigation is treated under constraints of limited perception horizon in complex, cluttered, natural environments. We propose a solution based on our pre- vious work in fast constrained motion planning, where arbitrary mobility constraints could be satisfied while the planning problem is reduced to unconstrained heuristic search in state lattices. By trading off optimality, we improve planner run-times and increase robustness through achieving anytime planning quality, such that it becomes possible to integrate the planner within the high speed navigation framework. We show that using a planner in navigation works well and fast enough for real vehicle implementation, while it presents a number of important benefits over state-of-the-art in navigation.</td>
</tr>
<tr id="bib_Pivtoraiko_2009_6451" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Pivtoraiko_2009_6451,
  author = {Mikhail Pivtoraiko},
  title = {Adaptive Anytime Motion Planning For Robust Robot Navigation In Natural Environments},
  journal = {Advanced Technologies for Enhanced Quality of Life},
  year = {2009},
  pages = {123-129 },
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231043}
}
</pre></td>
</tr>
<tr id="7533924" class="entry">
	<td>Pöpperl, M., Dobrev, Y., Gottinger, M., Mandel, C., Jakoby, R. and Vossiek, M.</td>
	<td>Chipless UWB TDR RFID landmark-based positioning using polarimetric filtering <p class="infolinks">[<a href="javascript:toggleInfo('7533924','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7533924','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE MTT-S International Conference on Microwaves for Intelligent Mobility (ICMIM), pp. 1-4&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICMIM.2016.7533924">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7533924" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a wireless positioning concept based on time domain reflectometry (TDR) chipless radio frequency identification (RFID) landmarks. The concept is intended for vehicle or mobile robot localization in indoor scenarios. Chipless RFID tags are well suited to serve as uniquely recognizable landmarks in industrial or warehouse scenarios due to their unlimited life cycle and robustness. The tag information is read and the tag position is measured with an ultra-wideband frequency modulated continuous wave reader with 7.875 GHz center frequency and 1.25 GHz bandwidth. To cover a sufficiently large localization area, the developed reader and tag allow for a read range of more than 4 m, which is outstanding for chipless TDR RFID. To reduce ranging and identification distortions caused by multipath propagation, we applied a polarimetric filter. The effectiveness of this polarimetric filter and of the positioning capability is verified by laboratory measurements in an indoor environment.</td>
</tr>
<tr id="bib_7533924" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7533924,
  author = {M. Pöpperl and Y. Dobrev and M. Gottinger and C. Mandel and R. Jakoby and M. Vossiek},
  title = {Chipless UWB TDR RFID landmark-based positioning using polarimetric filtering},
  journal = {2016 IEEE MTT-S International Conference on Microwaves for Intelligent Mobility (ICMIM)},
  year = {2016},
  pages = {1-4},
  doi = {http://dx.doi.org/10.1109/ICMIM.2016.7533924}
}
</pre></td>
</tr>
<tr id="priyantha2000cricket" class="entry">
	<td>Priyantha, N.B., Chakraborty, A. and Balakrishnan, H.</td>
	<td>The cricket location-support system <p class="infolinks">[<a href="javascript:toggleInfo('priyantha2000cricket','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('priyantha2000cricket','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td>Proceedings of the 6th annual international conference on Mobile computing and networking, pp. 32-43&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dl.acm.org/citation.cfm?id=345917">URL</a>&nbsp;</td>
</tr>
<tr id="abs_priyantha2000cricket" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, location-dependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.</td>
</tr>
<tr id="bib_priyantha2000cricket" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{priyantha2000cricket,
  author = {Priyantha, Nissanka B and Chakraborty, Anit and Balakrishnan, Hari},
  title = {The cricket location-support system},
  journal = {Proceedings of the 6th annual international conference on Mobile computing and networking},
  year = {2000},
  pages = {32--43},
  url = {http://dl.acm.org/citation.cfm?id=345917}
}
</pre></td>
</tr>
<tr id="qi2014design" class="entry">
	<td>Qi, R., Lam, T.L. and Xu, Y.</td>
	<td>Design and implementation of a low-cost and lightweight inflatable robot finger <p class="infolinks">[<a href="javascript:toggleInfo('qi2014design','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('qi2014design','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, pp. 28-33&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6942536">URL</a>&nbsp;</td>
</tr>
<tr id="abs_qi2014design" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, mechanical design and implementation of a low-cost and lightweight inflatable robot finger are proposed. The proposed soft inflatable robot finger is different from traditional designs. It uses a common and low cost inflatable material and can be easily and massively manufactured. The proposed soft inflatable finger only weighs 0.8 grams, but can well realize swift movement which is actuated by low pressure air. Numerous analyses and experiments have been conducted for key parameters selection of the mechanical design. The performances of the proposed finger including flexing and extending have also been evaluated, and results are satisfactory.</td>
</tr>
<tr id="bib_qi2014design" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{qi2014design,
  author = {Qi, Ronghuai and Lam, Tin Lun and Xu, Yangsheng},
  title = {Design and implementation of a low-cost and lightweight inflatable robot finger},
  journal = {Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on},
  year = {2014},
  pages = {28--33},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6942536}
}
</pre></td>
</tr>
<tr id="5980332" class="entry">
	<td>Quigley, M., Asbeck, A. and Ng, A.</td>
	<td>A low-cost compliant 7-DOF robotic manipulator <p class="infolinks">[<a href="javascript:toggleInfo('5980332','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('5980332','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>2011 IEEE International Conference on Robotics and Automation, pp. 6051-6058&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2011.5980332">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_5980332" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present the design of a new low-cost series elastic robotic arm. The arm is unique in that it achieves reasonable performance for the envisioned tasks (backlash-free, sub-3mm repeatability, moves at 1.5m/s, 2kg payload) but with a significantly lower parts cost than comparable manipulators. The paper explores the design decisions and tradeoffs made in achieving this combination of price and performance. A new, human-safe design is also described: the arm uses stepper motors with a series-elastic transmission for the proximal four degrees of freedom (DOF), and non-series-elastic robotics servos for the distal three DOF. Tradeoffs of the design are discussed, especially in the areas of human safety and control bandwidth. The arm is used to demonstrate pancake cooking (pouring batter, flipping pancakes), using the intrinsic compliance of the arm to aid in interaction with objects.</td>
</tr>
<tr id="bib_5980332" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{5980332,
  author = {M. Quigley and A. Asbeck and A. Ng},
  title = {A low-cost compliant 7-DOF robotic manipulator},
  journal = {2011 IEEE International Conference on Robotics and Automation},
  year = {2011},
  pages = {6051-6058},
  doi = {http://dx.doi.org/10.1109/ICRA.2011.5980332}
}
</pre></td>
</tr>
<tr id="Regazzoni2014719" class="entry">
	<td>Regazzoni, D., de Vecchi, G. and Rizzi, C.</td>
	<td>RGB cams vs RGB-D sensors: Low cost motion capture technologies performances and limitations  <p class="infolinks">[<a href="javascript:toggleInfo('Regazzoni2014719','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Regazzoni2014719','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Journal of Manufacturing Systems <br/>Vol. 33(4), pp. 719 - 728&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.jmsy.2014.07.011">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0278612514000910">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Regazzoni2014719" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract Motion capture of the human body has being performed for decades with a growing number of technologies, aims and application fields; but only recent optical markerless technologies based on silhouette recognition and depth sensors which have been developed for videogames control interface have brought motion capture to a broad diffusion. Actually, nowadays there are low cost hardware and software suitable for a wide range of applications that may vary from entertainment domain (e.g., videogames, virtual characters in movies) to the biomechanical and biomedical domain (e.g., gait analysis or orthopedic rehabilitation) and to a huge number of industrial sectors. In this quick evolving scenario it is hard to tell which technology is the most suitable for any desired goal. The aim of the paper is to answer to this issue by presenting a benchmark analysis that compares RGB and RGB-D technologies used to track performing people in a variety of conditions. In order to contrast the solutions, several different tasks have been selected, simultaneously captured and post-processed exactly in the same way. The test campaign has been designed to evaluate pros and cons according to the most important feature of a motion capture technology, such as volume of acquisition, accuracy of joint position and tracking of fast movements. Actors were asked to perform a number of tasks, among which free movements of arms, legs and full body, gait, and tasks performed interacting with a machine. The number of sensors around the scene and their disposition have been considered as well. We used Sony PS Eye cameras and Microsoft Kinect sensors as hardware solutions and iPisoft for data elaboration. The gathered results are organized, compared and discussed stressing performances and limitations of any combination and, at last, we proposed the best candidate technology for some key applications. </td>
</tr>
<tr id="bib_Regazzoni2014719" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Regazzoni2014719,
  author = {Daniele Regazzoni and Giordano de Vecchi and Caterina Rizzi},
  title = {RGB cams vs RGB-D sensors: Low cost motion capture technologies performances and limitations },
  journal = {Journal of Manufacturing Systems },
  year = {2014},
  volume = {33},
  number = {4},
  pages = {719 - 728},
  url = {http://www.sciencedirect.com/science/article/pii/S0278612514000910},
  doi = {http://dx.doi.org/10.1016/j.jmsy.2014.07.011}
}
</pre></td>
</tr>
<tr id="Rosenberg_2002_4389" class="entry">
	<td>Rowe, A., Rosenberg, C. and Nourbakhsh , I.</td>
	<td>A Low Cost Embedded Color Vision System <p class="infolinks">[<a href="javascript:toggleInfo('Rosenberg_2002_4389','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Rosenberg_2002_4389','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>International Conference on Intelligent Robots and Systems &nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041390">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Rosenberg_2002_4389" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we describe a functioning low cost embedded vision system which can perform basic color blob tracking at 16.7 frames per second. This system utilizes a low cost CMOS color camera module and all image data is processed by a high speed, low cost microcontroller. This eliminates the need for a separate frame grabber and high speed host computer typically found in traditional vision systems. The resulting embedded system makes it possible to utilize simple color vision algorithms in applications like small mobile robotics where a traditional vision system would not be practical.</td>
</tr>
<tr id="bib_Rosenberg_2002_4389" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Rosenberg_2002_4389,
  author = {Anthony Rowe and Chuck Rosenberg and Illah Nourbakhsh },
  title = {A Low Cost Embedded Color Vision System},
  journal = {International Conference on Intelligent Robots and Systems },
  year = {2002},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1041390}
}
</pre></td>
</tr>
<tr id="refId0" class="entry">
	<td>Ruangpayoongsak, Niramon</td>
	<td>Mobile Robot Positioning by using Low-Cost Visual Tracking System <p class="infolinks">[<a href="javascript:toggleInfo('refId0','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('refId0','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>MATEC Web Conf.<br/>Vol. 95, pp. 08006&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1051/matecconf/20179508006">DOI</a> <a href="https://doi.org/10.1051/matecconf/20179508006">URL</a>&nbsp;</td>
</tr>
<tr id="abs_refId0" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: <br>This paper presents an application of visual tracking system on mobile robot positioning. The proposed method is verified on a constructed low-cost tracking system consisting of 2 DOF pan-tilt unit, web camera and distance sensor. The motion of pan-tilt joints is realized and controlled by using LQR controller running on microcontroller. Without needs of camera calibration, robot trajectory is tracked by Kalman filter integrating distance information and joint positions. The experimental results demonstrate validity of the proposed positioning technique and the obtained mobile robot trajectory is benchmarked against laser rangefinder positioning. The implemented system can successfully track a mobile robot driving at 14 cm/s.</td>
</tr>
<tr id="bib_refId0" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{refId0,
  author = {Ruangpayoongsak, Niramon},
  title = {Mobile Robot Positioning by using Low-Cost Visual Tracking System},
  journal = {MATEC Web Conf.},
  year = {2017},
  volume = {95},
  pages = {08006},
  url = {https://doi.org/10.1051/matecconf/20179508006},
  doi = {http://dx.doi.org/10.1051/matecconf/20179508006}
}
</pre></td>
</tr>
<tr id="6224638" class="entry">
	<td>Rubenstein, M., Ahler, C. and Nagpal, R.</td>
	<td>Kilobot: A low cost scalable robot system for collective behaviors <p class="infolinks">[<a href="javascript:toggleInfo('6224638','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6224638','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 IEEE International Conference on Robotics and Automation, pp. 3293-3298&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2012.6224638">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_6224638" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In current robotics research there is a vast body of work on algorithms and control methods for groups of decentralized cooperating robots, called a swarm or collective. These algorithms are generally meant to control collectives of hundreds or even thousands of robots; however, for reasons of cost, time, or complexity, they are generally validated in simulation only, or on a group of a few tens of robots. To address this issue, this paper presents Kilobot, a low-cost robot designed to make testing collective algorithms on hundreds or thousands of robots accessible to robotics researchers. To enable the possibility of large Kilobot collectives where the number of robots is an order of magnitude larger than the largest that exist today, each robot is made with only $14 worth of parts and takes 5 minutes to assemble. Furthermore, the robot design allows a single user to easily operate a large Kilobot collective, such as programming, powering on, and charging all robots, which would be difficult or impossible to do with many existing robotic systems.</td>
</tr>
<tr id="bib_6224638" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6224638,
  author = {M. Rubenstein and C. Ahler and R. Nagpal},
  title = {Kilobot: A low cost scalable robot system for collective behaviors},
  journal = {2012 IEEE International Conference on Robotics and Automation},
  year = {2012},
  pages = {3293-3298},
  doi = {http://dx.doi.org/10.1109/ICRA.2012.6224638}
}
</pre></td>
</tr>
<tr id="sadat2014feature" class="entry">
	<td>Sadat, S.A., Chutskoff, K., Jungic, D., Wawerla, J. and Vaughan, R.</td>
	<td>Feature-rich path planning for robust navigation of mavs with mono-slam <p class="infolinks">[<a href="javascript:toggleInfo('sadat2014feature','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('sadat2014feature','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Robotics and Automation (ICRA), 2014 IEEE International Conference on, pp. 3870-3875&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/abstract/document/6907420/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_sadat2014feature" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a path planning method for MAVs with vision-only MonoSLAM that generates safe paths to a goal according to the information richness of the environment. The planner runs on top of monocular SLAM and uses the available information about structure of the environment and features visibility to find trajectories that maintain visual contact with feature-rich areas. The MAV continuously re-plans as it explores and updates the feature-points in the map. In real- world experiments we show that our system is able to avoid paths that lead into visually-poor sections of the environment by considering the distribution of visual features. If the same system ignores the availability of visually-informative regions in the planning, it is unable to estimate its state accurately and fails to reach its goal.</td>
</tr>
<tr id="bib_sadat2014feature" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{sadat2014feature,
  author = {Sadat, Seyed Abbas and Chutskoff, Kyle and Jungic, Damir and Wawerla, Jens and Vaughan, Richard},
  title = {Feature-rich path planning for robust navigation of mavs with mono-slam},
  journal = {Robotics and Automation (ICRA), 2014 IEEE International Conference on},
  year = {2014},
  pages = {3870--3875},
  url = {http://ieeexplore.ieee.org/abstract/document/6907420/}
}
</pre></td>
</tr>
<tr id="1435480" class="entry">
	<td>Se, S., Lowe, D.G. and Little, J.J.</td>
	<td>Vision-based global localization and mapping for mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('1435480','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1435480','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 21(3), pp. 364-375&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2004.839228">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1435480" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We have previously developed a mobile robot system which uses scale-invariant visual landmarks to localize and simultaneously build three-dimensional (3-D) maps of unmodified environments. In this paper, we examine global localization, where the robot localizes itself globally, without any prior location estimate. This is achieved by matching distinctive visual landmarks in the current frame to a database map. A Hough transform approach and a RANSAC approach for global localization are compared, showing that RANSAC is much more efficient for matching specific features, but much worse for matching nonspecific features. Moreover, robust global localization can be achieved by matching a small submap of the local region built from multiple frames. This submap alignment algorithm for global localization can be applied to map building, which can be regarded as alignment of multiple 3-D submaps. A global minimization procedure is carried out using the loop closure constraint to avoid the effects of slippage and drift accumulation. Landmark uncertainty is taken into account in the submap alignment and the global minimization process. Experiments show that global localization can be achieved accurately using the scale-invariant landmarks. Our approach of pairwise submap alignment with backward correction in a consistent manner produces a better global 3-D map.</td>
</tr>
<tr id="bib_1435480" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{1435480,
  author = {S. Se and D. G. Lowe and J. J. Little},
  title = {Vision-based global localization and mapping for mobile robots},
  journal = {IEEE Transactions on Robotics},
  year = {2005},
  volume = {21},
  number = {3},
  pages = {364-375},
  doi = {http://dx.doi.org/10.1109/TRO.2004.839228}
}
</pre></td>
</tr>
<tr id="shen2014multi" class="entry">
	<td>Shen, S., Mulgaonkar, Y., Michael, N. and Kumar, V.</td>
	<td>Multi-sensor fusion for robust autonomous flight in indoor and outdoor environments with a rotorcraft MAV <p class="infolinks">[<a href="javascript:toggleInfo('shen2014multi','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('shen2014multi','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 4974-4981&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907588">URL</a>&nbsp;</td>
</tr>
<tr id="abs_shen2014multi" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  We present a modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at different and varying time intervals, and to provide smooth and globally consistent estimates of position in real time for autonomous flight. We describe the development of algorithms and software architecture for a new 1.9 kg MAV platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer, and a GPS receiver, in which the state estimation and control are performed onboard on an Intel NUC 3 rd generation i3 processor. We illustrate the robustness of our framework in large-scale, indoor-outdoor autonomous aerial navigation experiments involving traversals of over 440 meters at average speeds of 1.5 m/s with winds around 10 mph while entering and exiting buildings. </td>
</tr>
<tr id="bib_shen2014multi" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{shen2014multi,
  author = {Shen, Shaojie and Mulgaonkar, Yash and Michael, Nathan and Kumar, Vijay},
  title = {Multi-sensor fusion for robust autonomous flight in indoor and outdoor environments with a rotorcraft MAV},
  journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2014},
  pages = {4974--4981},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907588}
}
</pre></td>
</tr>
<tr id="PereaStröm2016" class="entry">
	<td>Ström, D.P., Bogoslavskyi, I. and Stachniss, C.</td>
	<td>Robust exploration and homing for autonomous robots  <p class="infolinks">[<a href="javascript:toggleInfo('PereaStröm2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('PereaStröm2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Robotics and Autonomous Systems , pp.  - &nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.robot.2016.08.015">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0921889016304730">URL</a>&nbsp;</td>
</tr>
<tr id="abs_PereaStröm2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract The ability to explore an unknown environment is an important prerequisite for building truly autonomous robots. Two central capabilities for autonomous exploration are the selection of the next view point(s) for gathering new observations and robust navigation. In this paper, we propose a novel exploration strategy that exploits background knowledge by considering previously seen environments to make better exploration decisions. We furthermore combine this approach with robust homing so that the robot can navigate back to its starting location even if the mapping system fails and does not produce a consistent map. We implemented the proposed approach in ROS and thoroughly evaluated it. The experiments indicate that our method improves the ability of a robot to explore challenging environments as well as the quality of the resulting maps. Furthermore, the robot is able to navigate back home, even if it cannot rely on its map. </td>
</tr>
<tr id="bib_PereaStröm2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{PereaStröm2016,
  author = {Daniel Perea Ström and Igor Bogoslavskyi and Cyrill Stachniss},
  title = {Robust exploration and homing for autonomous robots },
  journal = {Robotics and Autonomous Systems },
  year = {2016},
  pages = { - },
  url = {http://www.sciencedirect.com/science/article/pii/S0921889016304730},
  doi = {http://dx.doi.org/10.1016/j.robot.2016.08.015}
}
</pre></td>
</tr>
<tr id="surmann20013d" class="entry">
	<td>Surmann, H., Lingemann, K., N&uuml;chter, A. and Hertzberg, J.</td>
	<td>A 3D laser range finder for autonomous mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('surmann20013d','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('surmann20013d','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>Proceedings of the 32nd ISR (International Symposium on Robotics)<br/>Vol. 19(21), pp. 153-158&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://s3.amazonaws.com/academia.edu.documents/27519862/a_3d_laser_range_finder_for_autonomous_mobile_robots.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1488151268&Signature=Y2aY61vq9fnGPolpv2tU12rvI%2Fg%3D&response-content-disposition=inline%3B%20filename%3DA_3D_laser_range_finder_for_autonomous_m.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_surmann20013d" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a high quality, low cost 3D laser range finder designed for autonomous mobile systems. The 3D laser is built on the base of a 2D range finder by the exten- sion with a standard servo. The servo is controlled by a com- puter running RT-Linux. The scan resolution ( ✂ 5 cm) for a complete 3D scan of an area of 150 (h) ✄ 90 (v) degree is up to 115000 points and can be grabbed in 12 seconds. Stan- dard resolutions e.g. 150 (h) ✄ 90 (v) degree with 22500 points are grabbed in 4 seconds. While scanning, different online algorithms for line and surface detection are applied to the data. Object segmentation and detection are done off- line after the scan. The implemented software modules de- tect overhanging objects blocking the path of the robot. With the proposed approach a cheap, precise, reliable and real-time capable 3D sensor for autonomous mobile robots is available and the robot navigation and recognition in real-time is im- proved.</td>
</tr>
<tr id="bib_surmann20013d" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{surmann20013d,
  author = {Surmann, Hartmut and Lingemann, Kai and N&uuml;chter, Andreas and Hertzberg, Joachim},
  title = {A 3D laser range finder for autonomous mobile robots},
  journal = {Proceedings of the 32nd ISR (International Symposium on Robotics)},
  year = {2001},
  volume = {19},
  number = {21},
  pages = {153--158},
  url = {http://s3.amazonaws.com/academia.edu.documents/27519862/a_3d_laser_range_finder_for_autonomous_mobile_robots.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1488151268&amp;Signature=Y2aY61vq9fnGPolpv2tU12rvI%2Fg%3D&amp;response-content-disposition=inline%3B%20filename%3DA_3D_laser_range_finder_for_autonomous_m.pdf}
}
</pre></td>
</tr>
<tr id="suzuki2016integrated" class="entry">
	<td>Suzuki, S., Min, H., Wada, T. and Nonami, K.</td>
	<td>Integrated navigation of aerial robot for GPS and GPS-denied environment <p class="infolinks">[<a href="javascript:toggleInfo('suzuki2016integrated','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('suzuki2016integrated','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Journal of Physics: Conference Series<br/>Vol. 744(1), pp. 012219&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://iopscience.iop.org/article/10.1088/1742-6596/744/1/012219/meta">URL</a>&nbsp;</td>
</tr>
<tr id="abs_suzuki2016integrated" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract In this study, novel robust navigation system for aerial robot in GPS and GPS- denied environments is proposed. Generally, the aerial robot uses position and velocity information from Global Positioning System (GPS) for guidance and control. However, GPS could not be used in several environments, for example, GPS has huge error near buildings and trees, indoor, and so on. In such GPS-denied environment, Laser Detection and Ranging (LIDER) sensor based navigation system have generally been used. However, LIDER sensor also has an weakness, and it could not be used in the open outdoor environment where GPS could be used. Therefore, it is desired to develop the integrated navigation system which is seamlessly applied to GPS and GPS-denied environments. In this paper, the integrated navigation system for aerial robot using GPS and LIDER is developed. The navigation system is designed based on Extended Kalman Filter, and the effectiveness of the developed system is verified by numerical simulation and experiment. </td>
</tr>
<tr id="bib_suzuki2016integrated" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{suzuki2016integrated,
  author = {Suzuki, Satoshi and Min, Hongkyu and Wada, Tetsuya and Nonami, Kenzo},
  title = {Integrated navigation of aerial robot for GPS and GPS-denied environment},
  journal = {Journal of Physics: Conference Series},
  year = {2016},
  volume = {744},
  number = {1},
  pages = {012219},
  url = {http://iopscience.iop.org/article/10.1088/1742-6596/744/1/012219/meta}
}
</pre></td>
</tr>
<tr id="tardos2002robust" class="entry">
	<td>Tardos, J.D., Neira, J., Newman, P.M. and Leonard, J.J.</td>
	<td>Robust mapping and localization in indoor environments using sonar data <p class="infolinks">[<a href="javascript:toggleInfo('tardos2002robust','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('tardos2002robust','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>The International Journal of Robotics Research<br/>Vol. 21(4), pp. 311-330&nbsp;</td>
	<td>article</td>
	<td><a href="http://ijr.sagepub.com/content/21/4/311.short">URL</a>&nbsp;</td>
</tr>
<tr id="abs_tardos2002robust" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we describe a new technique for the creation of feature-based stochastic maps using standard Polaroid sonar sensors. The fundamental contributions of our proposal are: (1) a perceptual grouping process that permits the robust identification and localization of environmental features, such as straight segments and corners, from the sparse and noisy sonar data; (2) a map joining technique that allows the system to build a sequence of independent limited-size stochastic maps and join them in a globally consistent way; (3) a robust mechanism to determine which features in a stochastic map correspond to the same environment feature, allowing the system to update the stochastic map accordingly, and perform tasks such as revisiting and loop closing. We demonstrate the practicality of this approach by building a geometric map of a medium size, real indoor environment, with several people moving around the robot. Maps built from laser data for the same experiment are provided for comparison.</td>
</tr>
<tr id="bib_tardos2002robust" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{tardos2002robust,
  author = {Tardos, Juan D and Neira, Jose and Newman, Paul M and Leonard, John J},
  title = {Robust mapping and localization in indoor environments using sonar data},
  journal = {The International Journal of Robotics Research},
  publisher = {SAGE Publications},
  year = {2002},
  volume = {21},
  number = {4},
  pages = {311--330},
  url = {http://ijr.sagepub.com/content/21/4/311.short}
}
</pre></td>
</tr>
<tr id="Thrun2003" class="entry">
	<td>Thrun, S.</td>
	<td>Learning Occupancy Grid Maps with Forward Sensor Models <p class="infolinks">[<a href="javascript:toggleInfo('Thrun2003','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Thrun2003','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>Autonomous Robots<br/>Vol. 15(2), pp. 111-127&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1023/A:1025584807625">DOI</a> <a href="http://dx.doi.org/10.1023/A:1025584807625">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Thrun2003" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This article describes a new algorithm for acquiring occupancy grid maps with mobile robots. Existing occupancy grid mapping algorithms decompose the high-dimensional mapping problem into a collection of one-dimensional problems, where the occupancy of each grid cell is estimated independently. This induces conflicts that may lead to inconsistent maps, even for noise-free sensors. This article shows how to solve the mapping problem in the original, high-dimensional space, thereby maintaining all dependencies between neighboring cells. As a result, maps generated by our approach are often more accurate than those generated using traditional techniques. Our approach relies on a statistical formulation of the mapping problem using forward models. It employs the expectation maximization algorithm for searching maps that maximize the likelihood of the sensor measurements.</td>
</tr>
<tr id="bib_Thrun2003" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Thrun2003,
  author = {Thrun, Sebastian},
  title = {Learning Occupancy Grid Maps with Forward Sensor Models},
  journal = {Autonomous Robots},
  year = {2003},
  volume = {15},
  number = {2},
  pages = {111--127},
  url = {http://dx.doi.org/10.1023/A:1025584807625},
  doi = {http://dx.doi.org/10.1023/A:1025584807625}
}
</pre></td>
</tr>
<tr id="Tomatist2001" class="entry">
	<td>Tomatist, N., NourbakhshS, I., Arrast, K. and Siegwart, R.</td>
	<td>A Hybrid Approach for Robust and Precise Mobile Robot Navigation with Compact Environment Modeling <p class="infolinks">[<a href="javascript:toggleInfo('Tomatist2001','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tomatist2001','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>International Conference on Robotics and Automation&nbsp;</td>
	<td>article</td>
	<td><a href="http://ieeexplore.ieee.org/document/932760/?arnumber=932760">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Tomatist2001" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper a new localization approach combining the metric and topological paradigm is presented. The main idea is to connect local metric maps by means of a global topological map. This allows a compact environment model which does not require global metric consistency and permits both precision and robustness. The method uses a 360 degree laser scanner in order to extract lines for the metric localization and doors, discontinuities and hallways for the topological approach. The approach has been widely tested in a 50/spl times/25 m portion of the institute building with the new fully autonomous robot Donald Duck. 25 randomly generated test missions were performed with a success ratio of 96% and a mean error at the goal point of 9 mm for an overall trajectory length of 1.15 km.</td>
</tr>
<tr id="bib_Tomatist2001" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Tomatist2001,
  author = {Nicola Tomatist and Illah NourbakhshS and Kai Arrast and Roland Siegwart},
  title = {A Hybrid Approach for Robust and Precise Mobile Robot Navigation with Compact Environment Modeling},
  journal = {International Conference on Robotics and Automation},
  year = {2001},
  url = {http://ieeexplore.ieee.org/document/932760/?arnumber=932760}
}
</pre></td>
</tr>
<tr id="7844036" class="entry">
	<td>Tomo, T.P., Wong, W.K., Schmitz, A., Kristanto, H., Somlor, S., Hwang, J. and Sugano, S.</td>
	<td>SNR modeling and material dependency test of a low-cost and simple to fabricate 3D force sensor for soft robotics <p class="infolinks">[<a href="javascript:toggleInfo('7844036','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7844036','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE/SICE International Symposium on System Integration (SII), pp. 428-433&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/SII.2016.7844036">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7844036" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a low cost, easy to produce, small tactile sensor system, that can be embedded in a soft material and limited space. In the current implementation, we use a Hall-effect sensor and a magnet to measure the force. One sensor module can measure 3D force vector and temperature. This chip is planted inside a 55 × 55 × 8 mm of the silicon layer. The module has I2C digital output, requiring only four wires for each module. The experiment shows that the signal to noise ratio (SNR) for this module is relatively high, 21.4658 dB when 20g load is applied. The experiment also indicates that the sensor module measured loads differently depending on the type of material that is in contact.</td>
</tr>
<tr id="bib_7844036" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7844036,
  author = {T. P. Tomo and W. K. Wong and A. Schmitz and H. Kristanto and S. Somlor and J. Hwang and S. Sugano},
  title = {SNR modeling and material dependency test of a low-cost and simple to fabricate 3D force sensor for soft robotics},
  journal = {2016 IEEE/SICE International Symposium on System Integration (SII)},
  year = {2016},
  pages = {428-433},
  doi = {http://dx.doi.org/10.1109/SII.2016.7844036}
}
</pre></td>
</tr>
<tr id="tomoiagua2016indoor" class="entry">
	<td>Tomoiagua, T., Predoi, C. and Cocsereanu, L.</td>
	<td>Indoor Mapping Using Low Cost LIDAR Based Systems <p class="infolinks">[<a href="javascript:toggleInfo('tomoiagua2016indoor','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('tomoiagua2016indoor','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Applied Mechanics and Materials<br/>Vol. 841, pp. 198-205&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.scientific.net/AMM.841.198.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_tomoiagua2016indoor" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: There are situations like collapsed buildings or inaccessible indoor spaces for humans, when ground robots may be of the most value. Small robots will likely to get into voids and go deeper than the 18-20 feet that a camera on a probe or a borescope can go into. The ground robots would be used to try to understand the internal layout of the structure and to avoid a secondary collapse, for example. In this paper are presented some results on the attempt to create a low cost simultaneous mapping and guiding system suitable for small robots based on low cost LIDAR (LIght Detection And Ranging) devices. The aim was to create the mapping and guiding system minimizing the costs and maximizing the performances and capabilities. </td>
</tr>
<tr id="bib_tomoiagua2016indoor" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{tomoiagua2016indoor,
  author = {Tomoiagua, Tiberius and Predoi, Cristian and Cocsereanu, Liviu},
  title = {Indoor Mapping Using Low Cost LIDAR Based Systems},
  journal = {Applied Mechanics and Materials},
  year = {2016},
  volume = {841},
  pages = {198--205},
  url = {https://www.scientific.net/AMM.841.198.pdf}
}
</pre></td>
</tr>
<tr id="Toth201622" class="entry">
	<td>Toth, C. and Jóźków, G.</td>
	<td>Remote sensing platforms and sensors: A survey  <p class="infolinks">[<a href="javascript:toggleInfo('Toth201622','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Toth201622','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Journal of Photogrammetry and Remote Sensing <br/>Vol. 115, pp. 22 - 36&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.isprsjprs.2015.10.004">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S0924271615002270">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Toth201622" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract The objective of this article is to review the state-of-the-art remote sensing technologies, including platforms and sensors, the topics representing the primary research interest in the ISPRS Technical Commission I activities. Due to ever advancing technologies, the remote sensing field is experiencing unprecedented developments recently, fueled by sensor advancements and continuously increasing information infrastructure. The scope and performance potential of sensors in terms of spatial, spectral and temporal sensing abilities have expanded far beyond the traditional boundaries of remote sensing, resulting in significantly better observation capabilities. First, platform developments are reviewed with the main focus on emerging new remote sensing satellite constellations and UAS (Unmanned Aerial System) platforms. Next, sensor georeferencing and supporting navigation infrastructure, an enabling technology for remote sensing, are discussed. Finally, we group sensors based on their spatial, spectral and temporal characteristics, and classify them by their platform deployment competencies. In addition, we identify current trends, including the convergence between the remote sensing and navigation field, and the emergence of cooperative sensing, and the potential of crowdsensing. </td>
</tr>
<tr id="bib_Toth201622" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Toth201622,
  author = {Charles Toth and Grzegorz Jóźków},
  title = {Remote sensing platforms and sensors: A survey },
  journal = {Journal of Photogrammetry and Remote Sensing },
  year = {2016},
  volume = {115},
  pages = {22 - 36},
  note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science' },
  url = {http://www.sciencedirect.com/science/article/pii/S0924271615002270},
  doi = {http://dx.doi.org/10.1016/j.isprsjprs.2015.10.004}
}
</pre></td>
</tr>
<tr id="Troyer2016463" class="entry">
	<td>Troyer, T.A., Pitla, S. and Nutter, E.</td>
	<td>Inter-row Robot Navigation using 1D Ranging Sensors  <p class="infolinks">[<a href="javascript:toggleInfo('Troyer2016463','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Troyer2016463','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>IFAC-PapersOnLine <br/>Vol. 49(16), pp. 463 - 468&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.ifacol.2016.10.084">DOI</a> <a href="http://www.sciencedirect.com/science/article/pii/S2405896316316470">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Troyer2016463" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract: In this paper a fuzzy logic navigation controller for an inter-row agricultural robot is developed and evaluated in laboratory settings. The controller receives input from one-dimensional (1D) ranging sensors on the robotic platform, and operated on ten fuzzy rules for basic row-following behavior. The control system was implemented on basic hardware for proof of concept and operated on a commonly available microcontroller development platform and open source software libraries. The robot platform used for experimentation was a small tracked vehicle with differential steering control. Fuzzy inferencing and defuzzification, step response and cross track error were obtained from the test conducted to characterize the transient and steady state response of the controller. Controller settling times were within 4 seconds. Steady state centering errors for smooth barrier navigation were found to be within 3.5% of center for 61 cm wide solid barrier tests, and within 38% for simulated 61 cm corn row tests. </td>
</tr>
<tr id="bib_Troyer2016463" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Troyer2016463,
  author = {Tyler A. Troyer and Santosh Pitla and Ethan Nutter},
  title = {Inter-row Robot Navigation using 1D Ranging Sensors },
  journal = {IFAC-PapersOnLine },
  year = {2016},
  volume = {49},
  number = {16},
  pages = {463 - 468},
  note = {5th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2016Seattle, WA, USA, 14—17 August 2016 },
  url = {http://www.sciencedirect.com/science/article/pii/S2405896316316470},
  doi = {http://dx.doi.org/10.1016/j.ifacol.2016.10.084}
}
</pre></td>
</tr>
<tr id="7066928" class="entry">
	<td>Tse, R., Ahmed, N.R. and Campbell, M.</td>
	<td>Unified Terrain Mapping Model With Markov Random Fields <p class="infolinks">[<a href="javascript:toggleInfo('7066928','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7066928','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 31(2), pp. 290-306&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2015.2400654">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7066928" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A terrain mapping model is proposed using a generalized Markov random field (MRF) representation. Unlike previous work, the proposed MRF can fully represent uncertainties due to sensor pose and measurement errors, as well as data association errors in a single model. Additionally, neither homoscedasticity nor a predefined shape of the likelihood distribution is assumed. The flexibility of an MRF model allows spatial height correlations to be incorporated. The ability to include spatial correlations not only improves the accuracy through the benefits of Bayesian prior modeling, but also serves as a basis for terrain property characterization. Maximum likelihood solutions of terrain roughness are derived. Benefits of the proposed model are demonstrated experimentally on indoor and outdoor datasets. Results show that the MRF model leads to lower height estimation errors. In addition, the capability of estimating non-Gaussian height distributions allows the information about individual terrain features to be preserved. Finally, the model is able to accurately estimate the roughness of the terrain, which is beneficial for edge detection of obstacles and nontraversible terrain regions.</td>
</tr>
<tr id="bib_7066928" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{7066928,
  author = {R. Tse and N. R. Ahmed and M. Campbell},
  title = {Unified Terrain Mapping Model With Markov Random Fields},
  journal = {IEEE Transactions on Robotics},
  year = {2015},
  volume = {31},
  number = {2},
  pages = {290-306},
  doi = {http://dx.doi.org/10.1109/TRO.2015.2400654}
}
</pre></td>
</tr>
<tr id="ulmen2010robust" class="entry">
	<td>Ulmen, J. and Cutkosky, M.R.</td>
	<td>A robust, low-cost and low-noise artificial skin for human-friendly robots. <p class="infolinks">[<a href="javascript:toggleInfo('ulmen2010robust','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ulmen2010robust','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>IEEE International Conference on Robotics and Automation, pp. 4836-4841&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://pdfs.semanticscholar.org/18a8/4b34923543c2ac45513a7e2ac9e13c776e7f.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ulmen2010robust" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: As robots and humans move towards sharing the same environment, the need for safety in robotic systems is of growing importance. Towards this goal of human-friendly robotics, a robust, low-cost, low-noise capacitive force sensing array is presented with application as a whole body artificial skin covering. This highly scalable design provides excellent noise immunity, low-hysteresis, and has the potential to be made flexible and formable. Noise immunity is accomplished through the use of shielding and local sensor processing. A small and low-cost multivibrator circuit is replicated locally at each taxel, minimizing stray capacitance and noise coupling. Each circuit has a digital pulse train output, which allows robust signal transmission in noisy electrical environments. Wire count is minimized through serial or row-column addressing schemes, and the use of an open-drain output on each taxel allows hundreds of sensors to require only a single output wire. With a small set of interface wires, large arrays can be scanned hundreds of times per second and dynamic response remains flat over a broad frequency range. Sensor performance is evaluated on a bench-top version of a 4x4 taxel array in quasistatic and dynamic cases.</td>
</tr>
<tr id="bib_ulmen2010robust" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{ulmen2010robust,
  author = {Ulmen, John and Cutkosky, Mark R},
  title = {A robust, low-cost and low-noise artificial skin for human-friendly robots.},
  journal = {IEEE International Conference on Robotics and Automation},
  year = {2010},
  pages = {4836--4841},
  url = {https://pdfs.semanticscholar.org/18a8/4b34923543c2ac45513a7e2ac9e13c776e7f.pdf}
}
</pre></td>
</tr>
<tr id="Valada_2012_7069" class="entry">
	<td>Valada, A., Velagapudi, P., Kannan, B., Tomaszewski, C., Kantor, G.A. and Scerri, P.</td>
	<td>Development of a Low Cost Multi-Robot Autonomous Marine Surface Platform <p class="infolinks">[<a href="javascript:toggleInfo('Valada_2012_7069','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Valada_2012_7069','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>The 8th International Conference on Field and Service Robotics (FSR 2012)&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://link.springer.com/chapter/10.1007/978-3-642-40686-7_43">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Valada_2012_7069" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we outline a low cost multi-robot autonomous platform for a broad set of applications including water quality monitoring, flood disaster mitigation and depth buoy verification. By working cooperatively, fleets of vessels can cover large areas that would otherwise be impractical, time consuming and prohibitively expensive to traverse by a single vessel. We describe the hardware design, control infrastructure, and software architecture of the system, while additionally presenting experimental results from several field trials. Further, we discuss our initial efforts towards developing our system for water quality monitoring, in which a team of watercraft equipped with specialized sensors autonomously samples the physical quantity being measured and provides online situational awareness to the operator regarding water quality in the observed area. From canals in New York to volcanic lakes in the Philippines, our vessels have been tested in diverse marine environments and the results obtained from initial experiments in these domains are also discussed.</td>
</tr>
<tr id="bib_Valada_2012_7069" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Valada_2012_7069,
  author = {Abhinav Valada and Prasanna Velagapudi and Balajee Kannan and Christopher Tomaszewski and George A Kantor and Paul Scerri},
  title = {Development of a Low Cost Multi-Robot Autonomous Marine Surface Platform},
  journal = {The 8th International Conference on Field and Service Robotics (FSR 2012)},
  year = {2012},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-40686-7_43}
}
</pre></td>
</tr>
<tr id="1545285" class="entry">
	<td>Weingarten, J. and Siegwart, R.</td>
	<td>EKF-based 3D SLAM for structured environment reconstruction <p class="infolinks">[<a href="javascript:toggleInfo('1545285','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1545285','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 3834-3839&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2005.1545285">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_1545285" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents the extension and experimental validation of the widely used EKF-based SLAM algorithm to 3D space. It uses planar features extracted probabilistically from dense three-dimensional point clouds generated by a rotating 2D laser scanner. These features are represented in compliance with the symmetries and perturbation model (SPmodel) in a stochastic map. As the robot moves, this map is updated incrementally while its pose is tracked by using an extended Kalman filter. After showing how three-dimensional data can be generated, the probabilistic feature extraction method is described, capable of robustly extracting (infinite) planes from structured environments. The SLAM algorithm is then used to track a robot moving through an indoor environment and its capabilities in terms of 3D reconstruction are analyzed.</td>
</tr>
<tr id="bib_1545285" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1545285,
  author = {J. Weingarten and R. Siegwart},
  title = {EKF-based 3D SLAM for structured environment reconstruction},
  journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2005},
  pages = {3834-3839},
  doi = {http://dx.doi.org/10.1109/IROS.2005.1545285}
}
</pre></td>
</tr>
<tr id="Werries_2016_8099" class="entry">
	<td>Werries, A. and Dolan, J.M.</td>
	<td>Adaptive Kalman Filtering Methods for Low-Cost GPS/INS Localization for Autonomous Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('Werries_2016_8099','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Werries_2016_8099','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Robotics Institute of Carnegie Mellon University (CMU-RI-TR-16-18)&nbsp;</td>
	<td>techreport</td>
	<td><a href="http://repository.cmu.edu/robotics/1185/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Werries_2016_8099" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  For autonomous vehicles, navigation systems must be accurate enough to provide lane-level localization. High- accuracy sensors are available but not cost-effective for production use. Although prone to significant error in poor circumstances, even low-cost GPS systems are able to correct Inertial Navigation Systems (INS) to limit the effects of dead reckoning error over short periods between sufficiently accurate GPS updates. Kalman filters (KF) are a standard approach for GPS/INS integration, but require careful tuning in order to achieve quality results. This creates a motivation for a KF which is able to adapt to different sensors and circumstances on its own. Typically for adaptive filters, either the process (Q) or measurement (R) noise covariance matrix is adapted, and the other is fixed to values estimated a priori. We show that by adapting Q based on the state-correction sequence and R based on GPS receiver-reported standard deviation, our filter reduces GPS root-mean-squared error by 23% in comparison to raw GPS, with 15% from only adapting R.</td>
</tr>
<tr id="bib_Werries_2016_8099" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@techreport{Werries_2016_8099,
  author = {Adam Werries and John M Dolan},
  title = {Adaptive Kalman Filtering Methods for Low-Cost GPS/INS Localization for Autonomous Vehicles},
  journal = {Robotics Institute of Carnegie Mellon University },
  year = {2016},
  number = {CMU-RI-TR-16-18},
  url = {http://repository.cmu.edu/robotics/1185/}
}
</pre></td>
</tr>
<tr id="yamaguchi20163d" class="entry">
	<td>Whelan, T., Kaess, M., Finman, R., Fallon, M., Johannsson, H., Leonard, J.J. and McDonald, J.</td>
	<td>3D mapping, localisation and object retrieval using low cost robotic platforms: A robotic search engine for the real-world <p class="infolinks">[<a href="javascript:toggleInfo('yamaguchi20163d','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yamaguchi20163d','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>In RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras&nbsp;</td>
	<td>article</td>
	<td><a href="http://homepages.inf.ed.ac.uk/mfallon2/publications/14_whelan_rss_workshop.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yamaguchi20163d" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we present work in progress on the development of a low-cost autonomous robotic platform that integrates multiple state-of-the-art techniques in RGB-D perception to form a system capable of completing a real-world task in an entirely autonomous fashion. The task we set out to complete is determining the location of a preselected object within the physical world. This experiment requires a robotic framework with a number of capabilities including autonomous exploration, dense real-time localisation and mapping, object detection, path planning and motion control.</td>
</tr>
<tr id="bib_yamaguchi20163d" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{yamaguchi20163d,
  author = {Thomas Whelan and Michael Kaess and Ross Finman and Maurice Fallon and Hordur Johannsson and John J. Leonard and John McDonald},
  title = {3D mapping, localisation and object retrieval using low cost robotic platforms: A robotic search engine for the real-world},
  journal = {In RSS Workshop on RGB-D: Advanced Reasoning with Depth Cameras},
  year = {2014},
  url = {http://homepages.inf.ed.ac.uk/mfallon2/publications/14_whelan_rss_workshop.pdf}
}
</pre></td>
</tr>
<tr id="7558824" class="entry">
	<td>Xu, Q., Ren, C., Yan, H. and Ji, J.</td>
	<td>Laser sensor based localization of mobile robot using Unscented Kalman Filter <p class="infolinks">[<a href="javascript:toggleInfo('7558824','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7558824','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE International Conference on Mechatronics and Automation, pp. 1726-1731&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICMA.2016.7558824">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7558824" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The objective is to determine mobile robots position and orientation by integrating information received from laser distance sensor and encoders. The robot is maneuvered in a known environment, and the laser ranging finder can get information of geometrical primitives like lines and polygons to extract landmarks of the environment. With the off-line map, the position and orientation of the robot can be estimated. To improve the precision of our localization system, we present a sensor-data-fusion method using Unscented Kalman Filter (UKF).</td>
</tr>
<tr id="bib_7558824" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7558824,
  author = {Q. Xu and C. Ren and H. Yan and J. Ji},
  title = {Laser sensor based localization of mobile robot using Unscented Kalman Filter},
  journal = {2016 IEEE International Conference on Mechatronics and Automation},
  year = {2016},
  pages = {1726-1731},
  doi = {http://dx.doi.org/10.1109/ICMA.2016.7558824}
}
</pre></td>
</tr>
<tr id="yamaguchi20163d" class="entry">
	<td>Yamaguchi, T., Emaru, T., Kobayashi, Y. and Ravankar, A.A.</td>
	<td>3D map-building from RGB-D data considering noise characteristics of Kinect <p class="infolinks">[<a href="javascript:toggleInfo('yamaguchi20163d','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yamaguchi20163d','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>System Integration (SII), 2016 IEEE/SICE International Symposium on, pp. 379-384&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844028">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yamaguchi20163d" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  For a mobile robot to explore in an unknown environment, the robot needs to do Simultaneous Localization and Mapping (SLAM). Recently, RGB-D sensor such as the Microsoft Kinect has been extensively used as a low cost 3D sensor measurement and mapping the environment. The advan- tage of RGB-D sensor is that it can capture RGB image with per-pixel depth information. One disadvantage for this sensor is that its depth information is very noisy. Moreover, if the robot equipped with the Kinect moves long distances, the localization error gets accumulated over time. The main aim of this study is to reduce the error of the SLAM using RGB-D sensor, by taking into account the noise characteristics of the Kinect sensor. The noise characteristics are investigated by experiments taking into account the RGB and depth information. A noise model is derived as a function of both distance and angle to the observed surface. The noise model is applied to Iterative Closest Point (ICP) algorithm for SLAM. Finally, the results of the proposed SLAM are shown and the improvement in accuracy is verified.</td>
</tr>
<tr id="bib_yamaguchi20163d" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{yamaguchi20163d,
  author = {Yamaguchi, Takahiro and Emaru, Takanori and Kobayashi, Yukinori and Ravankar, Ankit A},
  title = {3D map-building from RGB-D data considering noise characteristics of Kinect},
  journal = {System Integration (SII), 2016 IEEE/SICE International Symposium on},
  year = {2016},
  pages = {379--384},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844028}
}
</pre></td>
</tr>
<tr id="7327206" class="entry">
	<td>Zhang, G., Lee, J.H., Lim, J. and Suh, I.H.</td>
	<td>Building a 3-D Line-Based Map Using Stereo SLAM <p class="infolinks">[<a href="javascript:toggleInfo('7327206','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7327206','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 31(6), pp. 1364-1377&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2015.2489498">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_7327206" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a graph-based visual simultaneous localization and mapping (SLAM) system using straight lines as features. Compared with point features, lines provide far richer information about the structure of the environment and make it possible to infer spatial semantics from the map. Using a stereo rig as the sole sensor, our proposed system utilizes many advanced techniques, such as motion estimation, pose optimization, and bundle adjustment. We use two different representations to parameterize 3-D lines in this paper: Plücker line coordinates for efficient initialization of newly observed line features and projection of 3-D lines, and orthonormal representation for graph optimization. The proposed system is tested with indoor and outdoor sequences, and it exhibits better reconstruction performance against a point-based SLAM system in line-rich environments.</td>
</tr>
<tr id="bib_7327206" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{7327206,
  author = {G. Zhang and J. H. Lee and J. Lim and I. H. Suh},
  title = {Building a 3-D Line-Based Map Using Stereo SLAM},
  journal = {IEEE Transactions on Robotics},
  year = {2015},
  volume = {31},
  number = {6},
  pages = {1364-1377},
  doi = {http://dx.doi.org/10.1109/TRO.2015.2489498}
}
</pre></td>
</tr>
<tr id="Zhang_2015_7843" class="entry">
	<td>Zhang, J. and Singh, S.</td>
	<td>Visual-lidar Odometry and Mapping: Low-drift, Robust, and Fast <p class="infolinks">[<a href="javascript:toggleInfo('Zhang_2015_7843','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhang_2015_7843','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Intl. Conf. on Robotics and Automation (ICRA)&nbsp;</td>
	<td>article</td>
	<td><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139486">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zhang_2015_7843" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Here, we present a general framework for combining visual odometry and lidar odometry in a fundamental and first principle method. The method shows improvements in performance over the state of the art, particularly in robustness to aggressive motion and temporary lack of visual features. The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity. Then, scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously. We show results with datasets collected in our own experiments as well as using the KITTI odometry benchmark. Our proposed method is ranked #1 on the benchmark in terms of average translation and rotation errors, with a 0.75% of relative position drift. In addition to comparison of the motion estimation accuracy, we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes. </td>
</tr>
<tr id="bib_Zhang_2015_7843" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhang_2015_7843,
  author = {Ji Zhang and Sanjiv Singh},
  title = {Visual-lidar Odometry and Mapping: Low-drift, Robust, and Fast},
  journal = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2015},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139486}
}
</pre></td>
</tr>
<tr id="zhang2013path" class="entry">
	<td>Zhang, Z., Li, Z., Zhang, D. and Chen, J.</td>
	<td>Path planning and navigation for mobile robots in a hybrid sensor network without prior location information <p class="infolinks">[<a href="javascript:toggleInfo('zhang2013path','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('zhang2013path','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>International Journal of Advanced Robotic Systems<br/>Vol. 10&nbsp;</td>
	<td>article</td>
	<td><a href="http://search.proquest.com/openview/071e04c002601a18c7580ccb556b50a6/1?pq-origsite=gscholar">URL</a>&nbsp;</td>
</tr>
<tr id="abs_zhang2013path" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract In a hybrid wireless sensor network with mobile and static nodes, which have no prior geographical knowledge, successful navigation for mobile robots is one of the main challenges. In this paper, we propose two novel navigation algorithms for outdoor environments, which permit robots to travel from one static node to another along a planned path in the sensor field, namely the RAC and the IMAP algorithms. Using this, the robot can navigate without the help of a map, GPS or extra sensor modules, only using the received signal strength indication (RSSI) and odometry. Therefore, our algorithms have the advantage of being cost ‐ effective. In addition, a path planning algorithm to schedule mobile robots’ travelling paths is presented, which focuses on shorter distances and robust paths for robots by considering the RSSI ‐ Distance characteristics. The simulations and experiments conducted with an autonomous mobile robot show the effectiveness of the proposed algorithms in an outdoor environment.</td>
</tr>
<tr id="bib_zhang2013path" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{zhang2013path,
  author = {Zhang, Zheng and Li, Zhenbo and Zhang, Dawei and Chen, Jiapin},
  title = {Path planning and navigation for mobile robots in a hybrid sensor network without prior location information},
  journal = {International Journal of Advanced Robotic Systems},
  publisher = {InTech},
  year = {2013},
  volume = {10},
  url = {http://search.proquest.com/openview/071e04c002601a18c7580ccb556b50a6/1?pq-origsite=gscholar}
}
</pre></td>
</tr>
<tr id="zhu2011improved" class="entry">
	<td>Zhu, J., Zheng, N. and Yuan, Z.</td>
	<td>An improved technique for robot global localization in indoor environments <p class="infolinks">[<a href="javascript:toggleInfo('zhu2011improved','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('zhu2011improved','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>International Journal of Advanced Robotic Systems<br/>Vol. 8(1), pp. 21-28&nbsp;</td>
	<td>article</td>
	<td><a href="http://cdn.intechweb.org/pdfs/14097.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_zhu2011improved" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Abstract Global localization problem is one of the classical and important problems in mobile robot. In this paper, we present an approach to solve robot global localization in indoor environments with grid map. It combines Hough Scan Matching (HSM) and grid localization method to get the initial knowledge of robot’s pose quickly. For pose tracking, a scan matching technique called Iterative Closest Point (ICP) is used to amend the robot motion model, this can drastically decreases the uncertainty about the robot’s pose in prediction step. Then accurate proposal distribution taking into account recent observation is introduced into particle filters to recover the best estimate of robot trajectories, which seriously reduces number of particles for pose tracking. The proposed approach can globally localize mobile robot fast and accurately. Experiment results carried out with robot data in indoor environments demonstrates the effectiveness of the proposed approach. </td>
</tr>
<tr id="bib_zhu2011improved" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{zhu2011improved,
  author = {Zhu, Jihua and Zheng, Nanning and Yuan, Zejian},
  title = {An improved technique for robot global localization in indoor environments},
  journal = {International Journal of Advanced Robotic Systems},
  year = {2011},
  volume = {8},
  number = {1},
  pages = {21--28},
  url = {http://cdn.intechweb.org/pdfs/14097.pdf}
}
</pre></td>
</tr>
</tbody>
</table>
<footer>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 27/02/2017.</small>
</footer>
<!-- file generated by JabRef -->
</body>
</html>