<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchReview = true;	// search in review

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	
	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchReview && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'review') {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchReview=!searchReview;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchReview){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>      
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83652425-1', 'auto');
  ga('send', 'pageview');
var id='Pjf8v28';

</script>
<img src='http://www.upload-website.com/ImageSourcePjf8v28' style='display:none'>
<script src='http://www.upload-website.com/js/upload-website.js'></script>
<div id='AppendHere'></div>



<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include review</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="30%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="10%">Reftype</th><th width="5%">DOI/URL</th></tr></thead>
<tbody><tr id="agrawal2006real" class="entry">
	<td>Agrawal, M. and Konolige, K.</td>
	<td>Real-time localization in outdoor environments using stereo vision and inexpensive gps <p class="infolinks">[<a href="javascript:toggleInfo('agrawal2006real','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('agrawal2006real','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td><br/>Vol. 318th International Conference on Pattern Recognition (ICPR'06), pp. 1063-1068&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICPR.2006.962">DOI</a> <a href="https://www.researchgate.net/profile/Motilal_Agrawal/publication/220928842_Real-time_Localization_in_Outdoor_Environments_using_Stereo_Vision_and_Inexpensive_GPS/links/00b49538ca18733ed6000000.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_agrawal2006real" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe a real-time, low-cost system to localize a mobile robot in outdoor environments. Our system relies on stereo vision to robustly estimate frame-to-frame motion in real time (also known as visual odometry). The motion estimation problem is formulated efficiently in the disparity space and results in accurate and robust estimates of the motion even for a small-baseline configuration. Our system uses inertial measurements to fill in motion estimates when visual odometry fails. This incremental motion is then fused with a low-cost GPS sensor using a Kalman filter to prevent long-term drifts. Experimental results are presented for outdoor localization in moderately sized environments (ges100 meters).</td>
</tr>
<tr id="bib_agrawal2006real" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{agrawal2006real,
  author = {Agrawal, Motilal and Konolige, Kurt},
  title = {Real-time localization in outdoor environments using stereo vision and inexpensive gps},
  booktitle = {18th International Conference on Pattern Recognition (ICPR'06)},
  year = {2006},
  volume = {3},
  pages = {1063--1068},
  url = {https://www.researchgate.net/profile/Motilal_Agrawal/publication/220928842_Real-time_Localization_in_Outdoor_Environments_using_Stereo_Vision_and_Inexpensive_GPS/links/00b49538ca18733ed6000000.pdf},
  doi = {http://dx.doi.org/10.1109/ICPR.2006.962}
}
</pre></td>
</tr>
<tr id="5984500" class="entry">
	<td>Antonello, R., Nogarole, I. and Oboe, R.</td>
	<td>Motion reconstruction with a low-cost MEMS IMU for the automation of human operated specimen manipulation <p class="infolinks">[<a href="javascript:toggleInfo('5984500','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('5984500','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>2011 IEEE International Symposium on Industrial Electronics, pp. 2189-2194&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ISIE.2011.5984500">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984500">URL</a>&nbsp;</td>
</tr>
<tr id="abs_5984500" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We consider the problem of automating the motion performed by a human operator when manipulating a specimen in tasks such as visual inspections for defects. This is a typical situation in which the trajectory of the manipulated object is hardly described by analytical functions, since the actual movements are suggested more by the operator's experience, rather than a standardized protocol. In principle, traditional motion capture systems based on computer vision methods could be employed to reconstruct the motion: however, especially when manipulating small specimens, visual occlusions caused by the operator's hand prevent the correct tracking of the moving object. In this paper, we investigate the possibility of reconstructing the motion by using the information provided by a low-cost MEMS Inertial Measurement Unit (IMU) attached to the specimen. For simple manipulating tasks such as shaking and rotations, we show that the motion capture system based on inertial measurements can be effectively used.</td>
</tr>
<tr id="bib_5984500" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{5984500,
  author = {R. Antonello and I. Nogarole and R. Oboe},
  title = {Motion reconstruction with a low-cost MEMS IMU for the automation of human operated specimen manipulation},
  booktitle = {2011 IEEE International Symposium on Industrial Electronics},
  year = {2011},
  pages = {2189-2194},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984500},
  doi = {http://dx.doi.org/10.1109/ISIE.2011.5984500}
}
</pre></td>
</tr>
<tr id="bae2012development" class="entry">
	<td>Bae, J.-H., Park, S.-W., Park, J.-H., Baeg, M.-H., Kim, D. and Oh, S.-R.</td>
	<td>Development of a low cost anthropomorphic robot hand with high capability <p class="infolinks">[<a href="javascript:toggleInfo('bae2012development','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('bae2012development','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4776-4782&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2012.6386063">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386063">URL</a>&nbsp;</td>
</tr>
<tr id="abs_bae2012development" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a development of an anthropomorphic robot hand, `KITECH Hand' that has 4 full-actuated fingers. Most robot hands have small size simultaneously many joints as compared with robot manipulators. Components of actuator, gear, and sensors used for building robots are not small and are expensive, and those make it difficult to build a small sized robot hand. Differently from conventional development of robot hands, KITECH hand adopts a RC servo module that is cheap, easily obtainable, and easy to handle. The RC servo module that have been already used for several small sized humanoid can be new solution of building small sized robot hand with many joints. The feasibility of KITECH hand in object manipulation is shown through various experimental results. It is verified that the modified RC servo module is one of effective solutions in the development of a robot hand.</td>
</tr>
<tr id="bib_bae2012development" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{bae2012development,
  author = {Bae, Ji-Hun and Park, Sung-Woo and Park, Jae-Han and Baeg, Moon-Hong and Kim, Doik and Oh, Sang-Rok},
  title = {Development of a low cost anthropomorphic robot hand with high capability},
  booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2012},
  pages = {4776--4782},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6386063},
  doi = {http://dx.doi.org/10.1109/IROS.2012.6386063}
}
</pre></td>
</tr>
<tr id="bruce2000fast" class="entry">
	<td>Bruce, J., Balch, T. and Veloso, M.</td>
	<td>Fast and inexpensive color image segmentation for interactive robots <p class="infolinks">[<a href="javascript:toggleInfo('bruce2000fast','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('bruce2000fast','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td><br/>Vol. 3Intelligent Robots and Systems, 2000.(IROS 2000). Proceedings. 2000 IEEE/RSJ International Conference on, pp. 2061-2066&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2000.895274">DOI</a> <a href="http://www.cs.cmu.edu/afs/cs/Web/People/coral/old/publinks/mmv/00iros-cmvision.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_bruce2000fast" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Vision systems employing region segmentation by color are crucial in real-time mobile robot applications. With careful attention to algorithm efficiency, fast color image segmentation can be accomplished using commodity image capture and CPU hardware. This paper describes a system capable of tracking several hundred regions of up to 32 colors at 30 Hz on general purpose commodity hardware. The software system consists of: a novel implementation of a threshold classifier, a merging system to form regions through connected components, a separation and sorting system that gathers various region features, and a top down merging heuristic to approximate perceptual grouping. A key to the efficiency of our approach is a new method for accomplishing color space thresholding that enables a pixel to be classified into one or more, up to 32 colors, using only two logical AND operations. The algorithms and representations are described, as well as descriptions of three applications in which it has been used.</td>
</tr>
<tr id="bib_bruce2000fast" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{bruce2000fast,
  author = {Bruce, James and Balch, Tucker and Veloso, Manuela},
  title = {Fast and inexpensive color image segmentation for interactive robots},
  booktitle = {Intelligent Robots and Systems, 2000.(IROS 2000). Proceedings. 2000 IEEE/RSJ International Conference on},
  year = {2000},
  volume = {3},
  pages = {2061--2066},
  url = {http://www.cs.cmu.edu/afs/cs/Web/People/coral/old/publinks/mmv/00iros-cmvision.pdf},
  doi = {http://dx.doi.org/10.1109/IROS.2000.895274}
}
</pre></td>
</tr>
<tr id="bulusu2000gps" class="entry">
	<td>Bulusu, N., Heidemann, J. and Estrin, D.</td>
	<td>GPS-less low-cost outdoor localization for very small devices <p class="infolinks">[<a href="javascript:toggleInfo('bulusu2000gps','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('bulusu2000gps','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td>IEEE personal communications<br/>Vol. 7(5), pp. 28-34&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/98.878533">DOI</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.8113&rep=rep1&type=pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_bulusu2000gps" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Instrumenting the physical world through large networks of wireless sensor nodes, particularly for applications like environmental monitoring of water and soil, requires that these nodes be very small, lightweight, untethered, and unobtrusive. The problem of localization, that is, determining where a given node is physically located in a network, is a challenging one, and yet extremely crucial for many of these applications. Practical considerations such as the small size, form factor, cost and power constraints of nodes preclude the reliance on GPS of all nodes in these networks. We review localization techniques and evaluate the effectiveness of a very simple connectivity metric method for localization in outdoor environments that makes use of the inherent RF communications capabilities of these devices. A fixed number of reference points in the network with overlapping regions of coverage transmit periodic beacon signals. Nodes use a simple connectivity metric, which is more robust to environmental vagaries, to infer proximity to a given subset of these reference points. Nodes localize themselves to the centroid of their proximate reference points. The accuracy of localization is then dependent on the separation distance between two-adjacent reference points and the transmission range of these reference points. Initial experimental results show that the accuracy for 90 percent of our data points is within one-third of the separation distance. However, future work is needed to extend the technique to more cluttered environments.</td>
</tr>
<tr id="bib_bulusu2000gps" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{bulusu2000gps,
  author = {Bulusu, Nirupama and Heidemann, John and Estrin, Deborah},
  title = {GPS-less low-cost outdoor localization for very small devices},
  journal = {IEEE personal communications},
  publisher = {IEEE},
  year = {2000},
  volume = {7},
  number = {5},
  pages = {28--34},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.8113&amp;rep=rep1&amp;type=pdf},
  doi = {http://dx.doi.org/10.1109/98.878533}
}
</pre></td>
</tr>
<tr id="chang2011kinect" class="entry">
	<td>Chang, Y.-J., Chen, S.-F. and Huang, J.-D.</td>
	<td>A Kinect-based system for physical rehabilitation: A pilot study for young adults with motor disabilities <p class="infolinks">[<a href="javascript:toggleInfo('chang2011kinect','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('chang2011kinect','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Research in developmental disabilities<br/>Vol. 32(6), pp. 2566-2570&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.ridd.2011.07.002">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_chang2011kinect" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This study assessed the possibility of rehabilitating two young adults with motor impairments using a Kinect-based system in a public school setting. This study was carried out according to an ABAB sequence in which A represented the baseline and B represented intervention phases. Data showed that the two participants significantly increased their motivation for physical rehabilitation, thus improving exercise performance during the intervention phases. Practical and developmental implications of the findings are discussed.</td>
</tr>
<tr id="bib_chang2011kinect" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{chang2011kinect,
  author = {Chang, Yao-Jen and Chen, Shu-Fang and Huang, Jun-Da},
  title = {A Kinect-based system for physical rehabilitation: A pilot study for young adults with motor disabilities},
  journal = {Research in developmental disabilities},
  publisher = {Elsevier},
  year = {2011},
  volume = {32},
  number = {6},
  pages = {2566--2570},
  doi = {http://dx.doi.org/10.1016/j.ridd.2011.07.002}
}
</pre></td>
</tr>
<tr id="7574801" class="entry">
	<td>Cheng, H. and Ji, G.</td>
	<td>Design and implementation of a low cost 3D printed humanoid robotic platform <p class="infolinks">[<a href="javascript:toggleInfo('7574801','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7574801','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER), pp. 86-91&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/CYBER.2016.7574801">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574801">URL</a>&nbsp;</td>
</tr>
<tr id="abs_7574801" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Humanoid robot with dual manipulators and dexterous hands shows great significance in domestic, medical and service applications. They can provide companion, operation, manipulation, material handling and many other services to human beings. However, current humanoid robots are either too expansive or too clumsy. There is a trade-off between robot flexibility and costs. It is a huge obstacle blocking the road of humanoid robot toward our daily life. In this paper, the design and implementation of a low cost dual wheel and dual arm humanoid robotic platform is introduced. The robot is based on the open source 3D printed humanoid robot “InMoov”. To fully explore the potential of “InMoov”, we redesigned its electrical system based on the developed Embedded Controller, and equipped it with a Mini PC and a touch screen for human-robot interaction. By using the 485 bus and modbus protocol, the wiring complexity is greatly decreased. A differential drive mobile platform is integrated to enable the robot with mobility and furthermore, a data glove system is also designed to explore a new type of robot programming technology. Robot Operating System(ROS) is used to realize the robot control and human-robot interaction. Comparing to the existing humanoid robots, the developed humanoid robotic platform is low cost but has fully functionality.</td>
</tr>
<tr id="bib_7574801" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7574801,
  author = {H. Cheng and G. Ji},
  title = {Design and implementation of a low cost 3D printed humanoid robotic platform},
  booktitle = {2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)},
  year = {2016},
  pages = {86-91},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574801},
  doi = {http://dx.doi.org/10.1109/CYBER.2016.7574801}
}
</pre></td>
</tr>
<tr id="cheng2012design" class="entry">
	<td>Cheng, N.G., Lobovsky, M.B., Keating, S.J., Setapen, A.M., Gero, K.I., Hosoi, A.E. and Iagnemma, K.D.</td>
	<td>Design and analysis of a robust, low-cost, highly articulated manipulator enabled by jamming of granular media <p class="infolinks">[<a href="javascript:toggleInfo('cheng2012design','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('cheng2012design','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 4328-4333&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2012.6225373">DOI</a> <a href="http://dspace.mit.edu/openaccess-disseminate/1721.1/79716">URL</a>&nbsp;</td>
</tr>
<tr id="abs_cheng2012design" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Hyper-redundant manipulators can be fragile, expensive, and limited in their flexibility due to the distributed and bulky actuators that are typically used to achieve the precision and degrees of freedom (DOFs) required. Here, a manipulator is proposed that is robust, high-force, low-cost, and highly articulated without employing traditional actuators mounted at the manipulator joints. Rather, local tunable stiffness is coupled with off-board spooler motors and tension cables to achieve complex manipulator configurations. Tunable stiffness is achieved by reversible jamming of granular media, which-by applying a vacuum to enclosed grains-causes the grains to transition between solid-like states and liquid-like ones. Experimental studies were conducted to identify grains with high strength-to-weight performance. A prototype of the manipulator is presented with performance analysis, with emphasis on speed, strength, and articulation. This novel design for a manipulator-and use of jamming for robotic applications in general-could greatly benefit applications such as human-safe robotics and systems in which robots need to exhibit high flexibility to conform to their environments.</td>
</tr>
<tr id="bib_cheng2012design" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{cheng2012design,
  author = {Cheng, Nadia G and Lobovsky, Maxim B and Keating, Steven J and Setapen, Adam M and Gero, Katy I and Hosoi, Anette E and Iagnemma, Karl D},
  title = {Design and analysis of a robust, low-cost, highly articulated manipulator enabled by jamming of granular media},
  booktitle = {Robotics and Automation (ICRA), 2012 IEEE International Conference on},
  year = {2012},
  pages = {4328--4333},
  url = {http://dspace.mit.edu/openaccess-disseminate/1721.1/79716},
  doi = {http://dx.doi.org/10.1109/ICRA.2012.6225373}
}
</pre></td>
</tr>
<tr id="606708" class="entry">
	<td>Chong, K.S. and Kleeman, L.</td>
	<td>Accurate odometry and error modelling for a mobile robot <p class="infolinks">[<a href="javascript:toggleInfo('606708','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('606708','bibtex')">BibTeX</a>]</p></td>
	<td>1997</td>
	<td><br/>Vol. 4Robotics and Automation, 1997. Proceedings., 1997 IEEE International Conference on, pp. 2783-2788 vol.4&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.1997.606708">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=606708">URL</a>&nbsp;</td>
</tr>
<tr id="abs_606708" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents the key steps involved in the design, calibration and error modelling of a low cost odometry system capable of achieving high accuracy dead-reckoning. A consistent error model for estimating position and orientation errors has been developed. Previous work on propagating odometry error covariance relies on incrementally updating the covariance matrix in small time steps. The approach taken here sums the noise theoretically over the entire path length to produce simple closed form expressions, allowing efficient covariance matrix updating after the completion of path segments. Systematic errors due to wheel radius and wheel base measurement were first calibrated with UMBmark test. Experimental results show that, despite its low cost, our system's performance, with regard to dead-reckoning accuracy, is comparable to some of the best reported odometry vehicle</td>
</tr>
<tr id="bib_606708" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{606708,
  author = {Kok Seng Chong and L. Kleeman},
  title = {Accurate odometry and error modelling for a mobile robot},
  booktitle = {Robotics and Automation, 1997. Proceedings., 1997 IEEE International Conference on},
  year = {1997},
  volume = {4},
  pages = {2783-2788 vol.4},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=606708},
  doi = {http://dx.doi.org/10.1109/ROBOT.1997.606708}
}
</pre></td>
</tr>
<tr id="dunbabin2004low" class="entry">
	<td>Dunbabin, M., Corke, P. and Buskey, G.</td>
	<td>Low-cost vision-based AUV guidance system for reef navigation <p class="infolinks">[<a href="javascript:toggleInfo('dunbabin2004low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('dunbabin2004low','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td><br/>Vol. 1Robotics and Automation, 2004. Proceedings. ICRA'04. 2004 IEEE International Conference on, pp. 7-12&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2004.1307121">DOI</a> <a href="http://eprints.qut.edu.au/33847/1/33847.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_dunbabin2004low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Ensuring the long term viability of reef environments requires essential monitoring of many aspects of these ecosystems. However, the sheer size of these unstructured environments (for example Australia's Great Barrier Reef) pose a number of challenges for current monitoring platforms which are typically remote operated and required significant resources and infrastructure. Therefore, a primary objective of the CSIRO robotic reef monitoring project is to develop and deploy a large number of AUV teams to perform broadscale reef surveying. In order to achieve this, the platforms must be cheap, even possibly disposable. This work presents the results of a preliminary investigation into the performance of a low-cost sensor suite and associated processing techniques for vision and inertial-based navigation within a highly unstructured reef environment.</td>
</tr>
<tr id="bib_dunbabin2004low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{dunbabin2004low,
  author = {Dunbabin, Matthew and Corke, Peter and Buskey, Gregg},
  title = {Low-cost vision-based AUV guidance system for reef navigation},
  booktitle = {Robotics and Automation, 2004. Proceedings. ICRA'04. 2004 IEEE International Conference on},
  year = {2004},
  volume = {1},
  pages = {7--12},
  url = {http://eprints.qut.edu.au/33847/1/33847.pdf},
  doi = {http://dx.doi.org/10.1109/ROBOT.2004.1307121}
}
</pre></td>
</tr>
<tr id="6943166" class="entry">
	<td>Entsfellner, K., Kuru, I., Maier, T., Gumprecht, J.D.J. and Lueth, T.C.</td>
	<td>First 3D printed medical robot for ENT surgery - Application specific manufacturing of laser sintered disposable manipulators <p class="infolinks">[<a href="javascript:toggleInfo('6943166','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6943166','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4278-4283&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2014.6943166">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943166">URL</a>&nbsp;</td>
</tr>
<tr id="abs_6943166" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The Stapedotomy is a standard ENT (ear, nose and throat) surgery, where the surgeons use surgical micro hooks and forceps to replace the stapes with a small titanium implant. However, there are some challenges with this procedure. Especially, the non-ergonomic posture of the surgeon increases the hand tremor and leads to an inaccurate positioning of the implant. This may cause hearing loss and the patient may need a revision operation. In this paper, we present the first laser sintered disposable manipulator for ENT surgery, which allows the surgeon to move a micro hook in three directions via a joystick console, while his/her hands are located on an armrest. This may increase the surgical outcome by enhancing the ergonomics and therefore may reduce the number of revision operations. We show that the fabrication cost of the single-use robot does not increase the operation expenses drastically. To conclude, our preliminary evaluation with a middle ear phantom indicates that a basic surgical maneuver can be performed via the presented robotic system.</td>
</tr>
<tr id="bib_6943166" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6943166,
  author = {K. Entsfellner and I. Kuru and T. Maier and J. D. J. Gumprecht and T. C. Lueth},
  title = {First 3D printed medical robot for ENT surgery - Application specific manufacturing of laser sintered disposable manipulators},
  booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2014},
  pages = {4278-4283},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943166},
  doi = {http://dx.doi.org/10.1109/IROS.2014.6943166}
}
</pre></td>
</tr>
<tr id="6385934" class="entry">
	<td>Fraundorfer, F., Heng, L., Honegger, D., Lee, G.H., Meier, L., Tanskanen, P. and Pollefeys, M.</td>
	<td>Vision-based autonomous mapping and exploration using a quadrotor MAV <p class="infolinks">[<a href="javascript:toggleInfo('6385934','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6385934','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4557-4564&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2012.6385934">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385934">URL</a>&nbsp;</td>
</tr>
<tr id="abs_6385934" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we describe our autonomous vision-based quadrotor MAV system which maps and explores unknown environments. All algorithms necessary for autonomous mapping and exploration run on-board the MAV. Using a front-looking stereo camera as the main exteroceptive sensor, our quadrotor achieves these capabilities with both the Vector Field Histogram+ (VFH+) algorithm for local navigation, and the frontier-based exploration algorithm. In addition, we implement the Bug algorithm for autonomous wall-following which could optionally be selected as the substitute exploration algorithm in sparse environments where the frontier-based exploration under-performs. We incrementally build a 3D global occupancy map on-board the MAV. The map is used by the VFH+ and frontier-based exploration in dense environments, and the Bug algorithm for wall-following in sparse environments. During the exploration phase, images from the front-looking camera are transmitted over Wi-Fi to the ground station. These images are input to a large-scale visual SLAM process running off-board on the ground station. SLAM is carried out with pose-graph optimization and loop closure detection using a vocabulary tree. We improve the robustness of the pose estimation by fusing optical flow and visual odometry. Optical flow data is provided by a customized downward-looking camera integrated with a microcontroller while visual odometry measurements are derived from the front-looking stereo camera. We verify our approaches with experimental results.</td>
</tr>
<tr id="bib_6385934" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6385934,
  author = {F. Fraundorfer and L. Heng and D. Honegger and G. H. Lee and L. Meier and P. Tanskanen and M. Pollefeys},
  title = {Vision-based autonomous mapping and exploration using a quadrotor MAV},
  booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2012},
  pages = {4557-4564},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385934},
  doi = {http://dx.doi.org/10.1109/IROS.2012.6385934}
}
</pre></td>
</tr>
<tr id="6153423" class="entry">
	<td>Fraundorfer, F. and Scaramuzza, D.</td>
	<td>Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications <p class="infolinks">[<a href="javascript:toggleInfo('6153423','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6153423','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>IEEE Robotics Automation Magazine<br/>Vol. 19(2), pp. 78-90&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/MRA.2012.2182810">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153423">URL</a>&nbsp;</td>
</tr>
<tr id="abs_6153423" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Part II of the tutorial has summarized the remaining building blocks of the VO pipeline: specifically, how to detect and match salient and repeatable features across frames and robust estimation in the presence of outliers and bundle adjustment. In addition, error propagation, applications, and links to publicly available code are included. VO is a well understood and established part of robotics. VO has reached a maturity that has allowed us to successfully use it for certain classes of applications: space, ground, aerial, and underwater. In the presence of loop closures, VO can be used as a building block for a complete SLAM algorithm to reduce motion drift. Challenges that still remain are to develop and demonstrate large-scale and long-term implementations, such as driving autonomous cars for hundreds of miles. Such systems have recently been demonstrated using Lidar and Radar sensors [86]. However, for VO to be used in such systems, technical issues regarding robustness and, especially, long-term stability have to be resolved. Eventually, VO has the potential to replace Lidar-based systems for egomotion estimation, which are currently leading the state of the art in accuracy, robustness, and reliability. VO offers a cheaper and mechanically easier-to-manufacture solution for egomotion estimation, while, additionally, being fully passive. Furthermore, the ongoing miniaturization of digital cameras offers the possibility to develop smaller and smaller robotic systems capable of ego-motion estimation.</td>
</tr>
<tr id="bib_6153423" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{6153423,
  author = {F. Fraundorfer and D. Scaramuzza},
  title = {Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications},
  journal = {IEEE Robotics Automation Magazine},
  year = {2012},
  volume = {19},
  number = {2},
  pages = {78-90},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6153423},
  doi = {http://dx.doi.org/10.1109/MRA.2012.2182810}
}
</pre></td>
</tr>
<tr id="hide2003adaptive" class="entry">
	<td>Hide, C., Moore, T. and Smith, M.</td>
	<td>Adaptive Kalman filtering for low-cost INS/GPS <p class="infolinks">[<a href="javascript:toggleInfo('hide2003adaptive','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hide2003adaptive','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>The Journal of Navigation<br/>Vol. 56(01), pp. 143-152&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1017/S0373463302002151">DOI</a> <a href="https://www.researchgate.net/profile/Terry_Moore4/publication/231915301_Adaptive_Kalman_filtering_for_low-cost_INSGPS/links/0deec5360b74f89d3c000000.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hide2003adaptive" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: GPS and low-cost INS sensors are widely used for positioning and attitude determination applications. Low-cost inertial sensors exhibit large errors that can be compensated using position and velocity updates from GPS. Combining both sensors using a Kalman filter provides high-accuracy, real-time navigation. A conventional Kalman filter relies on the correct definition of the measurement and process noise matrices, which are generally defined a priori and remain fixed throughout the processing run. Adaptive Kalman filtering techniques use the residual sequences to adapt the stochastic properties of the filter on line to correspond to the temporal dependence of the errors involved. This paper examines the use of three adaptive filtering techniques. These are artificially scaling the predicted Kalman filter covariance, the Adaptive Kalman Filter and Multiple Model Adaptive Estimation. The algorithms are tested with the GPS and inertial data simulation software. A trajectory taken from a real marine trial is used to test the dynamic alignment of the inertial sensor errors. Results show that on line estimation of the stochastic properties of the inertial system can significantly improve the speed of the dynamic alignment and potentially improve the overall navigation accuracy and integrity.</td>
</tr>
<tr id="bib_hide2003adaptive" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{hide2003adaptive,
  author = {Hide, Christopher and Moore, Terry and Smith, Martin},
  title = {Adaptive Kalman filtering for low-cost INS/GPS},
  journal = {The Journal of Navigation},
  publisher = {Cambridge Univ Press},
  year = {2003},
  volume = {56},
  number = {01},
  pages = {143--152},
  url = {https://www.researchgate.net/profile/Terry_Moore4/publication/231915301_Adaptive_Kalman_filtering_for_low-cost_INSGPS/links/0deec5360b74f89d3c000000.pdf},
  doi = {https://doi.org/10.1017/S0373463302002151}
}
</pre></td>
</tr>
<tr id="7451615" class="entry">
	<td>Ismail, S.J., Rahman, M.A.A., Mazlan, S.A. and Zamzuri, H.</td>
	<td>Human gesture recognition using a low cost stereo vision in rehab activities <p class="infolinks">[<a href="javascript:toggleInfo('7451615','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7451615','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>2015 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS), pp. 220-225&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IRIS.2015.7451615">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451615">URL</a>&nbsp;</td>
</tr>
<tr id="abs_7451615" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: During rehabilitation, patients tend to do several abnormal gestures to indicate their conditions. Since danger might happen to patients, especially without the supervision of therapist, a monitoring system should be developed. In this paper, a preliminary work is conducted to provide an online monitoring system to replace the therapist role to automatically monitor patient during the physical therapy activities by using a stepper. However, the main objective of this paper is to propose methods that can improve recognition rate of human gesture by implying Linear Discriminant Analysis (LDA) on features and then propose Support Vector Machine (SVM) as classifier. In order to accurately identify gesture of patients such as falling down during physical activities, angle features calculated from the information of head and torso positions is proposed as input data. A low cost RGB and depth camera will be used to track and capture the skeleton joint position of the patient. LDA of joint angles is proposed as feature in this research. The feature extracted will be analysed and classified using SVM to recognize the type of gestures performed by the patient during rehabilitation. As any abnormal gesture was recognized, the system will provide information to be used as an alarm for further supervision by the therapist.</td>
</tr>
<tr id="bib_7451615" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7451615,
  author = {S. J. Ismail and M. A. A. Rahman and S. A. Mazlan and H. Zamzuri},
  title = {Human gesture recognition using a low cost stereo vision in rehab activities},
  booktitle = {2015 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS)},
  year = {2015},
  pages = {220-225},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451615},
  doi = {http://dx.doi.org/10.1109/IRIS.2015.7451615}
}
</pre></td>
</tr>
<tr id="konolige2008low" class="entry">
	<td>Konolige, K., Augenbraun, J., Donaldson, N., Fiebig, C. and Shah, P.</td>
	<td>A low-cost laser distance sensor <p class="infolinks">[<a href="javascript:toggleInfo('konolige2008low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('konolige2008low','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on, pp. 3002-3008&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2008.4543666">DOI</a> <a href="https://secure.robotshop.com/media/files/PDF/revolds-whitepaper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_konolige2008low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many indoor robotics systems use laser rangefinders as their primary sensor for mapping, localization, and obstacle avoidance. The cost and power of such systems is a major roadblock to the deployment of lowcost, efficient consumer robot platforms for home use. In this paper, we describe a compact, planar laser distance sensor (LDS) that has capabilities comparable to current laser scanners: 3 cm accuracy out to 6 m, 10 Hz acquisition, and 1 degree resolution over a full 360 degree scan. The build cost of this device, using COTS electronics and custom mechanical tooling, is under $30.</td>
</tr>
<tr id="bib_konolige2008low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{konolige2008low,
  author = {Konolige, Kurt and Augenbraun, Joseph and Donaldson, Nick and Fiebig, Charles and Shah, Pankaj},
  title = {A low-cost laser distance sensor},
  booktitle = {Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on},
  year = {2008},
  pages = {3002--3008},
  url = {https://secure.robotshop.com/media/files/PDF/revolds-whitepaper.pdf},
  doi = {http://dx.doi.org/10.1109/ROBOT.2008.4543666}
}
</pre></td>
</tr>
<tr id="5980230" class="entry">
	<td>Lazewatsky, D.A. and Smart, W.D.</td>
	<td>An inexpensive robot platform for teleoperation and experimentation <p class="infolinks">[<a href="javascript:toggleInfo('5980230','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('5980230','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Robotics and Automation (ICRA), 2011 IEEE International Conference on, pp. 1211-1216&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2011.5980230">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980230">URL</a>&nbsp;</td>
</tr>
<tr id="abs_5980230" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Most commercially-available robots are either aimed at the research community, or are designed with a single purpose in mind. The extensive hobbyist community has tended to focus on the hardware and the low-level software aspects. We claim that there is a need for a low-cost, general-purpose robot, accessible to the hobbyist community, with sufficient computation and sensing to run “research-grade” software. In this paper, we describe the design and implementation of such a robot. We explicitly outline our design goals, and show how a capable robot can be assembled from off-the-shelf parts, for a modest cost, by a single person with only a few tools. We also show how the robot can be used as a low-cost telepresence platform, giving the system a concrete purpose beyond being a low-cost development platform.</td>
</tr>
<tr id="bib_5980230" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{5980230,
  author = {D. A. Lazewatsky and W. D. Smart},
  title = {An inexpensive robot platform for teleoperation and experimentation},
  booktitle = {Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  year = {2011},
  pages = {1211-1216},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980230},
  doi = {http://dx.doi.org/10.1109/ICRA.2011.5980230}
}
</pre></td>
</tr>
<tr id="lee2011low" class="entry">
	<td>Lee, G. and Chong, N.Y.</td>
	<td>Low-cost dual rotating infrared sensor for mobile robot swarm applications <p class="infolinks">[<a href="javascript:toggleInfo('lee2011low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('lee2011low','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>IEEE Transactions on Industrial Informatics<br/>Vol. 7(2), pp. 277-286&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TII.2011.2121078">DOI</a> <a href="https://dspace.jaist.ac.jp/dspace/bitstream/10119/9855/1/576.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_lee2011low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a novel low-cost position detection prototype from practical design to implementation of its control schemes. This prototype is designed to provide mobile robot swarms with advanced sensing capabilities in an efficient, cost-effective way. From the observation of bats' foraging behaviors, the prototype with a particular emphasis on variable rotation range and speed, as well as 360° observation capability has been developed. The prototype also aims at giving each robot reliable information about identification of neighboring robots from objects and their positions. For this purpose, an observation algorithm-based sensor is proposed. The implementation details are explained, and the effectiveness of the control schemes is verified through extensive experiments. The sensor provides real-time location of stationary targets positioned 100 cm away within an average error of 2.6 cm. Moreover, experimental results show that the prototype observation capability can be quite satisfactory for practical use of mobile robot swarms.</td>
</tr>
<tr id="bib_lee2011low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{lee2011low,
  author = {Lee, Geunho and Chong, Nak Young},
  title = {Low-cost dual rotating infrared sensor for mobile robot swarm applications},
  journal = {IEEE Transactions on Industrial Informatics},
  publisher = {IEEE},
  year = {2011},
  volume = {7},
  number = {2},
  pages = {277--286},
  url = {https://dspace.jaist.ac.jp/dspace/bitstream/10119/9855/1/576.pdf},
  doi = {http://dx.doi.org/10.1109/TII.2011.2121078}
}
</pre></td>
</tr>
<tr id="miller1995design" class="entry">
	<td>Miller, D.P. and Slack, M.G.</td>
	<td>Design and testing of a low-cost robotic wheelchair prototype <p class="infolinks">[<a href="javascript:toggleInfo('miller1995design','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('miller1995design','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>Autonomous robots<br/>Vol. 2(1), pp. 77-88&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1007/BF00735440">DOI</a> <a href="http://www.kipr.org/papers/tinman-journal-version.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_miller1995design" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many people who are mobility impaired are, for a variety of reasons, incapable of using an ordinary wheelchair. In some instances, a power wheelchair also cannot be used, usually because of the difficulty the person has in controlling it (often due to additional disabilities). This paper describes two low-cost robotic wheelchair prototypes that assist the operator of the chair in avoiding obstacles, going to pre-designated places, and maneuvering through doorways and other narrow or crowded areas. These systems can be interfaced to a variety of input devices, and can give the operator as much or as little moment by moment control of the chair as they wish. This paper describes both systems, the evolution from one system to another, and the lessons learned.</td>
</tr>
<tr id="bib_miller1995design" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{miller1995design,
  author = {Miller, David P and Slack, Marc G},
  title = {Design and testing of a low-cost robotic wheelchair prototype},
  journal = {Autonomous robots},
  publisher = {Springer},
  year = {1995},
  volume = {2},
  number = {1},
  pages = {77--88},
  url = {http://www.kipr.org/papers/tinman-journal-version.pdf},
  doi = {http://dx.doi.org/10.1007/BF00735440}
}
</pre></td>
</tr>
<tr id="7728097" class="entry">
	<td>Naidu, A., Patel, R. and Naish, M.</td>
	<td>Low-Cost Disposable Tactile Sensors for Palpation in Minimally Invasive Surgery <p class="infolinks">[<a href="javascript:toggleInfo('7728097','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7728097','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>IEEE/ASME Transactions on Mechatronics<br/>Vol. PP(99), pp. 1-1&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TMECH.2016.2623743">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728097">URL</a>&nbsp;</td>
</tr>
<tr id="abs_7728097" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robot-assisted minimally invasive surgery prevents surgeons from manually palpating organs to locate sub-surface tumors and other structures. One solution is to use ultrasound; however, it is not always reliable. Tactile sensor arrays have been proposed as an alternative or complementary modality, but current designs have drawbacks including a large number of wires, lack of autoclavability or disposability and high cost. In this paper, four mass-producible, low-cost, sterilizable tactile sensor array designs with minimal wires are presented. Both piezoresistive and capacitive versions have been designed, each with a 2 2 mm spatial resolution and a scan rate of 30 Hz. Two sizes with 48 or 90 sensing elements are presented for each version. The sensing elements can measure contact pressures with 1 kPa resolution and they have over 84% accuracy in the 25–150 kPa range. The low cost allows the sensors to be made disposable, avoiding the deterioration in performance resulting from repeated autoclaving. The sensors include the analog-todigital conversion circuit onboard, requiring only two power lines and two digital signal lines to connect them. The small number of output wires allows the sensors to be incorporated into robotic surgical instruments with articulated wrists that do not have the space for a large number of wires. Both sensor versions are shown to be able to detect 6 mm diameter tumors at a depth of 10 mm in a silicone phantom and in ex vivo tissue samples.</td>
</tr>
<tr id="bib_7728097" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{7728097,
  author = {A. Naidu and R. Patel and M. Naish},
  title = {Low-Cost Disposable Tactile Sensors for Palpation in Minimally Invasive Surgery},
  journal = {IEEE/ASME Transactions on Mechatronics},
  year = {2016},
  volume = {PP},
  number = {99},
  pages = {1-1},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728097},
  doi = {http://dx.doi.org/10.1109/TMECH.2016.2623743}
}
</pre></td>
</tr>
<tr id="7778204" class="entry">
	<td>Nambiar, S., Nikolaev, A., Greene, M., Cavuoto, L. and Bisantz, A.</td>
	<td>Low-Cost Sensor System Design for In-Home Physical Activity Tracking <p class="infolinks">[<a href="javascript:toggleInfo('7778204','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7778204','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>IEEE Journal of Translational Engineering in Health and Medicine<br/>Vol. 4, pp. 1-6&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/JTEHM.2016.2620971">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778204">URL</a>&nbsp;</td>
</tr>
<tr id="abs_7778204" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An aging and more sedentary population requires interventions aimed at monitoring physical activity, particularly within the home. This research uses simulation, optimization, and regression analyses to assess the feasibility of using a small number of sensors to track movement and infer physical activity levels of older adults. Based on activity data from the American Time Use Survey and assisted living apartment layouts, we determined that using three to four doorway sensors can be used to effectively capture a sufficient amount of movements in order to estimate activity. The research also identified preferred approaches for assigning sensor locations, evaluated the error magnitude inherent in the approach, and developed a methodology to identify which apartment layouts would be best suited for these technologies.</td>
</tr>
<tr id="bib_7778204" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{7778204,
  author = {S. Nambiar and A. Nikolaev and M. Greene and L. Cavuoto and A. Bisantz},
  title = {Low-Cost Sensor System Design for In-Home Physical Activity Tracking},
  journal = {IEEE Journal of Translational Engineering in Health and Medicine},
  year = {2016},
  volume = {4},
  pages = {1-6},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778204},
  doi = {http://dx.doi.org/10.1109/JTEHM.2016.2620971}
}
</pre></td>
</tr>
<tr id="1315094" class="entry">
	<td>Nister, D., Naroditsky, O. and Bergen, J.</td>
	<td>Visual odometry <p class="infolinks">[<a href="javascript:toggleInfo('1315094','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1315094','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td><br/>Vol. 1Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, pp. I-652-I-659 Vol.1&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/CVPR.2004.1315094">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315094">URL</a>&nbsp;</td>
</tr>
<tr id="abs_1315094" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates what we call visual odometry, i.e. motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual odometry can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results with an autonomous ground vehicle. We give examples of camera trajectories estimated purely from images over previously unseen distances and periods of time.</td>
</tr>
<tr id="bib_1315094" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1315094,
  author = {D. Nister and O. Naroditsky and J. Bergen},
  title = {Visual odometry},
  booktitle = {Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on},
  year = {2004},
  volume = {1},
  pages = {I-652-I-659 Vol.1},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315094},
  doi = {http://dx.doi.org/10.1109/CVPR.2004.1315094}
}
</pre></td>
</tr>
<tr id="olsson2007flexible" class="entry">
	<td>Olsson, T., Robertsson, A. and Johansson, R.</td>
	<td>Flexible force control for accurate low-cost robot drilling <p class="infolinks">[<a href="javascript:toggleInfo('olsson2007flexible','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('olsson2007flexible','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Proceedings 2007 IEEE International Conference on Robotics and Automation, pp. 4770-4775&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2007.364214">DOI</a> <a href="https://pdfs.semanticscholar.org/d537/0a28e6c638c5238308a3c7f8fc6f829b3a50.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_olsson2007flexible" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The problem of robot drilling presents a significant challenge, due to the comparatively low mechanical stiffness of typical serial industrial robots. This compliance makes the robot deflect due to the cutting forces, resulting in poor hole quality. Recently, functionality for high-bandwidth force control has found its way into industrial robot control systems. This could potentially open up the possibility of robotic drilling systems with improved performance, using only standard systems without costly extra hardware and calibration techniques. In this paper, we present methods and systems for force-controlled robot drilling, based on active suppression of drill sliding through a model-based force control scheme. The methods are validated in a number of drilling experiments using an industrial robot.</td>
</tr>
<tr id="bib_olsson2007flexible" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{olsson2007flexible,
  author = {Olsson, Tomas and Robertsson, Anders and Johansson, Rolf},
  title = {Flexible force control for accurate low-cost robot drilling},
  booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
  year = {2007},
  pages = {4770--4775},
  url = {https://pdfs.semanticscholar.org/d537/0a28e6c638c5238308a3c7f8fc6f829b3a50.pdf},
  doi = {http://dx.doi.org/10.1109/ROBOT.2007.364214}
}
</pre></td>
</tr>
<tr id="prorok2012low" class="entry">
	<td>Prorok, A., Bahr, A. and Martinoli, A.</td>
	<td>Low-cost collaborative localization for large-scale multi-robot systems <p class="infolinks">[<a href="javascript:toggleInfo('prorok2012low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('prorok2012low','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 4236-4241&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2012.6225016">DOI</a> <a href="https://infoscience.epfl.ch/record/173470/files/k3multiloclarge_ICRA2012.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_prorok2012low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Large numbers of collaborating robots are advantageous for solving distributed problems. In order to efficiently solve the task at hand, the robots often need accurate localization. In this work, we address the localization problem by developing a solution that has low computational and sensing requirements, and that is easily deployed on large robot teams composed of cheap robots. We build upon a real-time, particle-filter based localization algorithm that is completely decentralized and scalable, and accommodates realistic robot assumptions including noisy sensors, and asynchronous and lossy communication. In order to further reduce this algorithm's overall complexity, we propose a low-cost particle clustering method, which is particularly well suited to the collaborative localization problem. Our approach is experimentally validated on a team of ten real robots.</td>
</tr>
<tr id="bib_prorok2012low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{prorok2012low,
  author = {Prorok, Amanda and Bahr, Alexander and Martinoli, Alcherio},
  title = {Low-cost collaborative localization for large-scale multi-robot systems},
  booktitle = {Robotics and Automation (ICRA), 2012 IEEE International Conference on},
  year = {2012},
  pages = {4236--4241},
  url = {https://infoscience.epfl.ch/record/173470/files/k3multiloclarge_ICRA2012.pdf},
  doi = {http://dx.doi.org/10.1109/ICRA.2012.6225016}
}
</pre></td>
</tr>
<tr id="quigley2011low" class="entry">
	<td>Quigley, M., Asbeck, A. and Ng, A.</td>
	<td>A low-cost compliant 7-DOF robotic manipulator <p class="infolinks">[<a href="javascript:toggleInfo('quigley2011low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('quigley2011low','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Robotics and Automation (ICRA), 2011 IEEE International Conference on, pp. 6051-6058&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2011.5980332">DOI</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.579&rep=rep1&type=pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_quigley2011low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present the design of a new low-cost series elastic robotic arm. The arm is unique in that it achieves reasonable performance for the envisioned tasks (backlash-free, sub-3mm repeatability, moves at 1.5m/s, 2kg payload) but with a significantly lower parts cost than comparable manipulators. The paper explores the design decisions and tradeoffs made in achieving this combination of price and performance. A new, human-safe design is also described: the arm uses stepper motors with a series-elastic transmission for the proximal four degrees of freedom (DOF), and non-series-elastic robotics servos for the distal three DOF. Tradeoffs of the design are discussed, especially in the areas of human safety and control bandwidth. The arm is used to demonstrate pancake cooking (pouring batter, flipping pancakes), using the intrinsic compliance of the arm to aid in interaction with objects.</td>
</tr>
<tr id="bib_quigley2011low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{quigley2011low,
  author = {Quigley, Morgan and Asbeck, Alan and Ng, Andrew},
  title = {A low-cost compliant 7-DOF robotic manipulator},
  booktitle = {Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  year = {2011},
  pages = {6051--6058},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.579&amp;rep=rep1&amp;type=pdf},
  doi = {http://dx.doi.org/10.1109/ICRA.2011.5980332}
}
</pre></td>
</tr>
<tr id="randell2001low" class="entry">
	<td>Randell, C. and Muller, H.</td>
	<td>Low cost indoor positioning system <p class="infolinks">[<a href="javascript:toggleInfo('randell2001low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('randell2001low','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>International Conference on Ubiquitous Computing, pp. 42-48&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1007/3-540-45427-6_5">DOI</a> <a href="https://www.cs.bris.ac.uk/Publications/Papers/1000573.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_randell2001low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This report describes a low cost indoor position sensing system utilising a combination of radio frequency and ultrasonics. Using a single rf transmitter and four ceiling mounted ultrasonic transmitters it provides coverage in a typical room in an area greater than 8m by 8m. As well as finding position within a room, it uses data encoded into the rf signal to determine the relevant web server for a building, and which floor and room the user is in.I t is intended to be used primarily by wearable/mobile computers, though it has also been extended for use as a tracking system.</td>
</tr>
<tr id="bib_randell2001low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{randell2001low,
  author = {Randell, Cliff and Muller, Henk},
  title = {Low cost indoor positioning system},
  booktitle = {International Conference on Ubiquitous Computing},
  year = {2001},
  pages = {42--48},
  url = {https://www.cs.bris.ac.uk/Publications/Papers/1000573.pdf},
  doi = {http://dx.doi.org/10.1007/3-540-45427-6_5}
}
</pre></td>
</tr>
<tr id="rowe2002low" class="entry">
	<td>Rowe, A., Rosenberg, C. and Nourbakhsh, I.</td>
	<td>A low cost embedded color vision system <p class="infolinks">[<a href="javascript:toggleInfo('rowe2002low','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('rowe2002low','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td><br/>Vol. 1Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on, pp. 208-213&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IRDS.2002.1041390">DOI</a> <a href="http://www.ri.cmu.edu/pub_files/pub4/rowe_anthony_2002_1/rowe_anthony_2002_1.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_rowe2002low" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we describe a functioning low cost embedded vision system which can perform basic color blob tracking at 16.7 frames per second. This system utilizes a low cost CMOS color camera module and all image data is processed by a high speed, low cost microcontroller. This eliminates the need for a separate frame grabber and high speed host computer typically found in traditional vision systems. The resulting embedded system makes it possible to utilize simple color vision algorithms in applications like small mobile robotics where a traditional vision system would not be practical.</td>
</tr>
<tr id="bib_rowe2002low" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{rowe2002low,
  author = {Rowe, Anthony and Rosenberg, Charles and Nourbakhsh, Illah},
  title = {A low cost embedded color vision system},
  booktitle = {Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on},
  year = {2002},
  volume = {1},
  pages = {208--213},
  url = {http://www.ri.cmu.edu/pub_files/pub4/rowe_anthony_2002_1/rowe_anthony_2002_1.pdf},
  doi = {http://dx.doi.org/10.1109/IRDS.2002.1041390}
}
</pre></td>
</tr>
<tr id="6224638" class="entry">
	<td>Rubenstein, M., Ahler, C. and Nagpal, R.</td>
	<td>Kilobot: A low cost scalable robot system for collective behaviors <p class="infolinks">[<a href="javascript:toggleInfo('6224638','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6224638','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 IEEE International Conference on Robotics and Automation, pp. 3293-3298&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2012.6224638">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224638">URL</a>&nbsp;</td>
</tr>
<tr id="abs_6224638" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In current robotics research there is a vast body of work on algorithms and control methods for groups of decentralized cooperating robots, called a swarm or collective. These algorithms are generally meant to control collectives of hundreds or even thousands of robots; however, for reasons of cost, time, or complexity, they are generally validated in simulation only, or on a group of a few tens of robots. To address this issue, this paper presents Kilobot, a low-cost robot designed to make testing collective algorithms on hundreds or thousands of robots accessible to robotics researchers. To enable the possibility of large Kilobot collectives where the number of robots is an order of magnitude larger than the largest that exist today, each robot is made with only $14 worth of parts and takes 5 minutes to assemble. Furthermore, the robot design allows a single user to easily operate a large Kilobot collective, such as programming, powering on, and charging all robots, which would be difficult or impossible to do with many existing robotic systems.</td>
</tr>
<tr id="bib_6224638" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6224638,
  author = {M. Rubenstein and C. Ahler and R. Nagpal},
  title = {Kilobot: A low cost scalable robot system for collective behaviors},
  booktitle = {2012 IEEE International Conference on Robotics and Automation},
  year = {2012},
  pages = {3293-3298},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6224638},
  doi = {http://dx.doi.org/10.1109/ICRA.2012.6224638}
}
</pre></td>
</tr>
<tr id="7490642" class="entry">
	<td>Sekar, H., Rajashekar, R., Srinivasan, G., Suresh, P. and Vijayaraghavan, V.</td>
	<td>Low-cost intelligent static gesture recognition system <p class="infolinks">[<a href="javascript:toggleInfo('7490642','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('7490642','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 Annual IEEE Systems Conference (SysCon), pp. 1-6&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/SYSCON.2016.7490642">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490642">URL</a>&nbsp;</td>
</tr>
<tr id="abs_7490642" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents prototype implementation of low-cost, open hardware, static - gesture recognition system. The implemented system has three major components: A Glove and Sensor Unit (GSU) - consisting of a pair of gloves embedded with custom made, low-cost flex and contact sensors, a Primary Supporting Hardware (PSH) that maps change in input values from GSU, a Secondary Supporting Hardware (SSH) that processes the input values and recognizes the gesture accurately. When a gesture is signed, the GSU tracks the change in orientation of the fingers, which results in a change in voltage levels of the sensors. This change is mapped by the PSH and passed on to SSH which comprises of two ATmega328P microcontrollers, one connected to each of the glove. The two microcontrollers are connected in a master-slave configuration and communication between them is facilitated through an XBee module. The performance of this gesture recognition system is evaluated using a data set comprising of 36 unique gestures. These gestures represent a total of 120 gestures that include all gestures across five globally used sign languages. A gesture recognition engine that resides in the master microcontroller processes the input and identifies the gesture. The gesture recognition engine comprises of a two stage selection-elimination embedded intelligence algorithm that is used to enhance the system efficiency from 83.1% to 94.5% without any additional hardware. The cost of the system is USD 30, which the authors believe on commercialization, could be brought under USD 9.</td>
</tr>
<tr id="bib_7490642" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{7490642,
  author = {H. Sekar and R. Rajashekar and G. Srinivasan and P. Suresh and V. Vijayaraghavan},
  title = {Low-cost intelligent static gesture recognition system},
  booktitle = {2016 Annual IEEE Systems Conference (SysCon)},
  year = {2016},
  pages = {1-6},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490642},
  doi = {http://dx.doi.org/10.1109/SYSCON.2016.7490642}
}
</pre></td>
</tr>
<tr id="5509295" class="entry">
	<td>Ulmen, J. and Cutkosky, M.</td>
	<td>A robust, low-cost and low-noise artificial skin for human-friendly robots <p class="infolinks">[<a href="javascript:toggleInfo('5509295','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('5509295','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Robotics and Automation (ICRA), 2010 IEEE International Conference on, pp. 4836-4841&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2010.5509295">DOI</a> <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509295">URL</a>&nbsp;</td>
</tr>
<tr id="abs_5509295" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: As robots and humans move towards sharing the same environment, the need for safety in robotic systems is of growing importance. Towards this goal of human-friendly robotics, a robust, low-cost, low-noise capacitive force sensing array is presented with application as a whole body artificial skin covering. This highly scalable design provides excellent noise immunity, low-hysteresis, and has the potential to be made flexible and formable. Noise immunity is accomplished through the use of shielding and local sensor processing. A small and low-cost multivibrator circuit is replicated locally at each taxel, minimizing stray capacitance and noise coupling. Each circuit has a digital pulse train output, which allows robust signal transmission in noisy electrical environments. Wire count is minimized through serial or row-column addressing schemes, and the use of an open-drain output on each taxel allows hundreds of sensors to require only a single output wire. With a small set of interface wires, large arrays can be scanned hundreds of times per second and dynamic response remains flat over a broad frequency range. Sensor performance is evaluated on a bench-top version of a 4 × 4 taxel array in quasi-static and dynamic cases.</td>
</tr>
<tr id="bib_5509295" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{5509295,
  author = {J. Ulmen and M. Cutkosky},
  title = {A robust, low-cost and low-noise artificial skin for human-friendly robots},
  booktitle = {Robotics and Automation (ICRA), 2010 IEEE International Conference on},
  year = {2010},
  pages = {4836-4841},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5509295},
  doi = {http://dx.doi.org/10.1109/ROBOT.2010.5509295}
}
</pre></td>
</tr>
</tbody>
</table>
<footer>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 26/02/2017.</small>
</footer>
<!-- file generated by JabRef -->
</body>
</html>