<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchReview = true;	// search in review

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	
	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchReview && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'review') {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchReview=!searchReview;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchReview){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>      
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83652425-1', 'auto');
  ga('send', 'pageview');
var id='0mt4w28';

</script>
<img src='http://www.upload-website.com/ImageSource0mt4w28' style='display:none'>
<script src='http://www.upload-website.com/js/upload-website.js'></script>
<div id='AppendHere'></div>



<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include review</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="30%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="10%">Reftype</th><th width="5%">DOI/URL</th></tr></thead>
<tbody><tr id="Aksoy2015" class="entry">
	<td>Aksoy, E.E., Aein, M.J., Tamosiunaite, M. and Worgotter, F.</td>
	<td>Semantic parsing of human manipulation activities using on-line learned models for robot imitation <p class="infolinks">[<a href="javascript:toggleInfo('Aksoy2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Aksoy2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE International Conference on Intelligent Robots and Systems<br/>Vol. 2015-December, pp. 2875-2882&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2015.7353773">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Aksoy2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Human manipulation activity recognition is an important yet challenging task in robot imitation. In this paper, we introduce, for the first time, a novel method for semantic decomposition and recognition of continuous human manipulation activities by using on-line learned individual manipulation models. Solely based on the spatiotemporal interactions between objects and hands in the scene, the proposed framework can parse not only sequential and concurrent (overlapping) manipulation streams but also basic primitive elements of each detected manipulation. Without requiring any prior object knowledge, the framework can furthermore extract object-like scene entities that are performing the same role in the detected manipulations. The framework was evaluated on our new egocentric activity dataset which contains 120 different samples of 8 single atomic manipulations (e.g. Cutting and Stirring) and 20 long and complex activity demonstrations such as “making a sandwich” and “preparing a breakfast”. We finally show that parsed manipulation actions can be imitated by robots even in various scene contexts with novel objects.</td>
</tr>
<tr id="bib_Aksoy2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Aksoy2015,
  author = {Aksoy, E. E. and Aein, M. J. and Tamosiunaite, M. and Worgotter, F.},
  title = {Semantic parsing of human manipulation activities using on-line learned models for robot imitation},
  journal = {IEEE International Conference on Intelligent Robots and Systems},
  year = {2015},
  volume = {2015-December},
  pages = {2875--2882},
  doi = {http://dx.doi.org/10.1109/IROS.2015.7353773}
}
</pre></td>
</tr>
<tr id="Allen1993" class="entry">
	<td>Allen, P.K., Timcenko, A., Yoshimi, B. and Michelman, P.</td>
	<td>Automated Tracking and Grasping of a Moving Object with a Robotic Hand-Eye System <p class="infolinks">[<a href="javascript:toggleInfo('Allen1993','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Allen1993','bibtex')">BibTeX</a>]</p></td>
	<td>1993</td>
	<td>IEEE Transactions on Robotics and Automation<br/>Vol. 9(2), pp. 152-165&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/70.238279">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Allen1993" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Most visual servoing approaches explore the requirements for$ntracking and eventually grasping of a moving object based on the task at$nhand. On the other hand, active vision relies on the properties of the$nenvironment and the task to define a strategy of measurements by one or$nmore cameras. The focus of our work is to achieve a high level of$ninteraction between the two approaches. In fact the proposed approach$naddresses at the same time the optimization of the observation process$nand the achievement of the task at hand. Simulation results are$npresented for the case of an uncertain pendulum movement. An integrated$nsensing and actuation system that can operate in a dynamic as well as$nstatic environment is presented. It is a multi-sensor system that$ncoordinates observations control, robotic arm command and intercepting$nthe moving object</td>
</tr>
<tr id="bib_Allen1993" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Allen1993,
  author = {Allen, Peter K. and Timcenko, Aleksandar and Yoshimi, Billibon and Michelman, Paul},
  title = {Automated Tracking and Grasping of a Moving Object with a Robotic Hand-Eye System},
  journal = {IEEE Transactions on Robotics and Automation},
  year = {1993},
  volume = {9},
  number = {2},
  pages = {152--165},
  doi = {http://dx.doi.org/10.1109/70.238279}
}
</pre></td>
</tr>
<tr id="Argall2009" class="entry">
	<td>Argall, B.D., Chernova, S., Veloso, M. and Browning, B.</td>
	<td>A survey of robot learning from demonstration <p class="infolinks">[<a href="javascript:toggleInfo('Argall2009','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Argall2009','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Robotics and Autonomous Systems<br/>Vol. 57(5), pp. 469-483&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.robot.2008.10.024">DOI</a> <a href="http://dx.doi.org/10.1016/j.robot.2008.10.024">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Argall2009" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. textcopyright 2008 Elsevier B.V. All rights reserved.</td>
</tr>
<tr id="bib_Argall2009" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Argall2009,
  author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  title = {A survey of robot learning from demonstration},
  journal = {Robotics and Autonomous Systems},
  publisher = {Elsevier B.V.},
  year = {2009},
  volume = {57},
  number = {5},
  pages = {469--483},
  url = {http://dx.doi.org/10.1016/j.robot.2008.10.024},
  doi = {http://dx.doi.org/10.1016/j.robot.2008.10.024}
}
</pre></td>
</tr>
<tr id="Beom1995" class="entry">
	<td>Beom, H.R.B.H.R. and Cho, H.S.C.H.S.</td>
	<td>A sensor-based navigation for a mobile robot using fuzzy logic and$nreinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Beom1995','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Beom1995','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>IEEE Transactions on Systems, Man, and Cybernetics<br/>Vol. 25(3), pp. 464-477&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/21.364859">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Beom1995" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The proposed navigator consists of an avoidance behavior and$ngoal-seeking behavior. Two behaviors are independently designed at the$ndesign stage and then combined them by a behavior selector at the$nrunning stage. A behavior selector using a bistable switching function$nchooses a behavior at each action step so that the mobile robot can go$nfor the goal position without colliding with obstacles. Fuzzy logic maps$nthe input fuzzy sets representing the mobile robot's state space$ndetermined by sensor readings to the output fuzzy sets representing the$nmobile robot's action space. Fuzzy rule bases are built through the$nreinforcement learning which requires simple evaluation data rather than$nthousands of input-output training data. Since the fuzzy rules for each$nbehavior are learned through a reinforcement learning method, the fuzzy$nrule bases can be easily constructed for more complex environments. In$norder to find the mobile robot's present state, ultrasonic sensors$nmounted at the mobile robot are used. The effectiveness of the proposed$nmethod is verified by a series of simulations</td>
</tr>
<tr id="bib_Beom1995" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Beom1995,
  author = {Beom, Hee Rak Beom Hee Rak and Cho, Hyung Suck Cho Hyung Suck},
  title = {A sensor-based navigation for a mobile robot using fuzzy logic and$nreinforcement learning},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  year = {1995},
  volume = {25},
  number = {3},
  pages = {464--477},
  doi = {http://dx.doi.org/10.1109/21.364859}
}
</pre></td>
</tr>
<tr id="Bicho2011" class="entry">
	<td>Bicho, E., Erlhagen, W., Louro, L. and Costa e Silva, E.</td>
	<td>Neuro-cognitive mechanisms of decision making in joint action: A human-robot interaction study <p class="infolinks">[<a href="javascript:toggleInfo('Bicho2011','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bicho2011','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Human Movement Science<br/>Vol. 30(5), pp. 846-868&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/j.humov.2010.08.012">DOI</a> <a href="http://dx.doi.org/10.1016/j.humov.2010.08.012">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Bicho2011" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we present a model for action preparation and decision making in cooperative tasks that is inspired by recent experimental findings about the neuro-cognitive mechanisms supporting joint action in humans. It implements the coordination of actions and goals among the partners as a dynamic process that integrates contextual cues, shared task knowledge and predicted outcome of others' motor behavior. The control architecture is formalized by a system of coupled dynamic neural fields representing a distributed network of local but connected neural populations. Different pools of neurons encode task-relevant information about action means, task goals and context in the form of self-sustained activation patterns. These patterns are triggered by input from connected populations and evolve continuously in time under the influence of recurrent interactions. The dynamic model of joint action is evaluated in a task in which a robot and a human jointly construct a toy object. We show that the highly context sensitive mapping from action observation onto appropriate complementary actions allows coping with dynamically changing joint action situations. ?? 2010 Elsevier B.V.</td>
</tr>
<tr id="bib_Bicho2011" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Bicho2011,
  author = {Bicho, Estela and Erlhagen, Wolfram and Louro, Luis and Costa e Silva, Eliana},
  title = {Neuro-cognitive mechanisms of decision making in joint action: A human-robot interaction study},
  journal = {Human Movement Science},
  publisher = {Elsevier B.V.},
  year = {2011},
  volume = {30},
  number = {5},
  pages = {846--868},
  url = {http://dx.doi.org/10.1016/j.humov.2010.08.012},
  doi = {http://dx.doi.org/10.1016/j.humov.2010.08.012}
}
</pre></td>
</tr>
<tr id="Brooks1989" class="entry">
	<td>Brooks, R.A.</td>
	<td>A Robot that Walks; Emergent Behaviors from a Carefully Evolved Network <p class="infolinks">[<a href="javascript:toggleInfo('Brooks1989','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Brooks1989','bibtex')">BibTeX</a>]</p></td>
	<td>1989</td>
	<td>Neural Computation<br/>Vol. 1(2), pp. 253-262&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1162/neco.1989.1.2.253">DOI</a> <a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.2.253">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Brooks1989" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Most animals have significant behavioral expertise built in without having to explicitly learn it all from scratch. This expertise is a product of evolution of the organism; it can be viewed as a very long-term form of learning which provides a structured system within which individuals might learn more specialized skills or abilities. This paper suggests one possible mechanism for analagous robot evolution by describing a carefully designed series of networks, each one being a strict augmentation of the previous one, which control a six-legged walking machine capable of walking over rough terrain and following a person passively sensed in the infrared spectrum. As the completely decentralized networks are augmented, the robot's performance and behavior repertoire demonstrably improve. The rationale for such demonstrations is that they may provide a hint as to the requirements for automatically building massive networks to carry out complex sensory-motor tasks. The experiments with an actual robot ensure ...</td>
</tr>
<tr id="bib_Brooks1989" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Brooks1989,
  author = {Brooks, Rodney A.},
  title = {A Robot that Walks; Emergent Behaviors from a Carefully Evolved Network},
  journal = {Neural Computation},
  year = {1989},
  volume = {1},
  number = {2},
  pages = {253--262},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.2.253},
  doi = {http://dx.doi.org/10.1162/neco.1989.1.2.253}
}
</pre></td>
</tr>
<tr id="Bullard2016" class="entry">
	<td>Bullard, K.</td>
	<td>Embodied queries for robot task learning <p class="infolinks">[<a href="javascript:toggleInfo('Bullard2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bullard2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>ACM/IEEE International Conference on Human-Robot Interaction<br/>Vol. 2016-April, pp. 599-600&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/HRI.2016.7451875">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Bullard2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Personal robots are intended to assist people in their daily lives; therefore, it is important for them to efficiently learn how to accomplish tasks delineated by their human partners, in the environment in which they are situated. Toward that end however, it is arguably even more important to enable these robots to characterize their own uncertainty as they attempt to generalize task-specific knowledge learned and equip them with a domain-independent framework for asking the appropriate questions of their human partners to resolve the uncertainty. The work presented in this extended abstract is a high level overview of current and future work that we are developing in this domain, using learning from demonstration and active robot learning. I.</td>
</tr>
<tr id="bib_Bullard2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Bullard2016,
  author = {Bullard, Kalesha},
  title = {Embodied queries for robot task learning},
  journal = {ACM/IEEE International Conference on Human-Robot Interaction},
  year = {2016},
  volume = {2016-April},
  pages = {599--600},
  doi = {http://dx.doi.org/10.1109/HRI.2016.7451875}
}
</pre></td>
</tr>
<tr id="Calinoti1997" class="entry">
	<td>Calinoti, S.</td>
	<td>Skills learning in robots by interaction with users and environment <p class="infolinks">[<a href="javascript:toggleInfo('Calinoti1997','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Calinoti1997','bibtex')">BibTeX</a>]</p></td>
	<td>1997</td>
	<td>2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2014(Urai), pp. 161-162&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/URAI.2014.7057522">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Calinoti1997" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The fast technological evolution and dissemination of multimodal sensors and compliant actuators bring a new human-centric perspective to robotics. The variety of human-robot interactions that stem from these new capabilities unveil compelling challenges for machine learning. The aim of this paper is to provide robots with a representation of rich motor skills able to handle recognition, prediction, synthesis and refinement in a continuous and synergistic way. It also requires to be robust to various sources of perturbation, persistently arising from the environment, from the user, and from the robot. One important challenge in this direction is to devise an encoding scheme that is able to generalize tasks to new situations, that can potentially act in multiple coordinate systems, and that can exploit the modern compliant control capabilities of robots to generate natural, efficient and safe movements for the surrounding users.</td>
</tr>
<tr id="bib_Calinoti1997" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Calinoti1997,
  author = {Calinoti, Sylvain},
  title = {Skills learning in robots by interaction with users and environment},
  journal = {2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2014},
  year = {1997},
  number = {Urai},
  pages = {161--162},
  doi = {http://dx.doi.org/10.1109/URAI.2014.7057522}
}
</pre></td>
</tr>
<tr id="Cao2008" class="entry">
	<td>Cao, M., Stewart, A. and Leonard, N.E.</td>
	<td>Integrating Human and Robot Decision-Making Dynamics with Feedback: Modesl and Convergence Analysis <p class="infolinks">[<a href="javascript:toggleInfo('Cao2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cao2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Proceedings of the 47th IEEE Conference on Decision and Control, pp. 8&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/CDC.2008.4739103">DOI</a> <a href="papers://a95032f2-422b-4b2a-927a-da2e44727e65/Paper/p4925">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Cao2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Leveraging research by psychologists on human decision-making, we present a human-robot decision-making problem associated with a complex task and study the cor- responding joint decision-making dynamics. The collaborative task is designed so that the human makes decisions just as human subjects make decisions in the two-alternative, forced- choice task, a well-studied decision-making task in behavioral experiments. The human subject chooses between two options at regular time intervals and receives a reward after each choice; for a variety of reward structures, the behavioral experiments show convergence to suboptimal choices. We propose a human- supervised robot foraging problem in which the human super- visor makes a sequence of binary decisions to assign the role of each robot in a group in response to a report from the robots on their resource return. We discuss conditions under which the decision dynamics of this human-robot task is reasonably well approximated by the kinds of reward structures studied in the psychology experiments. Using the Win-Stay, Lose- Switch human decision-making model, we prove convergence to the experimentally observed aggregate human decision-making behavior for reward structures with matching points. Finally, we propose an adaptive law for robot reward feedback designed to help the human make optimal decisions. I.</td>
</tr>
<tr id="bib_Cao2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Cao2008,
  author = {Cao, M and Stewart, A and Leonard, N E},
  title = {Integrating Human and Robot Decision-Making Dynamics with Feedback: Modesl and Convergence Analysis},
  journal = {Proceedings of the 47th IEEE Conference on Decision and Control},
  year = {2008},
  pages = {8},
  url = {papers://a95032f2-422b-4b2a-927a-da2e44727e65/Paper/p4925},
  doi = {http://dx.doi.org/10.1109/CDC.2008.4739103}
}
</pre></td>
</tr>
<tr id="Cao2008a" class="entry">
	<td>Cao, M., Stewart, A. and Leonard, N.E.</td>
	<td>Integrating Human and Robot Decision-Making Dynamics with Feedback: Modesl and Convergence Analysis <p class="infolinks">[<a href="javascript:toggleInfo('Cao2008a','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Proceedings of the 47th IEEE Conference on Decision and Control, pp. 8&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/CDC.2008.4739103">DOI</a> <a href="papers://a95032f2-422b-4b2a-927a-da2e44727e65/Paper/p4925">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Cao2008a" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Cao2008a,
  author = {Cao, M and Stewart, A and Leonard, N E},
  title = {Integrating Human and Robot Decision-Making Dynamics with Feedback: Modesl and Convergence Analysis},
  journal = {Proceedings of the 47th IEEE Conference on Decision and Control},
  year = {2008},
  pages = {8},
  url = {papers://a95032f2-422b-4b2a-927a-da2e44727e65/Paper/p4925},
  doi = {http://dx.doi.org/10.1109/CDC.2008.4739103}
}
</pre></td>
</tr>
<tr id="Cao1997" class="entry">
	<td>Cao, Y., Fukunaga, A., Kahng, A. and Meng, F.</td>
	<td>Cooperative mobile robotics: antecedents and directions <p class="infolinks">[<a href="javascript:toggleInfo('Cao1997','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cao1997','bibtex')">BibTeX</a>]</p></td>
	<td>1997</td>
	<td>Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots<br/>Vol. 23, pp. 226-234&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.1995.525801">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=525801">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Cao1997" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: There has been increased research interest in systems composed of multiple autonomous mobile $nrobots exhibiting cooperative behavior. Groups of mobile robots are constructed, with an aim to studying $nsuch issues as group architecture, resource conflict, origin of cooperation, learning, and geometric prob- $nlems. As yet, few applications of cooperative robotics have been reported, and supporting theory is still in $nits formative stages. In this paper, we give a critical survey of existing works and discuss open problems $nin this field, emphasizing the various theoretical issues that arise in the study of cooperative robotics. We $ndescribe the intellectual heritages that have guided early research, as well as possible additions to the set $nof existing motivations.</td>
</tr>
<tr id="bib_Cao1997" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Cao1997,
  author = {Cao, Y.U. and Fukunaga, A.S. and Kahng, A.B. and Meng, F.},
  title = {Cooperative mobile robotics: antecedents and directions},
  journal = {Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots},
  year = {1997},
  volume = {23},
  pages = {226--234},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=525801},
  doi = {http://dx.doi.org/10.1109/IROS.1995.525801}
}
</pre></td>
</tr>
<tr id="Chalup2007" class="entry">
	<td>Chalup, S.K., Murch, C.L. and Quinlan, M.J.</td>
	<td>Machine learning with AIBO robots in the four-legged league of RoboCup <p class="infolinks">[<a href="javascript:toggleInfo('Chalup2007','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chalup2007','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews<br/>Vol. 37(3), pp. 297-310&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TSMCC.2006.886964">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Chalup2007" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robot learning is a growing area of research at the intersection of robotics and machine learning. The main contributions of this paper include a review of how machine learning has been used on Sony AIBO robots and at RoboCup, with a focus on the four-legged league during the years 1998-2004. The review shows that the application-oriented use of machine learning in the four-legged league was still conservative and restricted to a few well-known and easy-to-use methods such as standard decision trees, evolutionary hill climbing, and support vector machines. Method-oriented spin-off studies emerged more frequently and increasingly addressed new and advanced machine learning techniques. Further, the paper presents some details about the growing impact of machine learning in the software system developed by the authors' robot soccer team-the NUbots</td>
</tr>
<tr id="bib_Chalup2007" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chalup2007,
  author = {Chalup, Stephan K. and Murch, Craig L. and Quinlan, Michael J.},
  title = {Machine learning with AIBO robots in the four-legged league of RoboCup},
  journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
  year = {2007},
  volume = {37},
  number = {3},
  pages = {297--310},
  doi = {http://dx.doi.org/10.1109/TSMCC.2006.886964}
}
</pre></td>
</tr>
<tr id="Chen1997" class="entry">
	<td>Chen, C., Xu, Y. and Yang, J.</td>
	<td>Human action learning via Hidden Markov Model <p class="infolinks">[<a href="javascript:toggleInfo('Chen1997','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chen1997','bibtex')">BibTeX</a>]</p></td>
	<td>1997</td>
	<td>IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans<br/>Vol. 27(1), pp. 34-44&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/3468.553220">DOI</a> <a href="http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=553220">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chen1997" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: To successfully interact with and learn from humans in cooperative modes, robots need a mechanism for recognizing, characterizing, and emulating human skills. In particular, it is our interest to develop the mechanism for recognizing and emulating simple human actions, i.e., a simple activity in a manual operation where no sensory feedback is available. To this end, we have developed a method to model such actions using a hidden Markov model (HMM) representation. We proposed an approach to address two critical problems in action modeling: classifying human action-intent, and learning human skill, for which we elaborated on the method, procedure, and implementation issues in this paper. This work provides a framework for modeling and learning human actions from observations. The approach can be applied to intelligent recognition of manual actions and high-level programming of control input within a supervisory control paradigm, as well as automatic transfer of human skills to robotic systems</td>
</tr>
<tr id="bib_Chen1997" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chen1997,
  author = {Chen, C.S. and Xu, Y and Yang, Jie},
  title = {Human action learning via Hidden Markov Model},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  year = {1997},
  volume = {27},
  number = {1},
  pages = {34--44},
  url = {http://ieeexplore.ieee.org/xpls/absall.jsp?arnumber=553220},
  doi = {http://dx.doi.org/10.1109/3468.553220}
}
</pre></td>
</tr>
<tr id="Cheng2014" class="entry">
	<td>Cheng, L. and Pan, S.J.</td>
	<td>Semi-supervised domain adaptation on manifolds <p class="infolinks">[<a href="javascript:toggleInfo('Cheng2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cheng2014','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>IEEE Transactions on Neural Networks and Learning Systems<br/>Vol. 25(12), pp. 2240-2249&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TNNLS.2014.2308325">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Cheng2014" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In real-life problems, the following semi-supervised domain adaptation scenario is often encountered: we have full access to some source data, which is usually very large; the target data distribution is under certain unknown transformation of the source data distribution; meanwhile, only a small fraction of the target instances come with labels. The goal is to learn a prediction model by incorporating information from the source domain that is able to generalize well on the target test instances. We consider an explicit form of transformation functions and especially linear transformations that maps examples from the source to the target domain, and we argue that by proper preprocessing of the data from both source and target domains, the feasible transformation functions can be characterized by a set of rotation matrices. This naturally leads to an optimization formulation under the special orthogonal group constraints. We present an iterative coordinate descent solver that is able to jointly learn the transformation as well as the model parameters, while the geodesic update ensures the manifold constraints are always satisfied. Our framework is sufficiently general to work with a variety of loss functions and prediction problems. Empirical evaluations on synthetic and real-world experiments demonstrate the competitive performance of our method with respect to the state-of-the-art.</td>
</tr>
<tr id="bib_Cheng2014" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Cheng2014,
  author = {Cheng, Li and Pan, Sinno Jialin},
  title = {Semi-supervised domain adaptation on manifolds},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year = {2014},
  volume = {25},
  number = {12},
  pages = {2240--2249},
  doi = {http://dx.doi.org/10.1109/TNNLS.2014.2308325}
}
</pre></td>
</tr>
<tr id="Dixon2004" class="entry">
	<td>Dixon, K. and Khosla, P.</td>
	<td>Learning by observation with mobile robots: a computational approach <p class="infolinks">[<a href="javascript:toggleInfo('Dixon2004','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Dixon2004','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004<br/>Vol. 1, pp. 1-6&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2004.1307136">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Dixon2004" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  We present a computational approach to learning by observation (LBO) that allows users to program mobile robots by demonstrating a task. Unlike previous approaches, our system incorporates statistical-learning techniques and concepts from control theory to reduce the amount of domain knowledge needed to infer the intent of the user. To improve the generalization ability of the system, the user can demonstrate the task multiple times. We extract task subgoals from these demonstrations and automatically associate them with objects in the environment. As these objects move, the subgoals are updated accordingly. This gives our system the ability to learn from demonstrations performed in different environments. In this paper, we present the concepts used in our LBO system as well as experimental laboratory results in learning motor-skill tasks.</td>
</tr>
<tr id="bib_Dixon2004" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Dixon2004,
  author = {Dixon, K.R. and Khosla, P.K.},
  title = {Learning by observation with mobile robots: a computational approach},
  journal = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},
  year = {2004},
  volume = {1},
  pages = {1--6},
  doi = {http://dx.doi.org/10.1109/ROBOT.2004.1307136}
}
</pre></td>
</tr>
<tr id="Doisy2014" class="entry">
	<td>Doisy, G., Meyer, J. and Edan, Y.</td>
	<td>The impact of human-robot interface design on the use of a learning robot system <p class="infolinks">[<a href="javascript:toggleInfo('Doisy2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Doisy2014','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>IEEE Transactions on Human-Machine Systems<br/>Vol. 44(6), pp. 788-795&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/THMS.2014.2331618">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Doisy2014" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: There is limited knowledge on how to design effective interfaces to enable nonexpert users to interact with robot learning algorithms. This paper focuses on an interface design challenge: How to provide the user with sufficient information about the learned behavior. A simulated robotic task where the robot has online learning capabilities was developed. This platform was used to study the impact of the variability of the environmental conditions, the information provided on the relation between the learned robot behavior and the conditions in the environment, and the presence of a preview of the learned robot behavior on the use of the system and on task performance. The results show significant effects of the type and the quantity of displayed information. Forty-two participants made the best use of brief and contextualized notifications about changes in the environment: their presence improved the overall performance and the usage of the automation and reduced the workload. In contrast, adding previews of the learned behavior surprisingly impaired performance and reduced the use of automation.</td>
</tr>
<tr id="bib_Doisy2014" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Doisy2014,
  author = {Doisy, Guillaume and Meyer, Joachim and Edan, Yael},
  title = {The impact of human-robot interface design on the use of a learning robot system},
  journal = {IEEE Transactions on Human-Machine Systems},
  year = {2014},
  volume = {44},
  number = {6},
  pages = {788--795},
  doi = {http://dx.doi.org/10.1109/THMS.2014.2331618}
}
</pre></td>
</tr>
<tr id="Dominey2008" class="entry">
	<td>Dominey, P.F., Metta, G., Nori, F. and Natale, L.</td>
	<td>Anticipation and initiative in human-humanoid interaction <p class="infolinks">[<a href="javascript:toggleInfo('Dominey2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Dominey2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>2008 8th IEEE-RAS International Conference on Humanoid Robots, Humanoids 2008, pp. 693-699&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICHR.2008.4755974">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Dominey2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: One of the long-term goals for humanoid robotics is to have these robots working side-by side with humans, helping the humans in a variety of open ended tasks, which can change in real-time. In such contexts a crucial component of the robot behavior will be to adapt as rapidly as possible to regularities that can be learned from the human. This will allow the robot to anticipate predictable events, in order to render the interaction more fluid. This will be particularly pertinent in the context of tasks that will be repeated several times, or that contain sub-tasks that will be repeated within the global task. Through exposure to repetition the robot should automatically extract and exploit the underlying regularities. Here we present results from human-robot cooperation experiments in the context of a cooperative assembly task. The architecture is characterized by the maintenance and use of an ldquointeraction historyrdquo - a literal record of all past interactions that have taken place. During on-line interaction, the system continuously searches the interaction history for sequences whose onset matches the actions that are currently being invoked. Recognition of such matches allows the robot to take different levels of anticipatory activity. As predicted sequences are successively validated by the user, the level of anticipation and learning increases. Level 1 anticipation allows the system to predict what the user will say, and thus eliminate the need for verification when the prediction holds. At Level 2 allows the system to take initiative to propose the predicted next event. At Level 3, the robot is highly confident and takes initiative to perform the predicted action. We demonstrate how these progressive levels render the cooperative interaction more fluid and more rapid. Implications for further refinement in the quality of human-robot cooperation are discussed.</td>
</tr>
<tr id="bib_Dominey2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Dominey2008,
  author = {Dominey, Peter Ford and Metta, Giorgio and Nori, Francesco and Natale, Lorenzo},
  title = {Anticipation and initiative in human-humanoid interaction},
  journal = {2008 8th IEEE-RAS International Conference on Humanoid Robots, Humanoids 2008},
  year = {2008},
  pages = {693--699},
  doi = {http://dx.doi.org/10.1109/ICHR.2008.4755974}
}
</pre></td>
</tr>
<tr id="Eyssel2011" class="entry">
	<td>Eyssel, F., Kuchenbrandt, D. and Bobinger, S.</td>
	<td>Effects of anticipated human-robot interaction and predictability of robot behavior on perceptions of anthropomorphism <p class="infolinks">[<a href="javascript:toggleInfo('Eyssel2011','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Eyssel2011','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Human-Robot Interaction (HRI), 2011 6th ACM/IEEE International Conference on, pp. 61-67&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1145/1957656.1957673">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Eyssel2011" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recent research has shown that anthropomorphism represents a means to facilitate HRI. Under which conditions do people anthropomorphize robots and other nonhuman agents? This research question was investigated in an experiment that manipulated participants' anticipation of a prospective human-robot interaction (HRI) with a robot whose behavior was characterized by either low or high predictability. We examined effects of these factors on perceptions of anthropomorphism and acceptance of the robot. Innovatively, the present research demonstrates that anticipation of HRI with an unpredictable agent increased anthropomorphic inferences and acceptance of the robot. Implications for future research on psychological determinants of anthropomorphism are discussed.</td>
</tr>
<tr id="bib_Eyssel2011" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Eyssel2011,
  author = {Eyssel, F and Kuchenbrandt, D and Bobinger, S},
  title = {Effects of anticipated human-robot interaction and predictability of robot behavior on perceptions of anthropomorphism},
  journal = {Human-Robot Interaction (HRI), 2011 6th ACM/IEEE International Conference on},
  year = {2011},
  pages = {61--67},
  doi = {http://dx.doi.org/10.1145/1957656.1957673}
}
</pre></td>
</tr>
<tr id="Finn2016" class="entry">
	<td>Finn, C., Goodfellow, I. and Levine, S.</td>
	<td>Unsupervised Learning for Physical Interaction through Video Prediction <p class="infolinks">[<a href="javascript:toggleInfo('Finn2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Finn2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Advances in Neural Information Processing Systems&nbsp;</td>
	<td>article</td>
	<td><a href="http://arxiv.org/abs/1605.07157">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Finn2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 50,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method not only produces more accurate video predictions, but also more accurately predicts object motion, when compared to prior methods.</td>
</tr>
<tr id="bib_Finn2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Finn2016,
  author = {Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
  title = {Unsupervised Learning for Physical Interaction through Video Prediction},
  journal = {Advances in Neural Information Processing Systems},
  year = {2016},
  url = {http://arxiv.org/abs/1605.07157}
}
</pre></td>
</tr>
<tr id="Fitzpatrick2003" class="entry">
	<td>Fitzpatrick, P.M., Metta, G., Natale, L., Rao, S. and Sandini, G.</td>
	<td>Learning About Objects Through Action - Initial Steps Towards Artificial Cognition <p class="infolinks">[<a href="javascript:toggleInfo('Fitzpatrick2003','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Fitzpatrick2003','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>Procceedings of the ICRA'03 IEEE International Conference on Robotics and Automation, pp. 3140-3145&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2003.1242073">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Fitzpatrick2003" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Within the ﬁeld of Neuro Robotics we are driven primarily by the desire to understand how humans and animals live and grow and solve every day's problems.To this aim we adopted a “learn by doing” approach by building artiﬁcial systems, e.g. robots that not only look like human beings but also represent a model of some brain process. They should, ideally, behave and interact like human beings (being situated). The main emphasis in robotics has been on systems that act as a reaction to an external stimulus (e.g. tracking, reaching), rather than as a result of an internal drive to explore or “understand” the environment. We think it is now appropriate to try to move from acting, in the sense explained above, to “understanding”. As a starting point we addressed the problem of learning about the effects and consequences of self-generated actions. How does the robot learn how to pull an object toward itself or to push it away? How does the robot learn that spherical objects roll while a cube only slides if pushed? Interacting with objects is important because it implicitly explores object representation, event understanding, and can provide deﬁnition of objecthood that could not be grasped with a mere passive observation of the world. Further, learning to understand what one's own body can do is an essential step toward learning by imitation. In this view two actions are similar not only if their kinematics and dynamics are similar but rather if the effects on the external world are the same. Along this line of research we discuss some recent experiments performed at the AILab at MIT and at the LIRA-Lab at the University of Genova on COG and Babybot respectively. We show how the humanoid robots can learn how to poke and prod objects to obtain a consistently repeatable effect (e.g. sliding in a given direction), to help visual segmentation, and to interpret a poking action performed by a human manipulator.</td>
</tr>
<tr id="bib_Fitzpatrick2003" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Fitzpatrick2003,
  author = {Fitzpatrick, Paul M. and Metta, Giorgio and Natale, Lorenzo and Rao, Sajit and Sandini, Giulio},
  title = {Learning About Objects Through Action - Initial Steps Towards Artificial Cognition},
  journal = {Procceedings of the ICRA'03 IEEE International Conference on Robotics and Automation},
  year = {2003},
  pages = {3140--3145},
  doi = {http://dx.doi.org/10.1109/ROBOT.2003.1242073}
}
</pre></td>
</tr>
<tr id="Forbus1993" class="entry">
	<td>Forbus, K.D.</td>
	<td>Qualitative process theory: twelve years after <p class="infolinks">[<a href="javascript:toggleInfo('Forbus1993','bibtex')">BibTeX</a>]</p></td>
	<td>1993</td>
	<td>Artificial Intelligence<br/>Vol. 59(1-2), pp. 115-123&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/0004-3702(93)90177-D">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Forbus1993" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Forbus1993,
  author = {Forbus, Kenneth D.},
  title = {Qualitative process theory: twelve years after},
  journal = {Artificial Intelligence},
  year = {1993},
  volume = {59},
  number = {1-2},
  pages = {115--123},
  doi = {http://dx.doi.org/10.1016/0004-3702(93)90177-D}
}
</pre></td>
</tr>
<tr id="Fukumoto1995" class="entry">
	<td>Fukumoto, S., Miyajima, H., Kishida, K. and Nagasawa, Y.</td>
	<td>A destructive learning method of fuzzy inference rules <p class="infolinks">[<a href="javascript:toggleInfo('Fukumoto1995','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Fukumoto1995','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>Proceedings of 1995 IEEE International Conference on Fuzzy Systems<br/>Vol. 2, pp. 203-210&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/FUZZY.1992.258618">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Fukumoto1995" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In order to construct a fuzzy system with a learning function, numerous studies combining fuzzy systems and neural networks (or descent method) are being carried out. The self-tuning method using the descent method has been proposed by Ichihashi et al. (1991) and it is known that the constructive method is more powerful than other methods using neural networks (or descent method). But this method does not have a sufficient generalization capability or an expressing capability for the acquired knowledge. In this paper, we propose a new learning method called a destructive method of fuzzy inference rules by the descent method. And we show that the destructive method is superior in the number of rules and inference errors but inferior in learning speed to the constructive one. Further more, in order to improve learning speed, we propose a learning method combining the constructive and the destructive methods. Some numerical examples are given to show the validity of the proposed methods, and applications of these methods to the obstacle avoidance problem are shown</td>
</tr>
<tr id="bib_Fukumoto1995" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Fukumoto1995,
  author = {Fukumoto, S and Miyajima, H and Kishida, K and Nagasawa, Y},
  title = {A destructive learning method of fuzzy inference rules},
  journal = {Proceedings of 1995 IEEE International Conference on Fuzzy Systems},
  year = {1995},
  volume = {2},
  pages = {203--210},
  doi = {http://dx.doi.org/10.1109/FUZZY.1992.258618}
}
</pre></td>
</tr>
<tr id="Garrell2013" class="entry">
	<td>Garrell, A., Villamizar, M., Moreno-Noguer, F. and Sanfeliu, A.</td>
	<td>Proactive behavior of an autonomous mobile robot for human-assisted learning <p class="infolinks">[<a href="javascript:toggleInfo('Garrell2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Garrell2013','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proceedings - IEEE International Workshop on Robot and Human Interactive Communication, pp. 107-113&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ROMAN.2013.6628463">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Garrell2013" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: During the last decade, there has been a growing interest in making autonomous social robots able to interact with people. However, there are still many open issues regarding the social capabilities that robots should have in order to perform these interactions more naturally. In this paper we present the results of several experiments conducted at the Barcelona Robot Lab in the campus of the “Universitat Politècnica de Catalunya” in which we have analyzed different important aspects of the interaction between a mobile robot and nontrained human volunteers. First, we have proposed different robot behaviors to approach a person and create an engagement with him/her. In order to perform this task we have provided the robot with several perception and action capabilities, such as that of detecting people, planning an approach and verbally communicating its intention to initiate a conversation. Once the initial engagement has been created, we have developed further communication skills in order to let people assist the robot and improve its face recognition system. After this assisted and online learning stage, the robot becomes able to detect people under severe changing conditions, which, in turn enhances the number and the manner that subsequent human-robot interactions are performed.</td>
</tr>
<tr id="bib_Garrell2013" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Garrell2013,
  author = {Garrell, A. and Villamizar, M. and Moreno-Noguer, F. and Sanfeliu, A.},
  title = {Proactive behavior of an autonomous mobile robot for human-assisted learning},
  journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
  year = {2013},
  pages = {107--113},
  doi = {http://dx.doi.org/10.1109/ROMAN.2013.6628463}
}
</pre></td>
</tr>
<tr id="Ghalamzan2015" class="entry">
	<td>Ghalamzan, A., Paxton, C., Hager, G.D. and Bascetta, L.</td>
	<td>An Incremental Approach to Learning Generalizable Robot Tasks from Human Demonstration An Incremental Approach to Learning Generalizable Robot Tasks from Human Demonstration <p class="infolinks">[<a href="javascript:toggleInfo('Ghalamzan2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ghalamzan2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Icra(Conference Paper), pp. 5616-5621&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2015.7139985">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Ghalamzan2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Dynamic MovementPrimitives (DMPs) are a common method for learning a control policy for a task from demonstration. This control policy consists of differential equations that can create a smooth trajectory to a new goal point. However, DMPs only have a limited ability to generalize the demonstration to new environments and solve problems such as obstacle avoidance. Moreover, standard DMP learning does not cope with the noise inherent to human demonstra- tions. Here, we propose an approach for robot learning from demonstration that can generalize noisy task demonstrations to a new goal point and to an environment with obstacles. This strategy for robot learning from demonstration results in a control policy that incorporates different types of learning from demonstration, which correspond to different types of observational learning as outlined in developmental psychology.</td>
</tr>
<tr id="bib_Ghalamzan2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ghalamzan2015,
  author = {Ghalamzan, Amir and Paxton, Chris and Hager, Gregory D and Bascetta, Luca},
  title = {An Incremental Approach to Learning Generalizable Robot Tasks from Human Demonstration An Incremental Approach to Learning Generalizable Robot Tasks from Human Demonstration},
  journal = {Icra},
  year = {2015},
  number = {Conference Paper},
  pages = {5616--5621},
  doi = {http://dx.doi.org/10.1109/ICRA.2015.7139985}
}
</pre></td>
</tr>
<tr id="Gielniak2011" class="entry">
	<td>Gielniak, M.J. and Thomaz, A.L.</td>
	<td>Generating anticipation in robot motion <p class="infolinks">[<a href="javascript:toggleInfo('Gielniak2011','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gielniak2011','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Proceedings - IEEE International Workshop on Robot and Human Interactive Communication, pp. 449-454&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ROMAN.2011.6005255">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Gielniak2011" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robots that display anticipatory motion provide their human partners with greater time to respond in interactive tasks because human partners are aware of robot intent earlier. We create anticipatory motion autonomously from a single motion exemplar by extracting hand and body symbols that communicate motion intent and moving them earlier in the motion. We validate that our algorithm extracts the most salient frame (i.e. the correct symbol) which is the most informative about motion intent to human observers. Furthermore, we show that anticipatory variants allow humans to discern motion intent sooner than motions without anticipation, and that humans are able to reliably predict motion intent prior to the symbol frame when motion is anticipatory. Finally, we quantified the time range for robot motion when humans can perceive intent more accurately and the collaborative social benefits of anticipatory motion are greatest.</td>
</tr>
<tr id="bib_Gielniak2011" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Gielniak2011,
  author = {Gielniak, Michael J. and Thomaz, Andrea L.},
  title = {Generating anticipation in robot motion},
  journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
  year = {2011},
  pages = {449--454},
  doi = {http://dx.doi.org/10.1109/ROMAN.2011.6005255}
}
</pre></td>
</tr>
<tr id="Grant1989" class="entry">
	<td>Grant, E. and Feng, C.</td>
	<td>Experiments in robot learning <p class="infolinks">[<a href="javascript:toggleInfo('Grant1989','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Grant1989','bibtex')">BibTeX</a>]</p></td>
	<td>1989</td>
	<td>Proceedings IEEE International Symposium on Intelligent Control 1989<br/>Vol. 11(May), pp. 8-33&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ISIC.1989.238642">DOI</a> <a href="http://journals.cambridge.org/abstract{\_}S1365100507060208">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Grant1989" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The authors attempt to bridge the planning gap problem that exists between the AI (artificial intelligence) and robotics communities. the objective was determining how procedural and parameter knowledge could be represented and used in task planning. Because AI planners lack the ability to deal with kinematic and kinetic information of a world model, and robot planners possess poor reasoning ability, an advanced robotics research environment was considered the appropriate demonstrator. Experiments were conducted on two generic operations that are common to robotics work, grasping and pushing an object, working from a starting point that was randomly selected. Task parameter constraints are derived from rules induced from the small amounts of raw sensory data collected. These rules are then used to indicate whether a task, such as object grasp or push, could be successfully completed</td>
</tr>
<tr id="bib_Grant1989" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Grant1989,
  author = {Grant, E and Feng, C},
  title = {Experiments in robot learning},
  journal = {Proceedings IEEE International Symposium on Intelligent Control 1989},
  year = {1989},
  volume = {11},
  number = {May},
  pages = {8--33},
  url = {http://journals.cambridge.org/abstractS1365100507060208},
  doi = {http://dx.doi.org/10.1109/ISIC.1989.238642}
}
</pre></td>
</tr>
<tr id="Hawkins2014" class="entry">
	<td>Hawkins, K.P., Bansal, S., Vo, N.N. and Bobick, A.F.</td>
	<td>Anticipating human actions for collaboration in the presence of task and sensor uncertainty <p class="infolinks">[<a href="javascript:toggleInfo('Hawkins2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hawkins2014','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Proceedings - IEEE International Conference on Robotics and Automation, pp. 2215-2222&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2014.6907165">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Hawkins2014" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A representation for structured activities is developed that allows a robot to probabilistically infer which task actions a human is currently performing and to predict which future actions will be executed and when they will occur. The goal is to enable a robot to anticipate collaborative actions in the presence of uncertain sensing and task ambiguity. The system can represent multi-path tasks where the task variations may contain partially ordered actions or even optional actions that may be skipped altogether. The task is represented by an AND-OR tree structure from which a probabilistic graphical model is constructed. Inference methods for that model are derived that support a planning and execution system for the robot which attempts to minimize a cost function based upon expected human idle time. We demonstrate the theory in both simulation and actual human-robot performance of a two-way-branch assembly task. In particular we show that the inference model can robustly anticipate the actions of the human even in the presence of unreliable or noisy detections because of its integration of all its sensing information along with knowledge of task structure.</td>
</tr>
<tr id="bib_Hawkins2014" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hawkins2014,
  author = {Hawkins, Kelsey P. and Bansal, Shray and Vo, Nam N. and Bobick, Aaron F.},
  title = {Anticipating human actions for collaboration in the presence of task and sensor uncertainty},
  journal = {Proceedings - IEEE International Conference on Robotics and Automation},
  year = {2014},
  pages = {2215--2222},
  doi = {http://dx.doi.org/10.1109/ICRA.2014.6907165}
}
</pre></td>
</tr>
<tr id="He2015" class="entry">
	<td>He, W., Chen, Y., Yin, Z. and Member, S.</td>
	<td>Adaptive Neural Network Control of an Uncertain Robot With Full-State Constraints <p class="infolinks">[<a href="javascript:toggleInfo('He2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('He2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td><br/>Vol. 46(3), pp. 1-10&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TCYB.2015.2411285">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_He2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper studies the tracking control problem for an uncertain n-link robot with full-state constraints. The rigid robotic manipulator is described as a multiinput and multioutput system. Adaptive neural network (NN) control for the robotic sys- tem with full-state constraints is designed. In the control design, the adaptive NNs are adopted to handle system uncertainties and disturbances. The Moore–Penrose inverse term is employed in order to prevent the violation of the full-state constraints. A bar- rier Lyapunov function is used to guarantee the uniform ultimate boundedness of the closed-loop system. The control performance of the closed-loop system is guaranteed by appropriately choos- ing the design parameters. Simulation studies are performed to illustrate the effectiveness of the proposed control.</td>
</tr>
<tr id="bib_He2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{He2015,
  author = {He, Wei and Chen, Yuhao and Yin, Zhao and Member, Student},
  title = {Adaptive Neural Network Control of an Uncertain Robot With Full-State Constraints},
  year = {2015},
  volume = {46},
  number = {3},
  pages = {1--10},
  doi = {http://dx.doi.org/10.1109/TCYB.2015.2411285}
}
</pre></td>
</tr>
<tr id="Heping2015" class="entry">
	<td>Heping, C., Binbin, L., Gravel, D., Zhang, G. and Biao, Z.</td>
	<td>Robot Learning for Complex Manufacturing Process <p class="infolinks">[<a href="javascript:toggleInfo('Heping2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Heping2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>2015 IEEE International Conference on Industrial Technology, pp. 3207-3211&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/icit.2015.7125572">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Heping2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: At the present time, industrial robots for assembly tasks only constitute a small portion of the annual robot sales. One of the main reasons is that it is difficult for conventional industrial robots to adapt to the complicity and flexibility of assembly processes. Therefore, intelligent industrial robotic sys- tems are attracting more and more attention. However, because of the modeling difficulty and low efficiency of the existing solutions, optimal performance is difficult to achieve. In this paper, a parameter learning method is developed based on Gaussian Process Regression Bayesian Optimization Algorithm (GPRBOA). Gaussian Process Regression(GPR) is utilized to model the relationship between the process parameters and system performance. GPRBOA is proposed to optimize the process parameters. The experiments were performed using a complex three stage torque converter assembly process. The experimental results verify the effectiveness of the robot learning method and demonstrate its efficiency compared to Design Of Experiment(DOE) methods. The proposed method can greatly increase the manufacturing efficiency and will generate big economic impact. I.</td>
</tr>
<tr id="bib_Heping2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Heping2015,
  author = {Heping, Chen and Binbin, Li and Gravel, D and Zhang, G and Biao, Zhang},
  title = {Robot Learning for Complex Manufacturing Process},
  journal = {2015 IEEE International Conference on Industrial Technology},
  year = {2015},
  pages = {3207--3211},
  doi = {http://dx.doi.org/10.1109/icit.2015.7125572}
}
</pre></td>
</tr>
<tr id="Hertzberg2008" class="entry">
	<td>Hertzberg, J. and Chatila, R.</td>
	<td>AI Reasoning Methods for Robotics <p class="infolinks">[<a href="javascript:toggleInfo('Hertzberg2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hertzberg2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Springer Handbook of Robotics, pp. 207-223&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1007/978-3-540-30301-5_10">DOI</a> <a href="http://dx.doi.org/10.1007/978-3-540-30301-5{\_}10$\backslash$nhttp://link.springer.com/static-content/0.5480/pdf/297/chp:10.1007/978-3-540-30301-5{\_}10.pdf?token=1350544024456--eab43db8747278a681f490ceae92a6f33f2ea39eee350d873953f03ccfa233ad9599da612b6acb4c64e3da601f06fc">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Hertzberg2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Artificial intelligence (AI) reasoning technology involving, e.g., inference, planning, and learning, has a track record with a healthy number of successful applications. So, can it be used as a toolbox of methods for autonomous mobile robots? Not necessarily, as reasoning on a mobile robot about its dynamic, partially known environment may differ substantially from that in knowledge-based pure software systems, where most of the named successes have been registered. This Chapter sketches the main robotics-relevant topics of symbol-based AI reasoning. Basic methods of knowledge representation and inference are described in general, covering both logic- and probability-based approaches. Then, some robotics-related particularities are addressed specially: issues in logic-based high-level robot control, fuzzy logics, and reasoning under time constraints. Two generic applications of reasoning are then described in some detail: action planning and learning. General reasoning is currently not a standard feature onboard autonomous mobile robots. Beyond sketching the state of the art in robotics-related AI reasoning, this Chapter points to the involved research problems that remain to be solved towards that end. The Chapter first reviews knowledge representation and deduction in general (Sect. 9.1), and then goes into some detail regarding reasoning issues that are considered particularly relevant for applications in robots (Sect. 9.2). Having presented reasoning methods, we then enter the field of generic reasoning applications, namely, action planning (Sect. 9.3) and machine learning (Sect. 9.4). Section 9.5 concludes.</td>
</tr>
<tr id="bib_Hertzberg2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hertzberg2008,
  author = {Hertzberg, Joachim and Chatila, Raja},
  title = {AI Reasoning Methods for Robotics},
  journal = {Springer Handbook of Robotics},
  year = {2008},
  pages = {207--223},
  url = {http://dx.doi.org/10.1007/978-3-540-30301-510$nhttp://link.springer.com/static-content/0.5480/pdf/297/chp:10.1007/978-3-540-30301-510.pdf?token=1350544024456--eab43db8747278a681f490ceae92a6f33f2ea39eee350d873953f03ccfa233ad9599da612b6acb4c64e3da601f06fc},
  doi = {http://dx.doi.org/10.1007/978-3-540-30301-5_10}
}
</pre></td>
</tr>
<tr id="Hinton2006" class="entry">
	<td>Hinton, G.E., Osindero, S. and Teh, Y.W.</td>
	<td>A fast learning algorithm for deep belief nets. <p class="infolinks">[<a href="javascript:toggleInfo('Hinton2006','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hinton2006','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>Neural computation<br/>Vol. 18(7), pp. 1527-54&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1162/neco.2006.18.7.1527">DOI</a> <a href="http://www.ncbi.nlm.nih.gov/pubmed/16764513">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Hinton2006" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.</td>
</tr>
<tr id="bib_Hinton2006" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hinton2006,
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
  title = {A fast learning algorithm for deep belief nets.},
  journal = {Neural computation},
  year = {2006},
  volume = {18},
  number = {7},
  pages = {1527--54},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513},
  doi = {http://dx.doi.org/10.1162/neco.2006.18.7.1527}
}
</pre></td>
</tr>
<tr id="Hoffman2008" class="entry">
	<td>Hoffman, G. and Breazeal, C.</td>
	<td>Achieving fluency through perceptual-symbol practice in human-robot collaboration <p class="infolinks">[<a href="javascript:toggleInfo('Hoffman2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hoffman2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Proceedings of the 3rd international conference on Human robot interaction - HRI '08, pp. 1&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1145/1349822.1349824">DOI</a> <a href="http://portal.acm.org/citation.cfm?doid=1349822.1349824">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Hoffman2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We have developed a cognitive architecture for robotic teammates based on the neuro-psychological principles of perceptual symbols and simulation, with the aim of attaining increased fluency in human-robot teams. An instantiation of this architecture was implemented on a robotic desk lamp, performing in a human-robot collaborative task. This paper describes initial results from a human-subject study measuring team efficiency and team fluency, in which the robot works on a joint task with untrained subjects. We find significant differences in a number of efficiency and fluency metrics, when comparing our architecture to a purely reactive robot with similar capabilities.</td>
</tr>
<tr id="bib_Hoffman2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hoffman2008,
  author = {Hoffman, Guy and Breazeal, Cynthia},
  title = {Achieving fluency through perceptual-symbol practice in human-robot collaboration},
  journal = {Proceedings of the 3rd international conference on Human robot interaction - HRI '08},
  year = {2008},
  pages = {1},
  url = {http://portal.acm.org/citation.cfm?doid=1349822.1349824},
  doi = {http://dx.doi.org/10.1145/1349822.1349824}
}
</pre></td>
</tr>
<tr id="Hoffman2007" class="entry">
	<td>Hoffman, G. and Breazeal, C.</td>
	<td>Cost-based anticipatory action selection for human-robot fluency <p class="infolinks">[<a href="javascript:toggleInfo('Hoffman2007','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hoffman2007','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 23(5), pp. 952-961&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2007.907483">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Hoffman2007" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A crucial skill for fluent action meshing in human team activity is a learned and calculated selection of anticipatory actions.We believe that the same holds for robotic teammates, if they are to perform in a similarly fluent manner with their human counterparts. In this work, we describe a model for human-robot joint action, and propose an adaptive action selection mechanism for a robotic teammate, which makes anticipatory decisions based on the confidence of their validity and their relative risk. We conduct an analysis of our method, predicting an improvement in task efficiency compared to a purely reactive process. We then present results from a study involving untrained human subjects working with a simulated version of a robot using our system. We show a significant improvement in best-case task efficiency when compared to a group of users working with a reactive agent, as well as a significant difference in the perceived commitment of the robot to the team and its contribution to the team's fluency and success. By way of explanation, we raise a number of fluency metric hypotheses, and evaluate their significance between the two study conditions.</td>
</tr>
<tr id="bib_Hoffman2007" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hoffman2007,
  author = {Hoffman, Guy and Breazeal, Cynthia},
  title = {Cost-based anticipatory action selection for human-robot fluency},
  journal = {IEEE Transactions on Robotics},
  year = {2007},
  volume = {23},
  number = {5},
  pages = {952--961},
  doi = {http://dx.doi.org/10.1109/TRO.2007.907483}
}
</pre></td>
</tr>
<tr id="Hsu2005" class="entry">
	<td>Hsu, M.</td>
	<td>Neural Systems Responding to Degrees of Uncertainty in Human Decision-Making <p class="infolinks">[<a href="javascript:toggleInfo('Hsu2005','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hsu2005','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>Science<br/>Vol. 310(5754), pp. 1680-1683&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1126/science.1115327">DOI</a> <a href="http://www.sciencemag.org/cgi/doi/10.1126/science.1115327">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Hsu2005" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Much is known about how people make decisions under varying levels of prob- ability (risk). Less is known about the neural basis of decision-making when probabilities are uncertain because of missing information (ambiguity). In decision theory, ambiguity about probabilities should not affect choices. Using functional brain imaging, we show that the level of ambiguity in choices cor- relates positively with activation in the amygdala and orbitofrontal cortex, and negatively with a striatal system.Moreover, striatal activity correlates positively with expected reward. Neurological subjects with orbitofrontal lesions were insensitive to the level of ambiguity and risk in behavioral choices. These data suggest a general neural circuit responding to degrees of uncertainty, contrary to decision theory.</td>
</tr>
<tr id="bib_Hsu2005" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hsu2005,
  author = {Hsu, M.},
  title = {Neural Systems Responding to Degrees of Uncertainty in Human Decision-Making},
  journal = {Science},
  year = {2005},
  volume = {310},
  number = {5754},
  pages = {1680--1683},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1115327},
  doi = {http://dx.doi.org/10.1126/science.1115327}
}
</pre></td>
</tr>
<tr id="Huang2005" class="entry">
	<td>Huang, B.-Q., Cao, G.-Y. and Guo, M.</td>
	<td>Reinforcement Learning Neural Network To the Problem of Autonomous Mobile Robot Obstacle Avoidance <p class="infolinks">[<a href="javascript:toggleInfo('Huang2005','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Huang2005','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>International Conference on Machine Learning and Cybernetics(August), pp. 18-21&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_Huang2005" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An approach to the problem of autonomous mobile robot<br>obstacle avoidance using reinforcement learning neural network is proposed in this paper. Q-learning is one kind of reinforcement learning method that is similar to dynamic programming and the neural network has a powerful ability<br>to store the Q values. We integrate these two methods with<br>the aim to ensure autonomous robot behavior in complicated unpredictable environment. The simulation results show that the simulated robot using the reinforcement learning neural network can enhance its learning ability obviously and can finish the given task in a complex environment.<br>Keywords:</td>
</tr>
<tr id="bib_Huang2005" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Huang2005,
  author = {Huang, Bing-Qiang and Cao, Guang-Yi and Guo, Min},
  title = {Reinforcement Learning Neural Network To the Problem of Autonomous Mobile Robot Obstacle Avoidance},
  journal = {International Conference on Machine Learning and Cybernetics},
  year = {2005},
  number = {August},
  pages = {18--21}
}
</pre></td>
</tr>
<tr id="Hutchinson1996" class="entry">
	<td>Hutchinson, S., Hager, G. and Corke, P.I.</td>
	<td>A tutorial on visual servo control <p class="infolinks">[<a href="javascript:toggleInfo('Hutchinson1996','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hutchinson1996','bibtex')">BibTeX</a>]</p></td>
	<td>1996</td>
	<td>IEEE International Conference on Robotics and Automation<br/>Vol. 12(5), pp. 651-670&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/70.538972">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Hutchinson1996" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: his article provides a tutorial introduction to visual servo control of robotic manipulators. Since the topic spans many disciplines our goal is limited to providing a basic conceptual framework. We begin by reviewing the prerequisite topics from robotics and computer vision, including a brief review of coordi- nate transformations, velocity representation, and a description of the geometric aspects of the image formation process. We then present a taxonomy of visual servo control systems. The two major classes of systems, position-based and image-based systems, are then discussed in detail. Since any visual servo system must be capable of tracking image features in a sequence of images, we also include an overview of feature-based and correlation-based methods for tracking. We conclude the tutorial with a number of observations on the current directions of the research field of visual servo control</td>
</tr>
<tr id="bib_Hutchinson1996" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hutchinson1996,
  author = {Hutchinson, S.a. and Hager, G.D. and Corke, Peter I.},
  title = {A tutorial on visual servo control},
  journal = {IEEE International Conference on Robotics and Automation},
  year = {1996},
  volume = {12},
  number = {5},
  pages = {651--670},
  doi = {http://dx.doi.org/10.1109/70.538972}
}
</pre></td>
</tr>
<tr id="Iwata2005" class="entry">
	<td>Iwata, H. and Sugano, S.</td>
	<td>Human-robot-contact-state identification based on tactile recognition <p class="infolinks">[<a href="javascript:toggleInfo('Iwata2005','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Iwata2005','bibtex')">BibTeX</a>]</p></td>
	<td>2005</td>
	<td>IEEE Transactions on Industrial Electronics<br/>Vol. 52(6), pp. 1468-1477&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TIE.2005.858739">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Iwata2005" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we propose a method for designing an identification system for human-robot contact states based on tactile recognition. The following ideas are incorporated: experimentation for human-robot contact, verbalization of contact states, extraction of characteristic parameters from acquired tactile information, quantification of the recipient's tactile recognition incorporating its redundancy (identification confusability among contact states), evaluation of the identification confusability with a new criterion, and identification of contact states based on the received tactile stimulation. The proposed method allows a robot to quantify tactile recognition of a human (recipient) touched by other people (touch initiator), in which the verbal response by the recipient is matched with tactile stimulation acquired during physical contact utilizing a tactile interface. In addition, the method enables a robot that comes into contact with a human to identify contact states nearly similar to that of the recipient, based on the features of the received tactile stimulation. At this point, the reproduction of the identification confusability of the recipient's tactile recognition is also accomplished by using a neural network called modified counterpropagation (MCP). Once a tactile stimulation is induced on the robot body, the probability of corresponding contact states is calculated and outputted by the system, based on the degree of similarity of the characteristics between the newly received and previously stored tactile stimulation. Experimental results indicate that the constructed system allows a successful quantification of the recipient's contact-state recognition incorporating the identification confusability and the accomplishment of a high level of accuracy in contact-state identification. These results confirm that the proposed method is useful for identifying human-robot contact states based on tactile recognition.</td>
</tr>
<tr id="bib_Iwata2005" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Iwata2005,
  author = {Iwata, Hiroyasu and Sugano, Shigeki},
  title = {Human-robot-contact-state identification based on tactile recognition},
  journal = {IEEE Transactions on Industrial Electronics},
  year = {2005},
  volume = {52},
  number = {6},
  pages = {1468--1477},
  doi = {http://dx.doi.org/10.1109/TIE.2005.858739}
}
</pre></td>
</tr>
<tr id="Kanda2009" class="entry">
	<td>Kanda, T., Glas, D.F., Shiomi, M. and Hagita, N.</td>
	<td>Abstracting peoples trajectories for social robots to proactively approach customers <p class="infolinks">[<a href="javascript:toggleInfo('Kanda2009','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kanda2009','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 25(6), pp. 1382-1396&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2009.2032969">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Kanda2009" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: For a robot providing services to people in a public space such as a shopping mall, it is important to distinguish potential customers, such as window shoppers, from other people, such as busy commuters. In this paper, we present a series of abstraction techniques for people's trajectories and a service framework for using these techniques in a social robot, which enables a designer to make the robot proactively approach customers by only providing information about target local behavior. We placed a ubiquitous sensor network consisting of six laser range finders in a shopping arcade. The system tracks people's positions as well as their local behaviors, such as fast walking, idle walking, wandering, or stopping. We accumulated people's trajectories for a week, applying a clustering technique to the accumulated trajectories to extract information about the use of space and people's typical global behaviors. This information enables the robot to target its services to people who are walking idly or stopping. The robot anticipates both the areas in which people are likely to perform these behaviors as well as the probable local behaviors of individuals a few seconds in the future. In a field experiment, we demonstrate that this service framework enables the robot to serve people efficiently.</td>
</tr>
<tr id="bib_Kanda2009" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Kanda2009,
  author = {Kanda, Takayuki and Glas, Dylan F. and Shiomi, Masahiro and Hagita, Norihiro},
  title = {Abstracting peoples trajectories for social robots to proactively approach customers},
  journal = {IEEE Transactions on Robotics},
  year = {2009},
  volume = {25},
  number = {6},
  pages = {1382--1396},
  doi = {http://dx.doi.org/10.1109/TRO.2009.2032969}
}
</pre></td>
</tr>
<tr id="Koppula2013" class="entry">
	<td>Koppula, H.S. and Saxena, A.</td>
	<td>Anticipating human activities for reactive robotic response (supporting video) <p class="infolinks">[<a href="javascript:toggleInfo('Koppula2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Koppula2013','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2071&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2013.6696634">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Koppula2013" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles.</td>
</tr>
<tr id="bib_Koppula2013" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Koppula2013,
  author = {Koppula, Hema Swetha and Saxena, Ashutosh},
  title = {Anticipating human activities for reactive robotic response (supporting video)},
  journal = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2013},
  pages = {2071},
  doi = {http://dx.doi.org/10.1109/IROS.2013.6696634}
}
</pre></td>
</tr>
<tr id="Kuipers1993" class="entry">
	<td>Kuipers, B., Froom, R., Lee, W.-Y. and Pierce, D.</td>
	<td>The Semantic Hierarchy in Robot Learning <p class="infolinks">[<a href="javascript:toggleInfo('Kuipers1993','bibtex')">BibTeX</a>]</p></td>
	<td>1993</td>
	<td>Robot Learning, pp. 141-170&nbsp;</td>
	<td>inbook</td>
	<td><a href="http://dx.doi.org/10.1007/978-1-4615-3184-5_6">DOI</a> <a href="http://dx.doi.org/10.1007/978-1-4615-3184-5{\_}6">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Kuipers1993" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inbook{Kuipers1993,
  author = {Kuipers, Benjamin and Froom, Richard and Lee, Wan-Yik and Pierce, David},
  title = {The Semantic Hierarchy in Robot Learning},
  booktitle = {Robot Learning},
  publisher = {Springer US},
  year = {1993},
  pages = {141--170},
  url = {http://dx.doi.org/10.1007/978-1-4615-3184-56},
  doi = {http://dx.doi.org/10.1007/978-1-4615-3184-5_6}
}
</pre></td>
</tr>
<tr id="Kuniyoshi1994" class="entry">
	<td>Kuniyoshi, Y., Inaba, M. and Inoue, H.</td>
	<td>Learning by Watching: Extracting Reusable Task Knowledge from Visual Observation of Human Performance <p class="infolinks">[<a href="javascript:toggleInfo('Kuniyoshi1994','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kuniyoshi1994','bibtex')">BibTeX</a>]</p></td>
	<td>1994</td>
	<td>IEEE Transactions on Robotics and Automation<br/>Vol. 10(6), pp. 799-822&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/70.338535">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Kuniyoshi1994" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A novel task instruction method for future intelligent robots is presented, In our method, a robot learns reusable task plans by watching a human perform assembly tasks. Functional units and working algorithms for visual recognition and analysis of human action sequences are presented. The overall system is model based and integrated at the symbolic level. Temporal segmentation of a continuous task performance into meaningful units and identification of each operation is processed in real time by concurrent recognition processes under active attention control. Dependency among assembly operations in the recognized action sequence is analyzed, which results in a hierarchical task plan describing the higher level structure of the task. In another workspace with a different initial state, the system re-instantiates and executes the task plan to accomplish an equivalent goal. The effectiveness of our method is supported by experimental results with block assembly tasks</td>
</tr>
<tr id="bib_Kuniyoshi1994" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Kuniyoshi1994,
  author = {Kuniyoshi, Yasuo and Inaba, Masayuki and Inoue, Hirochika},
  title = {Learning by Watching: Extracting Reusable Task Knowledge from Visual Observation of Human Performance},
  journal = {IEEE Transactions on Robotics and Automation},
  year = {1994},
  volume = {10},
  number = {6},
  pages = {799--822},
  doi = {http://dx.doi.org/10.1109/70.338535}
}
</pre></td>
</tr>
<tr id="Kwon2012" class="entry">
	<td>Kwon, W.Y. and Suh, I.H.</td>
	<td>A temporal Bayesian network with application to design of a proactive robotic assistant <p class="infolinks">[<a href="javascript:toggleInfo('Kwon2012','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kwon2012','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Proceedings - IEEE International Conference on Robotics and Automation, pp. 3685-3690&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2012.6224673">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Kwon2012" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: For effective human-robot interaction, a robot should be able to make prediction about future circumstance. This enables the robot to generate preparative behaviors to reduce waiting time, thereby greatly improving the quality of the interaction. In this paper, we propose a novel probabilistic temporal prediction method for proactive interaction that is based on a Bayesian network approach. In our proposed method, conditional probabilities of temporal events can be explicitly represented by defining temporal nodes in a Bayesian network. Utilizing these nodes, both temporal and causal infor- mation can be simultaneously inferred in a unified framework. An assistant robot can use the temporal Bayesian network to infer the best proactive action and the best time to act so that the waiting time for both the human and the robot is minimized. To validate our proposed method, we present experimental results for case in which a robot assists in a human assembly task.</td>
</tr>
<tr id="bib_Kwon2012" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Kwon2012,
  author = {Kwon, Woo Young and Suh, Il Hong},
  title = {A temporal Bayesian network with application to design of a proactive robotic assistant},
  journal = {Proceedings - IEEE International Conference on Robotics and Automation},
  year = {2012},
  pages = {3685--3690},
  doi = {http://dx.doi.org/10.1109/ICRA.2012.6224673}
}
</pre></td>
</tr>
<tr id="Lenz2015" class="entry">
	<td>Lenz, I., Lee, H. and Saxena, A.</td>
	<td>Deep learning for detecting robotic grasps <p class="infolinks">[<a href="javascript:toggleInfo('Lenz2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lenz2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>The International Journal of Robotics Research<br/>Vol. 34(4-5), pp. 705-724&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1177/0278364914549607">DOI</a> <a href="http://ijr.sagepub.com/content/34/4-5/705.short">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lenz2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.</td>
</tr>
<tr id="bib_Lenz2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lenz2015,
  author = {Lenz, I. and Lee, H. and Saxena, A.},
  title = {Deep learning for detecting robotic grasps},
  journal = {The International Journal of Robotics Research},
  year = {2015},
  volume = {34},
  number = {4-5},
  pages = {705--724},
  url = {http://ijr.sagepub.com/content/34/4-5/705.short},
  doi = {http://dx.doi.org/10.1177/0278364914549607}
}
</pre></td>
</tr>
<tr id="Lin1991" class="entry">
	<td>Lin, C.T. and Lee, G.C.S.</td>
	<td>Neural-Network-Based Fuzzy Logic Control and Decision System <p class="infolinks">[<a href="javascript:toggleInfo('Lin1991','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lin1991','bibtex')">BibTeX</a>]</p></td>
	<td>1991</td>
	<td>IEEE Transactions on Computers<br/>Vol. 40(12), pp. 1320-1336&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/12.106218">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Lin1991" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A general neural-network (connectionist) model for fuzzy logic control and decision systems is proposed. This connectionist model, in the form of feedforward multilayer net, combines the idea of fuzzy logic controller and neural-network structure and learning abilities into an integrated neural-network-based fuzzy logic control and decision system. A fuzzy logic control decision network is constructed automatically by learning the training examples itself. By combining both unsupervised (self-organized) and supervised learning schemes, the learning speed converges much faster than the original backpropagation learning algorithm. The connectionist structure avoids the rule-matching time of the inference engine in the traditional fuzzy logic system. Two examples are presented to illustrate the performance and applicability of the proposed model</td>
</tr>
<tr id="bib_Lin1991" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lin1991,
  author = {Lin, Chin Teng and Lee, George C S},
  title = {Neural-Network-Based Fuzzy Logic Control and Decision System},
  journal = {IEEE Transactions on Computers},
  year = {1991},
  volume = {40},
  number = {12},
  pages = {1320--1336},
  doi = {http://dx.doi.org/10.1109/12.106218}
}
</pre></td>
</tr>
<tr id="Lippman1987" class="entry">
	<td>Lippman, R.P. and Lippman, R.P.</td>
	<td>An Intrduction to Computing with Neural Nets <p class="infolinks">[<a href="javascript:toggleInfo('Lippman1987','bibtex')">BibTeX</a>]</p></td>
	<td>1987</td>
	<td>IEEE ASSP Magazine(April), pp. 4-22&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/MASSP.1987.1165576">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Lippman1987" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lippman1987,
  author = {Lippman, R P and Lippman, R P},
  title = {An Intrduction to Computing with Neural Nets},
  journal = {IEEE ASSP Magazine},
  year = {1987},
  number = {April},
  pages = {4--22},
  doi = {http://dx.doi.org/10.1109/MASSP.1987.1165576}
}
</pre></td>
</tr>
<tr id="Low-rank-decomposition2004" class="entry">
	<td>Low-rank-decomposition, I.T.</td>
	<td>A Model of Saliency-Based Visual Attention for Rapid Scene Analysis <p class="infolinks">[<a href="javascript:toggleInfo('Low-rank-decomposition2004','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Low-rank-decomposition2004','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,<br/>Vol. 23(11), pp. 1575-1580&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TPAMI.2012.125">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Low-rank-decomposition2004" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.<br>Index</td>
</tr>
<tr id="bib_Low-rank-decomposition2004" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Low-rank-decomposition2004,
  author = {Low-rank-decomposition, Index Terms},
  title = {A Model of Saliency-Based Visual Attention for Rapid Scene Analysis},
  journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,},
  year = {2004},
  volume = {23},
  number = {11},
  pages = {1575--1580},
  doi = {http://dx.doi.org/10.1109/TPAMI.2012.125}
}
</pre></td>
</tr>
<tr id="Macek2002" class="entry">
	<td>Macek, K., Petrovic, I. and Peric, N.</td>
	<td>A reinforcement learning approach to obstacle avoidance of mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('Macek2002','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>7th International Workshop on Advanced Motion Control. Proceedings (Cat. No.02TH8623)(i), pp. 462-466&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/AMC.2002.1026964">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1026964">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Macek2002" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Macek2002,
  author = {Macek, K. and Petrovic, I. and Peric, N.},
  title = {A reinforcement learning approach to obstacle avoidance of mobile robots},
  journal = {7th International Workshop on Advanced Motion Control. Proceedings (Cat. No.02TH8623)},
  year = {2002},
  number = {i},
  pages = {462--466},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1026964},
  doi = {http://dx.doi.org/10.1109/AMC.2002.1026964}
}
</pre></td>
</tr>
<tr id="Maniezzo1994" class="entry">
	<td>Maniezzo, V.</td>
	<td>Genetic Evolution of the Topology and Weight Distribution of Neural Networks <p class="infolinks">[<a href="javascript:toggleInfo('Maniezzo1994','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Maniezzo1994','bibtex')">BibTeX</a>]</p></td>
	<td>1994</td>
	<td>IEEE Transactions on Neural Networks<br/>Vol. 5(1), pp. 39-53&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/72.265959">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Maniezzo1994" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper proposes a system based on a parallel genetic algorithm with enhanced encoding and operational abilities. The system, used to evolve feedforward artificial neural networks, has been applied to two widely different problem areas: Boolean function learning and robot control. It is shown that the good results obtained in both cases are due to two factors: first, the enhanced exploration abilities provided by the search-space reducing evolution of both coding granularity and network topology, and, second, the enhanced exploitational abilities due to a recently proposed cooperative local optimizing genetic operator.</td>
</tr>
<tr id="bib_Maniezzo1994" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Maniezzo1994,
  author = {Maniezzo, Vittorio},
  title = {Genetic Evolution of the Topology and Weight Distribution of Neural Networks},
  journal = {IEEE Transactions on Neural Networks},
  year = {1994},
  volume = {5},
  number = {1},
  pages = {39--53},
  doi = {http://dx.doi.org/10.1109/72.265959}
}
</pre></td>
</tr>
<tr id="Matar1997" class="entry">
	<td>Matar, M.J.</td>
	<td>Reinforcement Learning in the Multi-Robot Domain <p class="infolinks">[<a href="javascript:toggleInfo('Matar1997','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Matar1997','bibtex')">BibTeX</a>]</p></td>
	<td>1997</td>
	<td>Autonomous Robots<br/>Vol. 4, pp. 73-83&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1023/A:1008819414322">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Matar1997" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes a formulation of reinforcement learning that enables learning in noisy, dynamic environments such as in the complex concurrent multi-robot learning domain. The methodology involves minimiz-ing the learning space through the use of behaviors and conditions, and dealing with the credit assignment problem through shaped reinforcement in the form of heterogeneous reinforcement functions and progress estimators. We experimentally validate the approach on a group of four mobile robots learning a foraging task.</td>
</tr>
<tr id="bib_Matar1997" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Matar1997,
  author = {Matar, Maja J},
  title = {Reinforcement Learning in the Multi-Robot Domain},
  journal = {Autonomous Robots},
  year = {1997},
  volume = {4},
  pages = {73--83},
  doi = {http://dx.doi.org/10.1023/A:1008819414322}
}
</pre></td>
</tr>
<tr id="Medina2013" class="entry">
	<td>Medina, J.R., Lawitzky, M., Molin, A. and Hirche, S.</td>
	<td>Dynamic strategy selection for physical robotic assistance in partially known tasks <p class="infolinks">[<a href="javascript:toggleInfo('Medina2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Medina2013','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proceedings - IEEE International Conference on Robotics and Automation, pp. 1180-1186&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2013.6630721">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Medina2013" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: It is well-known that physical robotic assistance to humans is significantly enhanced by including human behavior anticipation into robot planning and control. The challenge arises when the human goal/plan is uncertain or unknown to the robot. In this paper we propose a novel control scheme which dynamically selects between a model-based and a model-free strategy depending on the level of disagreement between the human and the robot. The disagreement is measured in terms of the interaction force. A task specific model-based controller is selected when the human's motion intention coincides with the robot's goal. A model-free control scheme based on the human force as motion prediction source is selected in case of disagreement and when the human goal/plan is unknown. The benefits of this approach are demonstrated in a human user study on human-robot cooperative object transport through a 2D maze in virtual reality.</td>
</tr>
<tr id="bib_Medina2013" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Medina2013,
  author = {Medina, Jose Ramon and Lawitzky, Martin and Molin, Adam and Hirche, Sandra},
  title = {Dynamic strategy selection for physical robotic assistance in partially known tasks},
  journal = {Proceedings - IEEE International Conference on Robotics and Automation},
  year = {2013},
  pages = {1180--1186},
  doi = {http://dx.doi.org/10.1109/ICRA.2013.6630721}
}
</pre></td>
</tr>
<tr id="Montesano2008" class="entry">
	<td>Montesano, L., Lopes, M., Bernardino, a. and Santos-Victor, J.</td>
	<td>Learning Object Affordances: From Sensory--Motor Coordination to Imitation <p class="infolinks">[<a href="javascript:toggleInfo('Montesano2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Montesano2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 24(1), pp. 15-26&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2007.914848">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Montesano2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Affordances encode relationships between actions, objects, and effects. They play an important role on basic cognitive capabilities such as prediction and planning. We address the problem of learning affordances through the interaction of a robot with the environment, a key step to understand the world properties and develop social skills. We present a general model for learning object affordances using Bayesian networks integrated within a general developmental architecture for social robots. Since learning is based on a probabilistic model, the approach is able to deal with uncertainty, redundancy, and irrelevant information. We demonstrate successful learning in the real world by having an humanoid robot interacting with objects. We illustrate the benefits of the acquired knowledge in imitation games.</td>
</tr>
<tr id="bib_Montesano2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Montesano2008,
  author = {Montesano, L. and Lopes, M. and Bernardino, a. and Santos-Victor, J.},
  title = {Learning Object Affordances: From Sensory--Motor Coordination to Imitation},
  journal = {IEEE Transactions on Robotics},
  year = {2008},
  volume = {24},
  number = {1},
  pages = {15--26},
  doi = {http://dx.doi.org/10.1109/TRO.2007.914848}
}
</pre></td>
</tr>
<tr id="Muthugala2016" class="entry">
	<td>Muthugala, M.A.V.J. and Jayasekara, A.G.B.P.</td>
	<td>MIRob: An intelligent service robot that learns from interactive discussions while handling uncertain information in user instructions <p class="infolinks">[<a href="javascript:toggleInfo('Muthugala2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2nd International Moratuwa Engineering Research Conference, MERCon 2016, pp. 397-402&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/MERCon.2016.7480174">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Muthugala2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Muthugala2016,
  author = {Muthugala, M. A Viraj J and Jayasekara, A. G Buddhika P},
  title = {MIRob: An intelligent service robot that learns from interactive discussions while handling uncertain information in user instructions},
  journal = {2nd International Moratuwa Engineering Research Conference, MERCon 2016},
  year = {2016},
  pages = {397--402},
  doi = {http://dx.doi.org/10.1109/MERCon.2016.7480174}
}
</pre></td>
</tr>
<tr id="Nehmzow" class="entry">
	<td>Nehmzow, U.</td>
	<td>Vision Processing for Robot Learning <p class="infolinks">[<a href="javascript:toggleInfo('Nehmzow','bibtex')">BibTeX</a>]</p></td>
	<td></td>
	<td><br/>Vol. 26(2), pp. 121-130&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="bib_Nehmzow" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Nehmzow,
  author = {Nehmzow, Ulrich},
  title = {Vision Processing for Robot Learning},
  volume = {26},
  number = {2},
  pages = {121--130}
}
</pre></td>
</tr>
<tr id="Nicolescu2003" class="entry">
	<td>Nicolescu, M.N. and Mataric, M.J.</td>
	<td>Natural Methods for Robot Task Learning: Instructive Demonstrations, Generalization and Practice <p class="infolinks">[<a href="javascript:toggleInfo('Nicolescu2003','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Nicolescu2003','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent systems (AAMAS), pp. 241&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1145/860575.860614">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Nicolescu2003" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Among humans, teaching various tasks is a complex process which relies on multiple means for interaction and learning, both on the part of the teacher and of the learner. Used together, these modalities lead to effective teaching and learning approaches, respectively. In the robotics domain, task teaching has been mostly addressed by using only one or very few of these interactions. In this paper we present an approach for teaching robots that relies on the key features and the general approach people use when teaching each other: first give a demonstration, then allow the learner to refine the acquired capabilities by practicing under the teacher's supervision, involving a small number of trials. Depending on the quality of the learned task, the teacher may either demonstrate it again or provide specific feedback during the learner's practice trial for further refinement. Also, as people do during demonstrations, the teacher can provide simple instructions and informative cues, increasing the performance of learning. Thus, instructive demonstrations, generalization over multiple demonstrations and practice trials are essential features for a successful human-robot teaching approach. We implemented a system that enables all these capabilities and validated these concepts with a Pioneer 2DX mobile robot learning tasks from multiple demonstrations and teacher feedback.</td>
</tr>
<tr id="bib_Nicolescu2003" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Nicolescu2003,
  author = {Nicolescu, Monica N. and Mataric, Maja J.},
  title = {Natural Methods for Robot Task Learning: Instructive Demonstrations, Generalization and Practice},
  journal = {Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent systems (AAMAS)},
  year = {2003},
  pages = {241},
  doi = {http://dx.doi.org/10.1145/860575.860614}
}
</pre></td>
</tr>
<tr id="Norrlof2002" class="entry">
	<td>Norrl&ouml;f, M.</td>
	<td>An adaptive iterative learning control algorithm with experiments on an industrial robot <p class="infolinks">[<a href="javascript:toggleInfo('Norrlof2002','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Norrlof2002','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td>IEEE Transactions on Robotics and Automation<br/>Vol. 18(2), pp. 245-251&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRA.2002.999653">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Norrlof2002" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An adaptive iterative learning control (ILC) algorithm based on an estimation procedure using a Kalman filter and an optimization of a quadratic criterion is presented. It is shown that by taking the measure- ment disturbance into consideration the resulting ILC filters become iter- ation-varying. Results from experiments on an industrial robot show that the algorithm is successful also in an application.</td>
</tr>
<tr id="bib_Norrlof2002" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Norrlof2002,
  author = {Norrl&ouml;f, Mikael},
  title = {An adaptive iterative learning control algorithm with experiments on an industrial robot},
  journal = {IEEE Transactions on Robotics and Automation},
  year = {2002},
  volume = {18},
  number = {2},
  pages = {245--251},
  doi = {http://dx.doi.org/10.1109/TRA.2002.999653}
}
</pre></td>
</tr>
<tr id="OFlaherty2015" class="entry">
	<td>O'Flaherty, R. and Egerstedt, M.</td>
	<td>Low-dimensional learning for complex robots <p class="infolinks">[<a href="javascript:toggleInfo('OFlaherty2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('OFlaherty2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Transactions on Automation Science and Engineering<br/>Vol. 12(1), pp. 19-27&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TASE.2014.2349915">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_OFlaherty2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents an algorithm for learning the switching policy and the boundaries conditions between primitive controllers that maximize the translational movements of a complex locomoting system. The algorithm learns an optimal action for each boundary condition instead of one for each discretized state-action pair of the system, as is typically done in machine learning. The system is modeled as a hybrid system because it contains both discrete and continuous dynamics. With this hybridification of the system and with this abstraction of learning boundary-action pairs, the &amp;x201C;curse of dimensionality&amp;x201D; is mitigated. The effectiveness of this learning algorithm is demonstrated on both a simulated system and on a physical robotic system. In both cases, the algorithm is able to learn the hybrid control strategy that maximizes the forward translational movement of the system without the need for human involvement.</td>
</tr>
<tr id="bib_OFlaherty2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{OFlaherty2015,
  author = {O'Flaherty, Rowland and Egerstedt, Magnus},
  title = {Low-dimensional learning for complex robots},
  journal = {IEEE Transactions on Automation Science and Engineering},
  year = {2015},
  volume = {12},
  number = {1},
  pages = {19--27},
  doi = {http://dx.doi.org/10.1109/TASE.2014.2349915}
}
</pre></td>
</tr>
<tr id="Obo2015" class="entry">
	<td>Obo, T., Loo, C.K. and Kubota, N.</td>
	<td>Robot posture generation based on genetic algorithm for imitation <p class="infolinks">[<a href="javascript:toggleInfo('Obo2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Obo2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>2015 IEEE Congress on Evolutionary Computation, CEC 2015 - Proceedings, pp. 552-557&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/CEC.2015.7256938">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Obo2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Human-like-motion performed by robots can have a contribution to exert a strong influence on human-robot interaction, because bodily expressions convey important and effective information. If the robots could adapt the features of human behavior to their motions and skills, the communication would become more smooth and natural. In this paper, we develop a posture measurement system for a robot imitation using a 3D image sensor. This paper proposes a method of robot posture generation based on a steady-state genetic algorithm (SSGA). SSGA is one of evolutionary optimization methods using selection, mutation, and crossover operators. Since SSGA is a simplified model, it is easy to implement into a real-time processing. Furthermore, we apply a continuous model of generation for an adaptive search in dynamical environment. Keywords—</td>
</tr>
<tr id="bib_Obo2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Obo2015,
  author = {Obo, Takenori and Loo, Chu Kiong and Kubota, Naoyuki},
  title = {Robot posture generation based on genetic algorithm for imitation},
  journal = {2015 IEEE Congress on Evolutionary Computation, CEC 2015 - Proceedings},
  year = {2015},
  pages = {552--557},
  doi = {http://dx.doi.org/10.1109/CEC.2015.7256938}
}
</pre></td>
</tr>
<tr id="Ogawara2003" class="entry">
	<td>Ogawara, K., Takaraatsu, J., Kimura, H. and Ikeuchi, K.</td>
	<td>Extraction of essential interactions through multiple observations of human demonstrations <p class="infolinks">[<a href="javascript:toggleInfo('Ogawara2003','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ogawara2003','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>IEEE Transactions on Industrial Electronics<br/>Vol. 50(4), pp. 667-675&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TIE.2003.814765">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Ogawara2003" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes a new approach on how to teach a robot everyday manipulation tasks under the "Learning from Observation" framework. In this approach, human demonstrations, which are made up of mutual interactions between a grasped object and an environmental object, are observed and a reusable manipulation task model is automatically generated. Most of the similar approaches so far assume that a demonstration can be well understood from a single demonstration. However, a single demonstration contains ambiguity, in that interactions which are essential to complete a task cannot be discerned without prior task dependent knowledge, which should be obtained from observation. To address these issues, a technique to integrate multiple observations of demonstrations is proposed. The demonstrations differ, but are virtually the same task. The shared interactions among all the demonstrations are considered to be essential and a task model is generated from their symbolic representations. Then, the relative trajectories corresponding to each essential interaction are generalized by calculating their mean and variance, and they are also stored in the task model, which is used to reproduce skilled behavior. This approach is examined by using a human-form robot, which successfully imitates human demonstrations of everyday tasks.</td>
</tr>
<tr id="bib_Ogawara2003" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ogawara2003,
  author = {Ogawara, Koichi and Takaraatsu, Jun and Kimura, Hiroshi and Ikeuchi, Katsushi},
  title = {Extraction of essential interactions through multiple observations of human demonstrations},
  journal = {IEEE Transactions on Industrial Electronics},
  year = {2003},
  volume = {50},
  number = {4},
  pages = {667--675},
  doi = {http://dx.doi.org/10.1109/TIE.2003.814765}
}
</pre></td>
</tr>
<tr id="Oudeyer2007" class="entry">
	<td>Oudeyer, P.-y. and Hafner, V.V.</td>
	<td>Intrinsic Motivation for Autonomous Mental Development To cite this version : Intrinsic Motivation Systems for Autonomous Mental Development <p class="infolinks">[<a href="javascript:toggleInfo('Oudeyer2007','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Oudeyer2007','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Evolutionary Computation, IEEE Transactions on<br/>Vol. 11(2), pp. 265-286&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1007/BF01252847">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Oudeyer2007" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Exploratory activities seem to be intrinsically re- warding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Exper- imental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology.</td>
</tr>
<tr id="bib_Oudeyer2007" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Oudeyer2007,
  author = {Oudeyer, Pierre-yves and Hafner, Verena V},
  title = {Intrinsic Motivation for Autonomous Mental Development To cite this version : Intrinsic Motivation Systems for Autonomous Mental Development},
  journal = {Evolutionary Computation, IEEE Transactions on},
  year = {2007},
  volume = {11},
  number = {2},
  pages = {265--286},
  doi = {http://dx.doi.org/10.1007/BF01252847}
}
</pre></td>
</tr>
<tr id="Ozisikyilmaz2008" class="entry">
	<td>Ozisikyilmaz, B., Memik, G. and Choudhary, A.</td>
	<td>Efficient system design space exploration using machine learning techniques <p class="infolinks">[<a href="javascript:toggleInfo('Ozisikyilmaz2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ozisikyilmaz2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>Proceedings of the 45th annual conference on Design automation - DAC '08, pp. 966&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1145/1391469.1391712">DOI</a> <a href="http://portal.acm.org/citation.cfm?doid=1391469.1391712">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ozisikyilmaz2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Computer manufacturers spend a huge amount of time, resources, and money in designing new systems and newer configurations, and their ability to reduce costs, charge competitive prices and gain market share depends on how good these systems perform. In this work, we develop predictive models for estimating the performance of systems by using performance numbers from only a small fraction of the overall design space. Specifically, we first develop three models, two based on artificial neural networks and another based on linear regression. Using these models, we analyze the published Standard Performance Evaluation Corporation (SPEC) benchmark results and show that by using the performance numbers of only 2% and 5% of the machines in the design space, we can estimate the performance of all the systems within 9.1% and 4.6% on average, respectively. Then, we show that the performance of future systems can be estimated with less than 2.2% error rate on average by using the data of systems from a previous year. We believe that these tools can accelerate the design space exploration significantly and aid in reducing the corresponding research/development cost and time- to-market.</td>
</tr>
<tr id="bib_Ozisikyilmaz2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ozisikyilmaz2008,
  author = {Ozisikyilmaz, Berkin and Memik, Gokhan and Choudhary, Alok},
  title = {Efficient system design space exploration using machine learning techniques},
  journal = {Proceedings of the 45th annual conference on Design automation - DAC '08},
  year = {2008},
  pages = {966},
  url = {http://portal.acm.org/citation.cfm?doid=1391469.1391712},
  doi = {http://dx.doi.org/10.1145/1391469.1391712}
}
</pre></td>
</tr>
<tr id="Paletta2007" class="entry">
	<td>Paletta, L., Fritz, G., Kintzler, F., Irran, J&ouml;. and Dorffner, G.</td>
	<td>Learning to perceive affordances in a framework of developmental embodied cognition <p class="infolinks">[<a href="javascript:toggleInfo('Paletta2007','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Paletta2007','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>2007 IEEE 6th International Conference on Development and Learning, ICDL, pp. 110-115&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/DEVLRN.2007.4354046">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Paletta2007" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently, the aspect of visual perception has been explored in the context of Gibson's concept of affordances in various ways. We focus in this work on the importance of developmental learning and the perceptual cueing for an agent's anticipation of opportunities for interaction, in extension to functional views on visual feature representations. The concept for the incremental learning of complex from basic affordances is presented in relation to learning of specific affordance features. We demonstrate the learning of causal relations between visual cues and associated anticipated interactions by reinforcement learning of predictive perceptual states. The work pursues a recently presented framework for cueing and recognition of affordance-based visual entities that plays an important role in robot control architectures, in analogy to human perception. We experimentally verify the concept within a real world robot scenario by learning predictive features from delayed rewards, and prove that features were selected for their relevance in predicting opportunities for interaction.</td>
</tr>
<tr id="bib_Paletta2007" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Paletta2007,
  author = {Paletta, Lucas and Fritz, Gerald and Kintzler, Florian and Irran, J&ouml;rg and Dorffner, Georg},
  title = {Learning to perceive affordances in a framework of developmental embodied cognition},
  journal = {2007 IEEE 6th International Conference on Development and Learning, ICDL},
  year = {2007},
  pages = {110--115},
  doi = {http://dx.doi.org/10.1109/DEVLRN.2007.4354046}
}
</pre></td>
</tr>
<tr id="Palma2009" class="entry">
	<td>Palma, F.D.</td>
	<td>Reducing complexity in robotic learning by experimentation <p class="infolinks">[<a href="javascript:toggleInfo('Palma2009','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Palma2009','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Advanced Robotics, 2009. ICAR 2009. International Conference on(029427), pp. 1-7&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_Palma2009" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In the learning by experimentation (LbE) paradigm, every knowledge is deduced from the analysis of experimental data, obtained with a proper experiment. The application of LbE in robotics is often limited by the excessive amount of sensor data. To face this problem a two-phases design strategy is possible. The former phase, called feature selection, isolates among the quantities involved in the learning; while the latter phase designs an experiment considering only a reduced subset of variables. This paper proposes a feature selection method: the feature section problem is formulated as a conditional independence problem and it is handled by applying the contingency table theory. The goodness of the work is tested on a LbE-based framework, extended to fit the presented method, regardless of the nature of the knowledge.</td>
</tr>
<tr id="bib_Palma2009" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Palma2009,
  author = {Palma, Federico Di},
  title = {Reducing complexity in robotic learning by experimentation},
  journal = {Advanced Robotics, 2009. ICAR 2009. International Conference on},
  year = {2009},
  number = {029427},
  pages = {1--7}
}
</pre></td>
</tr>
<tr id="Parisi2015" class="entry">
	<td>Parisi, S., Abdulsamad, H., Paraschos, A., Daniel, C. and Peters, J.</td>
	<td>Reinforcement learning vs human programming in tetherball robot games <p class="infolinks">[<a href="javascript:toggleInfo('Parisi2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Parisi2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE International Conference on Intelligent Robots and Systems<br/>Vol. 2015-Decem, pp. 6428-6434&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.2015.7354296">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Parisi2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning of motor skills is an im- portant challenge in order to endow robots with the ability to learn a wide range of skills and solve complex tasks. However, comparing reinforcement learning against human programming is not straightforward. In this paper, we create a motor learning framework consisting of state-of-the-art components in motor skill learning and compare it to a manually designed program on the task of robot tetherball. We use dynamical motor primitives for representing the robot's trajectories and relative entropy policy search to train the motor framework and improve its behavior by trial and error. These algorithmic components allow for high-quality skill learning while the experimental setup enables an accurate evaluation of our framework as robot players can compete against each other. In the complex game of robot tetherball, we show that our learning approach outperforms and wins a match against a high quality hand-crafted system</td>
</tr>
<tr id="bib_Parisi2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Parisi2015,
  author = {Parisi, Simone and Abdulsamad, Hany and Paraschos, Alexandros and Daniel, Christian and Peters, Jan},
  title = {Reinforcement learning vs human programming in tetherball robot games},
  journal = {IEEE International Conference on Intelligent Robots and Systems},
  year = {2015},
  volume = {2015-Decem},
  pages = {6428--6434},
  doi = {http://dx.doi.org/10.1109/IROS.2015.7354296}
}
</pre></td>
</tr>
<tr id="Pilarski2012" class="entry">
	<td>Pilarski, P.M., Dawson, M.R., Degris, T., Carey, J.P. and Sutton, R.S.</td>
	<td>Dynamic switching and real-time machine learning for improved human control of assistive biomedical robots <p class="infolinks">[<a href="javascript:toggleInfo('Pilarski2012','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Pilarski2012','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics, pp. 296-302&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/BioRob.2012.6290309">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Pilarski2012" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A general problem for human-machine interaction occurs when a machine's controllable dimensions outnumber the control channels available to its human user. In this work, we examine one prominent example of this problem: amputee switching between the multiple functions of a powered artificial limb. We propose a dynamic switching approach that learns during ongoing interaction to anticipate user behaviour, thereby presenting the most effective control option for a given context or task. Switching predictions are learned in real time using temporal difference methods and reinforcement learning, and demonstrated within the context of a robotic arm and a multifunction myoelectric controller. We find that a learned, dynamic switching order is able to out-perform the best fixed (non-adaptive) switching regime on a standard prosthetic proficiency task, increasing the number of optimal switching suggestions by 23%, and decreasing the expected transition time between degrees of freedom by more than 14%. These preliminary results indicate that real-time machine learning, specifically online prediction and anticipation, may be an important tool for developing more robust and intuitive controllers for assistive biomedical robots. We expect these techniques will transfer well to near-term use by patients. Future work will describe clinical testing of this approach with a population of amputee patients.</td>
</tr>
<tr id="bib_Pilarski2012" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Pilarski2012,
  author = {Pilarski, Patrick M. and Dawson, Michael R. and Degris, Thomas and Carey, Jason P. and Sutton, Richard S.},
  title = {Dynamic switching and real-time machine learning for improved human control of assistive biomedical robots},
  journal = {Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics},
  year = {2012},
  pages = {296--302},
  doi = {http://dx.doi.org/10.1109/BioRob.2012.6290309}
}
</pre></td>
</tr>
<tr id="Quinlan1986" class="entry">
	<td>Quinlan, J.R.</td>
	<td>Induction of Decision Trees <p class="infolinks">[<a href="javascript:toggleInfo('Quinlan1986','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Quinlan1986','bibtex')">BibTeX</a>]</p></td>
	<td>1986</td>
	<td>Machine Learning<br/>Vol. 1(1), pp. 81-106&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1023/A:1022643204877">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Quinlan1986" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.</td>
</tr>
<tr id="bib_Quinlan1986" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Quinlan1986,
  author = {Quinlan, J. R.},
  title = {Induction of Decision Trees},
  journal = {Machine Learning},
  year = {1986},
  volume = {1},
  number = {1},
  pages = {81--106},
  doi = {http://dx.doi.org/10.1023/A:1022643204877}
}
</pre></td>
</tr>
<tr id="Ramos2008" class="entry">
	<td>Ramos, C., Augusto, J.C. and Shapiro, D.</td>
	<td>Ambient Intelligence&amp;x02014;the Next Step for Artificial Intelligence <p class="infolinks">[<a href="javascript:toggleInfo('Ramos2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ramos2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>IEEE Intelligent Systems<br/>Vol. 23(2), pp. 15-18&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/MIS.2008.19">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Ramos2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Ambient intelligence (AmI) deals with a new world of ubiquitous computing devices, where physical environments interact intelligently and unobtrusively with people. These environments should be aware of people's needs, customizing requirements and forecasting behaviors. AmI environments can be diverse, such as homes, offices, meeting rooms, schools, hospitals, control centers, vehicles, tourist attractions, stores, sports facilities, and music devices. Artificial intelligence research aims to include more intelligence in AmI environments, allowing better support for humans and access to the essential knowledge for making better decisions when interacting with these environments. This article, which introduces a special issue on AmI, views the area from an artificial intelligence perspective.</td>
</tr>
<tr id="bib_Ramos2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ramos2008,
  author = {Ramos, Carlos and Augusto, Juan Carlos and Shapiro, Daniel},
  title = {Ambient Intelligence&amp;x02014;the Next Step for Artificial Intelligence},
  journal = {IEEE Intelligent Systems},
  year = {2008},
  volume = {23},
  number = {2},
  pages = {15--18},
  doi = {http://dx.doi.org/10.1109/MIS.2008.19}
}
</pre></td>
</tr>
<tr id="Rani2006" class="entry">
	<td>Rani, P., Liu, C., Sarkar, N. and Vanman, E.</td>
	<td>An empirical study of machine learning techniques for affect recognition in human-robot interaction <p class="infolinks">[<a href="javascript:toggleInfo('Rani2006','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Rani2006','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>Pattern Analysis and Applications<br/>Vol. 9(1), pp. 58-69&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1007/s10044-006-0025-y">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Rani2006" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Given the importance of implicit communication in human interactions, it would be valuable to have this capability in robotic systems wherein a robot can detect the motivations and emotions of the person it is working with. Recognizing affective states from physiological cues is an effective way of implementing implicit humanrobot interaction. Several machine learning techniques have been successfully employed in affect recognition to predict the affective state of an individual given a set of physiological features. However, a systematic comparison of the strengths and weaknesses of these methods has not yet been done. In this paper, we present a comparative study of four machine learning methods K-Nearest Neighbor, Regression Tree (RT), Bayesian Network and Support Vector Machine (SVM) as applied to the domain of affect recognition using physiological signals. The results showed that SVM gave the best classification accuracy even though all the methods performed competitively. RT gave the next best classification accuracy and was the most space and time efficient.</td>
</tr>
<tr id="bib_Rani2006" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Rani2006,
  author = {Rani, Pramila and Liu, Changchun and Sarkar, Nilanjan and Vanman, Eric},
  title = {An empirical study of machine learning techniques for affect recognition in human-robot interaction},
  journal = {Pattern Analysis and Applications},
  year = {2006},
  volume = {9},
  number = {1},
  pages = {58--69},
  doi = {http://dx.doi.org/10.1007/s10044-006-0025-y}
}
</pre></td>
</tr>
<tr id="Realmuto2016" class="entry">
	<td>Realmuto, J., Warrier, R.B., Devasia, S., Eng, M. and Washington, U.</td>
	<td>Iterative Learning Control for Human-Robot Collaborative Output Tracking <p class="infolinks">[<a href="javascript:toggleInfo('Realmuto2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Realmuto2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>(4)&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_Realmuto2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This article studies human-robot learning control for collaborative output-tracking tasks. We propose an algorithm to adaptively tune the frequency-dependent iteration gain and apply it to two cases: when the desired output is directly available to the robot and when the robot infers the desired output from human- achieved output. Experiment results are presented to illustrate the application of the proposed approach to a human-robot collaborative output-tracking task. Results show that the error converges to less than the closed-loop robot tracking error, and that the approach can provide varying levels of robot assistance by selecting the desired human-robot collaboration level.</td>
</tr>
<tr id="bib_Realmuto2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Realmuto2016,
  author = {Realmuto, Jonathan and Warrier, Rahul B and Devasia, Santosh and Eng, Mech and Washington, U},
  title = {Iterative Learning Control for Human-Robot Collaborative Output Tracking},
  year = {2016},
  number = {4}
}
</pre></td>
</tr>
<tr id="Rozo2016" class="entry">
	<td>Rozo, L., Calinon, S., Caldwell, D.G., Jimenez, P. and Torras, C.</td>
	<td>Learning Physical Collaborative Robot Behaviors From Human Demonstrations <p class="infolinks">[<a href="javascript:toggleInfo('Rozo2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Rozo2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>IEEE Transactions on Robotics<br/>Vol. 32(3), pp. 513-527&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TRO.2016.2540623">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Rozo2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robots are becoming safe and smart enough to work alongside people not only on manufacturing production lines, but also in spaces such as houses, museums, or hospitals. This can be significantly exploited in situations in which a human needs the help of another person to perform a task, because a robot may take the role of the helper. In this sense, a human and the robotic assistant may cooperatively carry out a variety of tasks, therefore requiring the robot to communicate with the person, understand his/her needs, and behave accordingly. To achieve this, we propose a framework for a user to teach a robot collaborative skills from demonstrations.We mainly focus on tasks involving physical con- tact with the user, in which not only position, but also force sensing and compliance become highly relevant. Specifically, we present an approach that combines probabilistic learning, dynamical sys- tems, and stiffness estimation to encode the robot behavior along the task. Our method allows a robot to learn not only trajectory following skills, but also impedance behaviors. To show the func- tionality and flexibility of our approach, two different testbeds are used: a transportation task and a collaborative table assembly.</td>
</tr>
<tr id="bib_Rozo2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Rozo2016,
  author = {Rozo, Leonel and Calinon, Sylvain and Caldwell, Darwin G. and Jimenez, Pablo and Torras, Carme},
  title = {Learning Physical Collaborative Robot Behaviors From Human Demonstrations},
  journal = {IEEE Transactions on Robotics},
  year = {2016},
  volume = {32},
  number = {3},
  pages = {513--527},
  doi = {http://dx.doi.org/10.1109/TRO.2016.2540623}
}
</pre></td>
</tr>
<tr id="Sampei1988" class="entry">
	<td>Sampei, M. and Furuta, K.</td>
	<td>Robot Control in the Neighborhood of Singular Points. <p class="infolinks">[<a href="javascript:toggleInfo('Sampei1988','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sampei1988','bibtex')">BibTeX</a>]</p></td>
	<td>1988</td>
	<td><br/>Vol. 4(3)IEEE journal of robotics and automation, pp. 303-309&nbsp;</td>
	<td>misc</td>
	<td><a href="http://dx.doi.org/10.1109/56.791">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Sampei1988" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An alternative method is proposed for designing a robot-controller$nin Cartesian coordinates using a time-scale transformation. The proposed$ncontroller achieves slow poles in the area of poor manipulative ability$nand fast poles in the area of good manipulative ability. Therefore, the$nrobot can be properly controlled in the neighbourhood of singular points$nwithout reducing the performance outside the neighborhood of these$npoints</td>
</tr>
<tr id="bib_Sampei1988" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Sampei1988,
  author = {Sampei, Mitsuji and Furuta, Katsuhisa},
  title = {Robot Control in the Neighborhood of Singular Points.},
  booktitle = {IEEE journal of robotics and automation},
  year = {1988},
  volume = {4},
  number = {3},
  pages = {303--309},
  doi = {http://dx.doi.org/10.1109/56.791}
}
</pre></td>
</tr>
<tr id="Satake2009" class="entry">
	<td>Satake, S., Kanda, T., Glas, D.F., Imai, M., Ishiguro, H. and Hagita, N.</td>
	<td>How to approach humans? <p class="infolinks">[<a href="javascript:toggleInfo('Satake2009','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Satake2009','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Proceedings of the 4th ACM/IEEE international conference on Human robot interaction - HRI '09<br/>Vol. 28(3), pp. 109&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1145/1514095.1514117">DOI</a> <a href="http://portal.acm.org/citation.cfm?doid=1514095.1514117">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Satake2009" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper proposes a model of approach behavior with which a robot can initiate conversation with people who are walking. We developed the model by learning from the failures in a simplistic approach behavior used in a real shopping mall. Sometimes people were unaware of the robot's presence, even when it spoke to them. Sometimes, people were not sure whether the robot was really trying to start a conversation, and they did not start talking with it even though they displayed interest. To prevent such failures, our model includes the following functions: predicting the walking behavior of people, choosing a target person, planning its approaching path, and nonverbally indicating its intention to initiate a conversation. The approach model was implemented and used in a real shopping mall. The field trial demonstrated that our model significantly improves the robot's performance in initiating conversations.</td>
</tr>
<tr id="bib_Satake2009" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Satake2009,
  author = {Satake, Satoru and Kanda, Takayuki and Glas, Dylan F. and Imai, Michita and Ishiguro, Hiroshi and Hagita, Norihiro},
  title = {How to approach humans?},
  journal = {Proceedings of the 4th ACM/IEEE international conference on Human robot interaction - HRI '09},
  year = {2009},
  volume = {28},
  number = {3},
  pages = {109},
  url = {http://portal.acm.org/citation.cfm?doid=1514095.1514117},
  doi = {http://dx.doi.org/10.1145/1514095.1514117}
}
</pre></td>
</tr>
<tr id="Schaal1999" class="entry">
	<td>Schaal, S.</td>
	<td>Is Imitation Learnig the Route to Humanoid Robots? <p class="infolinks">[<a href="javascript:toggleInfo('Schaal1999','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Schaal1999','bibtex')">BibTeX</a>]</p></td>
	<td>1999</td>
	<td>Trends in Cognitive Sciences<br/>Vol. 3(6), pp. 233-242&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_Schaal1999" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This review investigates two recent developments in artificial intelligence and neural computation: learning from imitation and the development of humanoid robots. It will be postulated that the study of imitation learning offers a promising route to gain new insights into mechanisms of perceptual motor control that could ultimately lead to the creation of autonomous humanoid robots. Imitation learning focuses on three important issues: efficient motor learning, the connection between action and perception, and modular motor control in form of movement primitives. It will be reviewed how re- search on representations of, and functional connections between action and perception have contributed to our understanding of motor acts of other beings. The recent discov- ery that some areas in the primate brain are active during both movement perception and execution has provided a hypothetical neural basis of imitation. Computational ap- proaches to imitation learning will also be described, initially from the perspective of traditional AI and robotics, but also from the perspective of neural network models and statistical learning research. Parallels and differences between biological and computa- tional approaches to imitation will be highlighted and an overview of current projects that actually employ imitation learning for humanoid robots will be given. In</td>
</tr>
<tr id="bib_Schaal1999" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Schaal1999,
  author = {Schaal, Stefan},
  title = {Is Imitation Learnig the Route to Humanoid Robots?},
  journal = {Trends in Cognitive Sciences},
  year = {1999},
  volume = {3},
  number = {6},
  pages = {233--242}
}
</pre></td>
</tr>
<tr id="Schaal1996" class="entry">
	<td>Schaal, S.</td>
	<td>Learning from demonstration <p class="infolinks">[<a href="javascript:toggleInfo('Schaal1996','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Schaal1996','bibtex')">BibTeX</a>]</p></td>
	<td>1996</td>
	<td>Advances in Neural Information Processing Systems (NIPS), pp. 1040-1046&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_Schaal1996" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely at- tempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based rein- forcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor. 1.</td>
</tr>
<tr id="bib_Schaal1996" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Schaal1996,
  author = {Schaal, S},
  title = {Learning from demonstration},
  journal = {Advances in Neural Information Processing Systems (NIPS)},
  year = {1996},
  pages = {1040--1046}
}
</pre></td>
</tr>
<tr id="Schneider1995" class="entry">
	<td>Schneider, J.G. and Brown, C.M.</td>
	<td>Cooperative coaching in robot learning <p class="infolinks">[<a href="javascript:toggleInfo('Schneider1995','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Schneider1995','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>Intelligent Robots and Systems 95. 'Human Robot Interaction and Cooperative Robots', Proceedings. 1995 IEEE/RSJ International Conference on<br/>Vol. 3, pp. 332-337 vol.3&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/IROS.1995.525905">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Schneider1995" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many closed loop learning algorithms perform gradient descent on a cost function with respect to the parameters of a learning controller. The authors observe that both local closed loop learners, which consider only the cost of the current time step, and optimal control based closed loop learners, which consider the future effects of control actions, can become stuck in sub-optimal local minima in the cost function. The authors propose the use of “cooperating coaches” to deal with this problem. Each coach attempts gradient descent based on its own cost function and they work together to avoid getting stuck in local minima. When one coach has achieved the best result it can (the gradient for its cost function is zero), another coach takes over to guide the search through the parameter space. The authors demonstrate cooperative coaching on the problem of curve tracking with an inverted pendulum and show that it yields faster, smoother tracking of target curves by combining the best aspects of two different coaches</td>
</tr>
<tr id="bib_Schneider1995" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Schneider1995,
  author = {Schneider, J G and Brown, C M},
  title = {Cooperative coaching in robot learning},
  journal = {Intelligent Robots and Systems 95. 'Human Robot Interaction and Cooperative Robots', Proceedings. 1995 IEEE/RSJ International Conference on},
  year = {1995},
  volume = {3},
  pages = {332--337 vol.3},
  doi = {http://dx.doi.org/10.1109/IROS.1995.525905}
}
</pre></td>
</tr>
<tr id="Senthilkumar2009" class="entry">
	<td>Senthilkumar, K.S. and Bharadwaj, K.K.</td>
	<td>Hybrid genetic-fuzzy approach to autonomous mobile robot <p class="infolinks">[<a href="javascript:toggleInfo('Senthilkumar2009','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Senthilkumar2009','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>2009 IEEE International Conference on Technologies for Practical Robot Applications, TePRA 2009, pp. 29-34&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TEPRA.2009.5339649">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Senthilkumar2009" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An Autonomous Mobile Robot (AMR) is a machine able to extract information from its environment and use knowledge about its world to move safely in a meaningful and purposeful manner. Robot Navigation and Obstacle Avoidance are from the most important problems in mobile robots, especially in unknown environments. It must be able to interact with other objects safely. Several techniques such as Fuzzy logic, Reinforcement learning, Neural Networks and Genetic Algorithms, have applied to AMR in order to improve their performance. During the past several years Hybrid Genetic-fuzzy method has emerged as one of the most active and fruitful areas for research in the application of intelligent system design. The objective of this work is to provide a Hybrid method by which an improved set of rules governing the actions and behavior of a simple navigating and obstacle avoiding AMR. Genes are in the form of distances and angles labels. The chromosomes are represented as a rule written in a Boolean algebraic form. The method used to enhance the performance employs a simulation model designed by using Visual Basic software.</td>
</tr>
<tr id="bib_Senthilkumar2009" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Senthilkumar2009,
  author = {Senthilkumar, K. S. and Bharadwaj, K. K.},
  title = {Hybrid genetic-fuzzy approach to autonomous mobile robot},
  journal = {2009 IEEE International Conference on Technologies for Practical Robot Applications, TePRA 2009},
  year = {2009},
  pages = {29--34},
  doi = {http://dx.doi.org/10.1109/TEPRA.2009.5339649}
}
</pre></td>
</tr>
<tr id="Shah2011" class="entry">
	<td>Shah, J. and Wiken, J.</td>
	<td>Improved Human-Robot Team Performance Using Chaski, A Human-Inspired Plan Execution System <p class="infolinks">[<a href="javascript:toggleInfo('Shah2011','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Shah2011','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Artificial Intelligence, pp. 29-36&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_Shah2011" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe the design and evaluation of Chaski, a robot plan execution system that uses insights from human-human teaming to make human-robot teaming more natural and fluid. Chaski is a task-level executive that enables a robot to collaboratively execute a shared plan with a person. The system chooses and schedules the robot's actions, adapts to the human partner, and acts to minimize the human's idle time. We evaluate Chaski in human subject experiments in which a person works with a mobile and dexterous robot to col- laboratively assemble structures using building blocks. We measure team performance outcomes for robots controlled by Chaski compared to robots that are verbally commanded, step-by-step by the human teammate. We show that Chaski reduces the human's idle time by 85%, a statistically signif- icant difference. This result supports the hypothesis that human-robot team performance is improved when a robot emulates the effective coordination behaviors observed in hu- man teams. Categories</td>
</tr>
<tr id="bib_Shah2011" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Shah2011,
  author = {Shah, Julie and Wiken, James},
  title = {Improved Human-Robot Team Performance Using Chaski, A Human-Inspired Plan Execution System},
  journal = {Artificial Intelligence},
  year = {2011},
  pages = {29--36}
}
</pre></td>
</tr>
<tr id="Siagian2007" class="entry">
	<td>Siagian, C. and Itti, L.</td>
	<td>Rapid biologically-inspired scene classification using features shared with visual attention <p class="infolinks">[<a href="javascript:toggleInfo('Siagian2007','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Siagian2007','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>IEEE Transactions on Pattern Analysis and Machine Intelligence<br/>Vol. 29(2), pp. 300-312&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TPAMI.2007.40">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Siagian2007" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe and validate a simple context-based scene recognition algorithm for mobile robotics applications. The system can differentiate outdoor scenes from various sites on a college campus using a multiscale set of early-visual features, which capture the "gist" of the scene into a low-dimensional signature vector. Distinct from previous approaches, the algorithm presents the advantage of being biologically plausible and of having low-computational complexity, sharing its low-level features with a model for visual attention that may operate concurrently on a robot. We compare classification accuracy using scenes filmed at three outdoor sites on campus (13,965 to 34,711 frames per site). Dividing each site into nine segments, we obtain segment classification rates between 84.21 percent and 88.62 percent. Combining scenes from all sites (75,073 frames in total) yields 86.45 percent correct classification, demonstrating the generalization and scalability of the approach.</td>
</tr>
<tr id="bib_Siagian2007" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Siagian2007,
  author = {Siagian, Christian and Itti, Laurent},
  title = {Rapid biologically-inspired scene classification using features shared with visual attention},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2007},
  volume = {29},
  number = {2},
  pages = {300--312},
  doi = {http://dx.doi.org/10.1109/TPAMI.2007.40}
}
</pre></td>
</tr>
<tr id="SiangKokSim2003" class="entry">
	<td>Siang Kok Sim, Kai Wei Ong and Seet, G.</td>
	<td>A Foundation for Robot Learning <p class="infolinks">[<a href="javascript:toggleInfo('SiangKokSim2003','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('SiangKokSim2003','bibtex')">BibTeX</a>]</p></td>
	<td>2003</td>
	<td>The Fourth International Conference on Control and Automation 2003 ICCA Final Program and Book of Abstracts ICCA-03(June), pp. 649-653&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICCA.2003.1595102">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1595102">URL</a>&nbsp;</td>
</tr>
<tr id="abs_SiangKokSim2003" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper considers the fundamental issues of robot learning in which answers to basic questions on robot learning, such as "What can the robot learn?", "What are the consequences of robot learning?", "How does the robot learn?", "How fast do robots need to learn?", and "When do robots learn?" are addressed. The answers to these questions may lead to the identification of the elements of robot learning and the interaction between these elements. Hence, the purpose of this paper is to discuss the fundamental issues in a holistic manner so that key elements that characterise robot learning can be formalised into a framework.</td>
</tr>
<tr id="bib_SiangKokSim2003" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{SiangKokSim2003,
  author = {Siang Kok Sim and Kai Wei Ong and Seet, G},
  title = {A Foundation for Robot Learning},
  journal = {The Fourth International Conference on Control and Automation 2003 ICCA Final Program and Book of Abstracts ICCA-03},
  year = {2003},
  number = {June},
  pages = {649--653},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1595102},
  doi = {http://dx.doi.org/10.1109/ICCA.2003.1595102}
}
</pre></td>
</tr>
<tr id="Son2015" class="entry">
	<td>Son, J.-H. and Ahn, H.-S.</td>
	<td>A Robot Learns How to Entice an Insect <p class="infolinks">[<a href="javascript:toggleInfo('Son2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Son2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>IEEE Intelligent Systems<br/>Vol. 30(4), pp. 54-63&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/MIS.2015.37">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7106380">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Son2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This article shows that a robot can learn, through a trial-and-error learning process, how to entice a biological insect into moving closely along a given trajectory without human aid. The authors designed a robot that has a camera to recognize the insect and its heading angle, and then spread a specific odor source to attract the insect.</td>
</tr>
<tr id="bib_Son2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Son2015,
  author = {Son, Ji-Hwan and Ahn, Hyo-Sung},
  title = {A Robot Learns How to Entice an Insect},
  journal = {IEEE Intelligent Systems},
  year = {2015},
  volume = {30},
  number = {4},
  pages = {54--63},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7106380},
  doi = {http://dx.doi.org/10.1109/MIS.2015.37}
}
</pre></td>
</tr>
<tr id="Stone2000" class="entry">
	<td>Stone, P. and Veloso, M.</td>
	<td>Multiagent systems: a survey from a machine learning perspective <p class="infolinks">[<a href="javascript:toggleInfo('Stone2000','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Stone2000','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td><br/>Vol. 8Autonomous Robots. 8 (2000), pp. 345-383&nbsp;</td>
	<td>misc</td>
	<td><a href="http://link.springer.com/article/10.1023/A:1008942012299">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Stone2000" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally,DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight howmultiagent systems can be and have been used to build complex systems.Whenoptions exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue. Keywords:</td>
</tr>
<tr id="bib_Stone2000" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Stone2000,
  author = {Stone, P and Veloso, M},
  title = {Multiagent systems: a survey from a machine learning perspective},
  booktitle = {Autonomous Robots. 8 (2000)},
  year = {2000},
  volume = {8},
  pages = {345--383},
  url = {http://link.springer.com/article/10.1023/A:1008942012299}
}
</pre></td>
</tr>
<tr id="Subbulakshmi2013" class="entry">
	<td>Subbulakshmi, T. and Afroze, a.F.</td>
	<td>Multiple learning based classifiers using layered approach and Feature Selection for attack detection <p class="infolinks">[<a href="javascript:toggleInfo('Subbulakshmi2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Subbulakshmi2013','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>2013 IEEE International Conference ON Emerging Trends in Computing, Communication and Nanotechnology (ICECCN)(Iceccn), pp. 308-314&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICE-CCN.2013.6528514">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6528514">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Subbulakshmi2013" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: — One of the major shares of the current security infrastructure is formed by the Intrusion Detection Systems (IDS). The attack launched towards the security systems are increasing in a rapid way. The sophistication of attack methods with more automated tools enables the attackers to gain control over the systems and produce threats to the information assets. The normal way of detecting the attacks is by using tools that produce alerts to the system administrators. But most of the attacks would normally escape from these tools since they are mostly rule-based. So the need for enhanced attack detection methods becomes vital for the security infrastructure. The attack detection methods are normally statistical based or probabilistic based. This paper focuses on attack detection using multiple learning based classifiers such as J48, Na&iuml;ve Bayes, Random Forest, Random Tree, KStar, RotationForest, RandomSubspace, Ordinal Class Classifier, Data Near BalancedND and Multiclass classifier. Correlation Based Feature Selection (CFS) is also used to select the best features of the kddcup 99 dataset for the attack classes such as DoS, Probe, U2R and R2L. The feature selection enables the classifiers to improve the accuracy of classification. The multiple classifiers are used in four layers for detecting the four types of attack classes. The classification rate of above 99% is obtained. Cost – Benefit analysis is done for the various attack detection methods and the ROC curves are also plotted.</td>
</tr>
<tr id="bib_Subbulakshmi2013" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Subbulakshmi2013,
  author = {Subbulakshmi, T. and Afroze, a. Farah},
  title = {Multiple learning based classifiers using layered approach and Feature Selection for attack detection},
  journal = {2013 IEEE International Conference ON Emerging Trends in Computing, Communication and Nanotechnology (ICECCN)},
  year = {2013},
  number = {Iceccn},
  pages = {308--314},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6528514},
  doi = {http://dx.doi.org/10.1109/ICE-CCN.2013.6528514}
}
</pre></td>
</tr>
<tr id="Sutton2012" class="entry">
	<td>Sutton, R.S. and Barto, A.G.</td>
	<td>Reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('Sutton2012','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sutton2012','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Learning<br/>Vol. 3(9), pp. 322&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/MED.2013.6608833">DOI</a> <a href="https://books.google.com/books?id=CAFR6IBF4xYC{\&}pgis=1$\backslash$nhttp://incompleteideas.net/sutton/book/the-book.html$\backslash$nhttps://www.dropbox.com/s/f4tnuhipchpkgoj/book2012.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sutton2012" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.</td>
</tr>
<tr id="bib_Sutton2012" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sutton2012,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  title = {Reinforcement learning},
  journal = {Learning},
  year = {2012},
  volume = {3},
  number = {9},
  pages = {322},
  url = {https://books.google.com/books?id=CAFR6IBF4xYC&amp;pgis=1$nhttp://incompleteideas.net/sutton/book/the-book.html$nhttps://www.dropbox.com/s/f4tnuhipchpkgoj/book2012.pdf},
  doi = {http://dx.doi.org/10.1109/MED.2013.6608833}
}
</pre></td>
</tr>
<tr id="Sutton1998" class="entry">
	<td>Sutton, R.S. and Barto, A.G.</td>
	<td>Reinforcement Learning I : Introduction <p class="infolinks">[<a href="javascript:toggleInfo('Sutton1998','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sutton1998','bibtex')">BibTeX</a>]</p></td>
	<td>1998</td>
	<td>Learning<br/>Vol. 3, pp. 1-15&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1.1.32.7692">DOI</a> <a href="http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sutton1998" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In which we tru to give a basic intuitive sense of what reinforcement learning is and how it differs and relates to other fields, e.g., supervised learning and neural networks, genetic algorithms and artificial life, control theory. Intuituvely, Rl is trial and error (variation and selection, search) plus learning (association, memory). We argue that RL is the only field that seriously addresses the special features of the problem of learning from interaction to achieve long-term goals.</td>
</tr>
<tr id="bib_Sutton1998" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sutton1998,
  author = {Sutton, Richard S and Barto, Andrew G},
  title = {Reinforcement Learning I : Introduction},
  journal = {Learning},
  year = {1998},
  volume = {3},
  pages = {1--15},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
  doi = {http://dx.doi.org/10.1.1.32.7692}
}
</pre></td>
</tr>
<tr id="SylvainCalinonFlorentGuenter2007" class="entry">
	<td>Sylvain Calinon Florent Guenter, A.B.</td>
	<td>On Learning, Representing and Generalizing a Task in a Humanoid Robot <p class="infolinks">[<a href="javascript:toggleInfo('SylvainCalinonFlorentGuenter2007','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('SylvainCalinonFlorentGuenter2007','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>IEEE Transactions on Systems, Man and Cybernetics<br/>Vol. 37(2), pp. 286-298&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_SylvainCalinonFlorentGuenter2007" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a programming-by-demonstration framework for generically extracting the relevant features of a given task and for addressing the problem of generalizing the acquired knowledge to different contexts. We validate the archi- tecture through a series of experiments, in which a human demon- strator teaches a humanoid robot simple manipulatory tasks. A probability-based estimation of the relevance is suggested by first projecting the motion data onto a generic latent space using principal component analysis. The resulting signals are encoded using a mixture of Gaussian/Bernoulli distributions (Gaussian mixture model/Bernoulli mixturemodel). This provides ameasure of the spatio-temporal correlations across the different modalities collected from the robot, which can be used to determine a metric of the imitation performance. The trajectories are then generalized using Gaussian mixture regression. Finally, we analytically com- pute the trajectory which optimizes the imitation metric and use this to generalize the skill to different contexts. Index</td>
</tr>
<tr id="bib_SylvainCalinonFlorentGuenter2007" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{SylvainCalinonFlorentGuenter2007,
  author = {Sylvain Calinon Florent Guenter, Aude Billard},
  title = {On Learning, Representing and Generalizing a Task in a Humanoid Robot},
  journal = {IEEE Transactions on Systems, Man and Cybernetics},
  year = {2007},
  volume = {37},
  number = {2},
  pages = {286--298}
}
</pre></td>
</tr>
<tr id="Takayama2011" class="entry">
	<td>Takayama, L., Dooley, D. and Ju, W.</td>
	<td>Expressing thought: improving robot readability with animation principles <p class="infolinks">[<a href="javascript:toggleInfo('Takayama2011','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Takayama2011','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Proceedings of the 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pp. 69-76&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1145/1957656.1957674">DOI</a> <a href="http://dl.acm.org/citation.cfm?id=1957674">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Takayama2011" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The animation techniques of anticipation and reaction can help create robot behaviors that are human readable such that people can figure out what the robot is doing, reasonably predict what the robot will do next, and ultimately interact with the robot in an effective way. By showing forethought before action and expressing a reaction to the task outcome (success or failure), we prototyped a set of human-robot interaction behaviors. In a 2 (forethought vs. none: between) x 2 (reaction to outcome vs. none: between) x 2 (success vs. failure task outcome: within) experiment, we tested the influences of forethought and reaction upon people's perceptions of the robot and the robot's readabil- ity. In this online video prototype experiment (N=273), we have found support for the hypothesis that perceptions of robots are influenced by robots showing forethought, the task outcome (success or failure), and showing goal-oriented reactions to those task outcomes. Implications for theory and design are discussed.</td>
</tr>
<tr id="bib_Takayama2011" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Takayama2011,
  author = {Takayama, Leila and Dooley, Doug and Ju, Wendy},
  title = {Expressing thought: improving robot readability with animation principles},
  journal = {Proceedings of the 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  year = {2011},
  pages = {69--76},
  url = {http://dl.acm.org/citation.cfm?id=1957674},
  doi = {http://dx.doi.org/10.1145/1957656.1957674}
}
</pre></td>
</tr>
<tr id="Tan2016" class="entry">
	<td>Tan, H.</td>
	<td>A Behavior Generation Framework for Robots to Learn from Demonstrations <p class="infolinks">[<a href="javascript:toggleInfo('Tan2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tan2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015, pp. 947-953&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/SMC.2015.173">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Tan2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper proposes a framework of generating behavior sequences for robots, especially for humanoid robots, to perform complex tasks. This framework provides a method for the robot to generalize common features of demonstrated behaviors, to store the learned behaviors in the memory system, to construct a behavior graph to describe relationships among learned behaviors, to find and assemble a behavior sequence, and to generate similar motion trajectories of basic behaviors when it is placed in a similar but slightly different task-relevant situation. Additionally, we successfully use behavior graph to describe the dynamic relationship among behaviors and we successfully apply shortest path searching methods in behavior sequence generation, which provides a novel solution to associate knowledge representation with behavior generation. Simulation and experiments are carried on a humanoid robot to validate our proposed</td>
</tr>
<tr id="bib_Tan2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Tan2016,
  author = {Tan, Huan},
  title = {A Behavior Generation Framework for Robots to Learn from Demonstrations},
  journal = {Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
  year = {2016},
  pages = {947--953},
  doi = {http://dx.doi.org/10.1109/SMC.2015.173}
}
</pre></td>
</tr>
<tr id="Thrun1995" class="entry">
	<td>Thrun, S. and Mitchell, T.M.</td>
	<td>Lifelong Robot Learning 1 <p class="infolinks">[<a href="javascript:toggleInfo('Thrun1995','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Thrun1995','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>Robotics and Autonomous Systems<br/>Vol. 15(March 1993), pp. 25-46&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/0921-8890(95)00004-Y">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Thrun1995" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Learning provides a useful tool for the automatic design of autonomous$nrobots. Recent research on learning robot control has predominantly$nfocused on learning single tasks that were studied in isolation.$nIf robots encounter a multitude of control learning tasks over their$nentire lifetime there is an opportunity to transfer knowledge between$nthem. In order to do so, robots may learn the invariants and the$nregularities of the individual tasks and environments. This task-independent$nknowledge can be employed to bias generalization when learning control,$nwhich reduces the need for real-world experimentation. We argue that$nknowledge transfer is essential if robots are to learn control with$nmoderate learning times in complex scenarios. Two approaches to lifelong$nrobot learning which both capture invariant knowledge about the robot$nand its environments are presented. Both approaches have been evaluated$nusing a HERO-2000 mobile robot. Learning tasks included navigation$nin unknown indoor environments and a simple find-and-fetch task.</td>
</tr>
<tr id="bib_Thrun1995" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Thrun1995,
  author = {Thrun, Sebastian and Mitchell, Tom M},
  title = {Lifelong Robot Learning 1},
  journal = {Robotics and Autonomous Systems},
  year = {1995},
  volume = {15},
  number = {March 1993},
  pages = {25--46},
  doi = {http://dx.doi.org/10.1016/0921-8890(95)00004-Y}
}
</pre></td>
</tr>
<tr id="Trautmann2011" class="entry">
	<td>Trautmann, E. and Ray, L.</td>
	<td>Mobility characterization for autonomous mobile robots using machine learning <p class="infolinks">[<a href="javascript:toggleInfo('Trautmann2011','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Trautmann2011','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Autonomous Robots<br/>Vol. 30(4), pp. 369-383&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1007/s10514-011-9224-5">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Trautmann2011" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a supervised learning approach to improving the autonomous mobility of wheeled robots through sensing the robot's interaction with terrain ‘underfoot.' Mobility characterization is cast as a hierarchical task, in which pre-immobilization detection is achieved using support vector machines in time to prevent full immobilization, and if a pre-immobilization condition is detected, the associated terrain feature affecting mobility is identified using a Hidden Markov model. These methods are implemented using a hierarchical, layered control scheme developed for the Yeti robot, a 73-kg, four-wheeled robot designed to perform autonomous medium-range missions in polar terrain. The methodology is motivated by the difficultly of visually recognizing terrain features that impact mobility in low contrast terrain. The efficacy of the approach is evaluated using data from a suite of proprioceptive sensors. Real-time implementation shows that Yeti can consistently detect pre-immobilization conditions, stop in time to avoid unrecoverable immobilization, identify the terrain feature presenting the mobility challenge, and execute an escape sequence to retreat from the condition.</td>
</tr>
<tr id="bib_Trautmann2011" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Trautmann2011,
  author = {Trautmann, Eric and Ray, Laura},
  title = {Mobility characterization for autonomous mobile robots using machine learning},
  journal = {Autonomous Robots},
  year = {2011},
  volume = {30},
  number = {4},
  pages = {369--383},
  doi = {http://dx.doi.org/10.1007/s10514-011-9224-5}
}
</pre></td>
</tr>
<tr id="Tzafestas2006" class="entry">
	<td>Tzafestas, C.S., Palaiologou, N. and Alifragis, M.</td>
	<td>Virtual and remote robotic laboratory: Comparative experimental evaluation <p class="infolinks">[<a href="javascript:toggleInfo('Tzafestas2006','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tzafestas2006','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>IEEE Transactions on Education<br/>Vol. 49(3), pp. 360-369&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TE.2006.879255">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Tzafestas2006" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes the development and experimental evaluation of an e-laboratory platform in the field of robotics. The system in its current configuration is designed to enable distance training of students in real scenarios of robot manipulator programming. From a technological perspective, the research work presented in this paper is directed towards the adaptation of concepts and techniques developed in the field of telerobotics and virtual reality, and their integration in such e-laboratory settings. This paper focuses particularly on the educational impact of such systems. The goal is to assess the performance of e-laboratory scenarios in terms of the efficacy of training provided to students. The results of a pilot experimental study are presented, providing a comparative evaluation for three training modalities: real, remote, and virtual training on robot manipulator programming. The experiments were conducted according to an evaluation protocol specially designed for the considered target training task, using scoring charts to obtain quantitative performance measures and assess the performance of the student groups participating in the course. Training, as a dynamic process, is approached according to a classical three dimensional model, and performance scores are accordingly assessed in these dimensions (namely: low-level versus mid and high-level skills and understanding). The obtained results reveal certain differences between the three groups, particularly as related to the low-level skill training score, giving some insight about the training `dimensions' that are expected to be mostly affected by the absence of physical (or realistic virtual) presence in a real hands-on experimentation. Statistical analysis indicates, however, that, despite these apparent differences, such e-laboratory modules can be integrated quite effectively in practical scenarios, creating virtual training environments that can provide adequate learning elements, as related p-$n-$narticularly to mid and high-level skill acquisition. Further work and large-scale studies are still needed, though, in order to explore the extent to which such a general conclusion is valid in different training settings, and to form the basis of a more theoretical evaluation for a comprehensive understanding of the pedagogical differences between real, virtual, and remote learning/training methodologies and experiences</td>
</tr>
<tr id="bib_Tzafestas2006" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Tzafestas2006,
  author = {Tzafestas, Costas S. and Palaiologou, Nektaria and Alifragis, Manthos},
  title = {Virtual and remote robotic laboratory: Comparative experimental evaluation},
  journal = {IEEE Transactions on Education},
  year = {2006},
  volume = {49},
  number = {3},
  pages = {360--369},
  doi = {http://dx.doi.org/10.1109/TE.2006.879255}
}
</pre></td>
</tr>
<tr id="Urgen2016" class="entry">
	<td>Urgen, B.A., Pehlivan, S. and Saygin, A.P.</td>
	<td>Representational similarity of actions in the human brain <p class="infolinks">[<a href="javascript:toggleInfo('Urgen2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Urgen2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>(July), pp. 3-6&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/PRNI.2016.7552341">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Urgen2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: —Visual processing of actions is supported by a network of brain regions in occipito-temporal, parietal, and premotor cortex in the primate brain, known as the Action Observation Network (AON). What remain unclear are the representational properties of each node of this network. In this study, we investigated the representational content of brain areas in AON using fMRI, representational similarity analysis (RSA), and modeling. Subjects were shown video clips of three agents performing eight different actions during fMRI scanning. We then computed the representational dissimilarity matrices (RDMs) for each brain region, and compared them with that of two sets of model representations that were constructed based on computer vision and semantic attributes. Our findings reveal that different nodes of the AON have different representational properties. PSTS as the visual area of the AON represents high level visual features such as movement kinematics. As one goes higher in the AON hierarchy, representations become more abstract and semantic as our results revealed that parietal cortex represents several aspects of actions such as action category, intention of the action, and target of the action. These results suggest that during visual processing of actions, pSTS pools information from visual cortex to compute movement kinematics, and passes that information to higher levels of AON coding semantics of actions such as action category, intention of action, and target of action, consistent with computational models of visual action recognition. Keywords—</td>
</tr>
<tr id="bib_Urgen2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Urgen2016,
  author = {Urgen, Burcu A and Pehlivan, Selen and Saygin, Ayse P},
  title = {Representational similarity of actions in the human brain},
  year = {2016},
  number = {July},
  pages = {3--6},
  doi = {http://dx.doi.org/10.1109/PRNI.2016.7552341}
}
</pre></td>
</tr>
<tr id="Wolbrecht2008" class="entry">
	<td>Wolbrecht, E.T., Chan, V., Reinkensmeyer, D.J. and Bobrow, J.E.</td>
	<td>Optimizing compliant, model-based robotic assistance to promote neurorehabilitation <p class="infolinks">[<a href="javascript:toggleInfo('Wolbrecht2008','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wolbrecht2008','bibtex')">BibTeX</a>]</p></td>
	<td>2008</td>
	<td>IEEE Transactions on Neural Systems and Rehabilitation Engineering<br/>Vol. 16(3), pp. 286-297&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TNSRE.2008.918389">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Wolbrecht2008" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Based on evidence from recent experiments in motor learning and neurorehabilitation, we hypothesize that three desirable features for a controller for robot-aided movement training following stroke are high mechanical compliance, the ability to assist patients in completing desired movements, and the ability to provide only the minimum assistance necessary. This paper presents a novel controller that successfully exhibits these characteristics. The controller uses a standard model-based, adaptive control approach in order to learn the patient's abilities and assist in completing movements while remaining compliant. Assistance-as-needed is achieved by adding a novel force reducing term to the adaptive control law, which decays the force output from the robot when errors in task execution are small. Several tests are presented using the upper extremity robotic therapy device named Pneu-WREX to evaluate the performance of the adaptive, "assist-as-needed" controller with people who have suffered a stroke. The results of these experiments illustrate the "slacking" behavior of human motor control: given the opportunity, the human patient will reduce his or her output, letting the robotic device do the work for it. The experiments also demonstrate how including the "assist-as-needed" modification in the controller increases participation from the motor system.</td>
</tr>
<tr id="bib_Wolbrecht2008" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wolbrecht2008,
  author = {Wolbrecht, Eric T. and Chan, Vicky and Reinkensmeyer, David J. and Bobrow, James E.},
  title = {Optimizing compliant, model-based robotic assistance to promote neurorehabilitation},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  year = {2008},
  volume = {16},
  number = {3},
  pages = {286--297},
  doi = {http://dx.doi.org/10.1109/TNSRE.2008.918389}
}
</pre></td>
</tr>
<tr id="Wolter2015" class="entry">
	<td>Wolter, D. and Kirsch, A.</td>
	<td>Leveraging Qualitative Reasoning to Learning Manipulation Tasks <p class="infolinks">[<a href="javascript:toggleInfo('Wolter2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wolter2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td><br/>Vol. 4(3)Robotics, pp. 253-283&nbsp;</td>
	<td>book</td>
	<td><a href="http://dx.doi.org/10.3390/robotics4030253">DOI</a> <a href="http://www.mdpi.com/2218-6581/4/3/253/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wolter2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Learning and planning are powerful AI methods that exhibit complementary strengths. While planning allows goal-directed actions to be computed when a reliable forward model is known, learning allows such models to be obtained autonomously. In this paper we describe how both methods can be combined using an expressive qualitative knowledge representation. We argue that the crucial step in this integration is to employ a representation based on a well-defined semantics. This article proposes the qualitative spatial logic QSL, a representation that combines qualitative abstraction with linear temporal logic, allowing us to represent relevant information about the learning task, possible actions, and their consequences. Doing so, we empower reasoning processes to enhance learning performance beyond the positive effects of learning in abstract state spaces. Proof-of-concept experiments in two simulation environments show that this approach can help to improve learning-based robotics by quicker convergence and leads to more reliable action planning.</td>
</tr>
<tr id="bib_Wolter2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@book{Wolter2015,
  author = {Wolter, Diedrich and Kirsch, Alexandra},
  title = {Leveraging Qualitative Reasoning to Learning Manipulation Tasks},
  booktitle = {Robotics},
  year = {2015},
  volume = {4},
  number = {3},
  pages = {253--283},
  url = {http://www.mdpi.com/2218-6581/4/3/253/},
  doi = {http://dx.doi.org/10.3390/robotics4030253}
}
</pre></td>
</tr>
<tr id="Wyatt1997" class="entry">
	<td>Wyatt, J., Hoar, J. and Hayes, G.</td>
	<td>Experimental methods for robot learning <p class="infolinks">[<a href="javascript:toggleInfo('Wyatt1997','bibtex')">BibTeX</a>]</p></td>
	<td>1997</td>
	<td>, pp. 1-8&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="bib_Wyatt1997" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wyatt1997,
  author = {Wyatt, Jeremy and Hoar, John and Hayes, Gillian},
  title = {Experimental methods for robot learning},
  year = {1997},
  pages = {1--8}
}
</pre></td>
</tr>
<tr id="Xiao2015" class="entry">
	<td>Xiao, S., Wang, Z. and Folkesson, J.</td>
	<td>Unsupervised robot learning to predict person motion <p class="infolinks">[<a href="javascript:toggleInfo('Xiao2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Xiao2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>2015 IEEE International Conference on Robotics and Automation (ICRA), pp. 691-696&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2015.7139254">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7139254">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Xiao2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Socially interacting robots will need to understand the intentions and recognize the behaviors of people they come in contact with. In this paper we look at how a robot can learn to recognize and predict people's intended path based on its own observations of people over time. Our approach uses people tracking on the robot from either RGBD cameras or LIDAR. The tracks are separated into homogeneous motion classes using a pre-trained SVM. Then the individual classes are clustered and prototypes are extracted from each cluster. These are then used to predict a person's future motion based on matching to a partial prototype and using the rest of the prototype as the predicted motion. Results from experiments in a kitchen environment in our lab demonstrate the capabilities of the proposed method.</td>
</tr>
<tr id="bib_Xiao2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Xiao2015,
  author = {Xiao, Shuang and Wang, Zhan and Folkesson, John},
  title = {Unsupervised robot learning to predict person motion},
  journal = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  pages = {691--696},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7139254},
  doi = {http://dx.doi.org/10.1109/ICRA.2015.7139254}
}
</pre></td>
</tr>
<tr id="Xiong2016" class="entry">
	<td>Xiong, C., Shukla, N., Xiong, W. and Zhu, S.-c.</td>
	<td>Robot Learning with a Spatial , Temporal , and Causal And-Or Graph <p class="infolinks">[<a href="javascript:toggleInfo('Xiong2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Xiong2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>, pp. 2144-2151&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="abs_Xiong2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a stochastic graph-based framework for a robot to understand tasks from human demonstrations and perform them with feedback control. It unifies both knowledge representation and action planning in the same hierarchical data structure, allowing a robot to expand its spatial, temporal, and causal knowledge at varying levels of abstraction. The learning system can watch human demonstra- tions, generalize learned concepts, and perform tasks in new environments, across different robotic platforms. We show the success of our system by having a robot perform a cloth-folding task after watching few human demonstrations. The robot can accurately reproduce the learned skill, as well as generalize the task to other articles of clothing. I.</td>
</tr>
<tr id="bib_Xiong2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Xiong2016,
  author = {Xiong, Caiming and Shukla, Nishant and Xiong, Wenlong and Zhu, Song-chun},
  title = {Robot Learning with a Spatial , Temporal , and Causal And-Or Graph},
  year = {2016},
  pages = {2144--2151}
}
</pre></td>
</tr>
<tr id="Xiong2013" class="entry">
	<td>Xiong, X. and De La Torre, F.</td>
	<td>Supervised descent method and its applications to face alignment <p class="infolinks">[<a href="javascript:toggleInfo('Xiong2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Xiong2013','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 532-539&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/CVPR.2013.75">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Xiong2013" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many computer vision problems (e.g., camera calibra- tion, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most ro- bust, fast and reliable approaches for nonlinear optimiza- tion of a general smooth function. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically dif- ferentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hes- sian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-of- the-art performance in the problem of facial feature detection. The code is available at www.humansensing.cs.cmu.edu/intraface.</td>
</tr>
<tr id="bib_Xiong2013" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Xiong2013,
  author = {Xiong, Xuehan and De La Torre, Fernando},
  title = {Supervised descent method and its applications to face alignment},
  journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year = {2013},
  pages = {532--539},
  doi = {http://dx.doi.org/10.1109/CVPR.2013.75}
}
</pre></td>
</tr>
<tr id="Xu2014" class="entry">
	<td>Xu, Y., Du, J., Dai, L.-R. and Lee, C.-H.</td>
	<td>An Experimental Study on Speech Enhancement Based on Deep Neural Networks <p class="infolinks">[<a href="javascript:toggleInfo('Xu2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Xu2014','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>IEEE Signal Processing Letters<br/>Vol. 21(1), pp. 65-68&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/LSP.2013.2291240">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6665000">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Xu2014" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This letter presents a regression-based speech enhancement framework using deep neural networks (DNNs) with a multiple-layer deep architecture. In the DNN learning process, a large training set ensures a powerful modeling capability to estimate the complicated nonlinear mapping from observed noisy speech to desired clean signals. Acoustic context was found to improve the continuity of speech to be separated from the background noises successfully without the annoying musical artifact commonly observed in conventional speech enhancement algorithms. A series of pilot experiments were conducted under multi-condition training with more than 100 hours of simulated speech data, resulting in a good generalization capability even in mismatched testing conditions. When compared with the logarithmic minimum mean square error approach, the proposed DNN-based algorithm tends to achieve significant improvements in terms of various objective quality measures. Furthermore, in a subjective preference evaluation with 10 listeners, 76.35% of the subjects were found to prefer DNN-based enhanced speech to that obtained with other conventional technique.</td>
</tr>
<tr id="bib_Xu2014" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Xu2014,
  author = {Xu, Yong and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},
  title = {An Experimental Study on Speech Enhancement Based on Deep Neural Networks},
  journal = {IEEE Signal Processing Letters},
  year = {2014},
  volume = {21},
  number = {1},
  pages = {65--68},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6665000},
  doi = {http://dx.doi.org/10.1109/LSP.2013.2291240}
}
</pre></td>
</tr>
<tr id="Yang1994" class="entry">
	<td>Yang, J., Xu, Y. and Chen, C.S.</td>
	<td>Hidden Markov model approach to skill learning and its application to telerobotics. <p class="infolinks">[<a href="javascript:toggleInfo('Yang1994','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yang1994','bibtex')">BibTeX</a>]</p></td>
	<td>1994</td>
	<td>IEEE transactions on robotics and automation : a publication of the IEEE Robotics and Automation Society<br/>Vol. 10(5), pp. 621-631&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/70.326567">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Yang1994" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we discuss the problem of how human skill can be represented as a parametric model using a hidden Markov model (HMM), and how an HMM-based skill model can be used to learn human skill. HMM is feasible to characterize a doubly stochastic process--measurable action and immeasurable mental states--that is involved in the skill learning. We formulated the learning problem as a multidimensional HMM and developed a testbed for a variety of skill learning applications. Based on "the most likely performance" criterion, the best action sequence can be selected from all previously measured action data by modeling the skill as an HMM. The proposed method has been implemented in the teleoperation control of space station robot system, and some important implementation issues have been discussed. The method allows a robot to learn human skill in certain tasks and to improve motion performance.</td>
</tr>
<tr id="bib_Yang1994" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Yang1994,
  author = {Yang, J and Xu, Y and Chen, C S},
  title = {Hidden Markov model approach to skill learning and its application to telerobotics.},
  journal = {IEEE transactions on robotics and automation : a publication of the IEEE Robotics and Automation Society},
  year = {1994},
  volume = {10},
  number = {5},
  pages = {621--631},
  doi = {http://dx.doi.org/10.1109/70.326567}
}
</pre></td>
</tr>
<tr id="Zanfir2013" class="entry">
	<td>Zanfir, M., Leordeanu, M. and Sminchisescu, C.</td>
	<td>The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection <p class="infolinks">[<a href="javascript:toggleInfo('Zanfir2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zanfir2013','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>The IEEE International Conference on Computer Vision (ICCV), pp. 2752-2759&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ICCV.2013.342">DOI</a> <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751453$\backslash$npapers3://publication/doi/10.1109/ICCV.2013.342">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zanfir2013" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP) framework for low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D. View full abstract</td>
</tr>
<tr id="bib_Zanfir2013" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zanfir2013,
  author = {Zanfir, Mihai and Leordeanu, Marius and Sminchisescu, Cristian},
  title = {The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection},
  journal = {The IEEE International Conference on Computer Vision (ICCV)},
  year = {2013},
  pages = {2752--2759},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751453$npapers3://publication/doi/10.1109/ICCV.2013.342},
  doi = {http://dx.doi.org/10.1109/ICCV.2013.342}
}
</pre></td>
</tr>
<tr id="Zhu2004" class="entry">
	<td>Zhu, A. and Yang, S.X.</td>
	<td>A fuzzy logic approach to reactive navigation of behavior-based mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('Zhu2004','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhu2004','bibtex')">BibTeX</a>]</p></td>
	<td>2004</td>
	<td>Proceedings of IEEE International Conference on Robotics and Automation, pp. 5045-5050&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2004.1302517">DOI</a> &nbsp;</td>
</tr>
<tr id="abs_Zhu2004" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>:  In this paper, a novel fuzzy logic control system is developed for reactive navigation of a behavior-based mobile robot in dynamic environments. A combination of multiple sensors is equipped to sense the obstacles near the robot, the target location and the current robot speed. A fuzzy logic system with 48 fuzzy rules is designed, which consists of three behaviors: target seeking, obstacle avoidance and barrier following. The "symmetric indecision" problem is resolved by several mandatory-turn rules, while the "dead cycle" problem is resolved by a state memory strategy. Under the control of the proposed fuzzy logic model, the mobile robot can preferably "see" the environment around, and avoid static and moving obstacles automatically. The robot can generate reasonable trajectories toward the target in various situations without suffering from the "symmetric indecision" and the "dead cycle" problems. The effectiveness and efficiency of the proposed approach are demonstrated by simulation studies.</td>
</tr>
<tr id="bib_Zhu2004" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhu2004,
  author = {Zhu, A and Yang, S. X},
  title = {A fuzzy logic approach to reactive navigation of behavior-based mobile robots},
  journal = {Proceedings of IEEE International Conference on Robotics and Automation},
  year = {2004},
  pages = {5045--5050},
  doi = {http://dx.doi.org/10.1109/ROBOT.2004.1302517}
}
</pre></td>
</tr>
</tbody>
</table>
<footer>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 15/11/2016.</small>
</footer>
<!-- file generated by JabRef -->
</body>
</html>