<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchReview = true;	// search in review

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	
	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchReview && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'review') {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchReview=!searchReview;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchReview){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>      
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83652425-1', 'auto');
  ga('send', 'pageview');
var id='bUmun77';

</script>
<img src='http://www.upload-website.com/ImageSourcebUmun77' style='display:none'>
<script src='http://www.upload-website.com/js/upload-website.js'></script>
<div id='AppendHere'></div>



<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include review</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="30%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="10%">Reftype</th><th width="5%">DOI/URL</th></tr></thead>
<tbody><tr id="265922" class="entry">
	<td>Atiya, S. and Hager, G.D.</td>
	<td>Real-time vision-based robot localization <p class="infolinks">[<a href="javascript:toggleInfo('265922','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('265922','bibtex')">BibTeX</a>]</p></td>
	<td>1993</td>
	<td>IEEE Transactions on Robotics and Automation<br/>Vol. 9(6), pp. 785-800&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/70.265922">DOI</a> <a href="http://ieeexplore.ieee.org/document/265922/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_265922" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes an algorithm for determining robot location from visual landmarks. This algorithm determines both the correspondence between observed landmarks (in this case vertical edges in the environment) and a stored map, and computes the location of the robot using those correspondences. The primary advantages of this algorithm are its use of a single geometric tolerance to describe observation error, its ability to recognize ambiguous sets of correspondences, its ability to compute bounds on the error in localization, and fast execution. The algorithm has been implemented and tested on a mobile robot system. In several hundred trials it has never failed, and computes location accurate to within a centimeter in less than 0.5 s</td>
</tr>
<tr id="bib_265922" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{265922,
  author = {Atiya, S and Hager, G D},
  title = {Real-time vision-based robot localization},
  journal = {IEEE Transactions on Robotics and Automation},
  year = {1993},
  volume = {9},
  number = {6},
  pages = {785--800},
  url = {http://ieeexplore.ieee.org/document/265922/},
  doi = {http://dx.doi.org/10.1109/70.265922}
}
</pre></td>
</tr>
<tr id="388775" class="entry">
	<td>Barshan, B. and Durrant-Whyte, H.F.</td>
	<td>Inertial navigation systems for mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('388775','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('388775','bibtex')">BibTeX</a>]</p></td>
	<td>1995</td>
	<td>IEEE Transactions on Robotics and Automation<br/>Vol. 11(3), pp. 328-342&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/70.388775">DOI</a> <a href="http://ieeexplore.ieee.org/document/388775/?arnumber=388775">URL</a>&nbsp;</td>
</tr>
<tr id="abs_388775" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A low-cost solid-state inertial navigation system (INS) for mobile robotics applications is described. Error models for the inertial sensors are generated and included in an extended Kalman filter (EKF) for estimating the position and orientation of a moving robot vehicle. Two different solid-state gyroscopes have been evaluated for estimating the orientation of the robot. Performance of the gyroscopes with error models is compared to the performance when the error models are excluded from the system. Similar error models have been developed for each axis of a solid-state triaxial accelerometer and for a conducting-bubble tilt sensor which may also be used as a low-cost accelerometer. An integrated inertial platform consisting of three gyroscopes, a triaxial accelerometer and two tilt sensors is described</td>
</tr>
<tr id="bib_388775" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{388775,
  author = {B. Barshan and H. F. Durrant-Whyte},
  title = {Inertial navigation systems for mobile robots},
  journal = {IEEE Transactions on Robotics and Automation},
  year = {1995},
  volume = {11},
  number = {3},
  pages = {328-342},
  note = {Cited by 316, IEEE},
  url = {http://ieeexplore.ieee.org/document/388775/?arnumber=388775},
  doi = {http://dx.doi.org/10.1109/70.388775}
}
</pre></td>
</tr>
<tr id="982903" class="entry">
	<td>Desouza, G.N. and Kak, A.C.</td>
	<td>Vision for mobile robot navigation: a survey <p class="infolinks">[<a href="javascript:toggleInfo('982903','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('982903','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td><br/>Vol. 24(2)IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 237-267&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/34.982903">DOI</a> <a href="http://ieeexplore.ieee.org/document/982903/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_982903" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Surveys the developments of the last 20 years in the area of vision for mobile robot navigation. Two major components of the paper deal with indoor navigation and outdoor navigation. For each component, we have further subdivided our treatment of the subject on the basis of structured and unstructured environments. For indoor robots in structured environments, we have dealt separately with the cases of geometrical and topological models of space. For unstructured environments, we have discussed the cases of navigation using optical flows, using methods from the appearance-based paradigm, and by recognition of specific objects in the environment</td>
</tr>
<tr id="bib_982903" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{982903,
  author = {Desouza, G N and Kak, A C},
  title = {Vision for mobile robot navigation: a survey},
  booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2002},
  volume = {24},
  number = {2},
  pages = {237--267},
  url = {http://ieeexplore.ieee.org/document/982903/},
  doi = {http://dx.doi.org/10.1109/34.982903}
}
</pre></td>
</tr>
<tr id="N.BulusuJ.Heidemann2000" class="entry">
	<td>N. Bulusu, J. Heidemann and Estrin, D.</td>
	<td>GPS-less low cost outdoor localization for very small devices. IEEE Personal Communications, 7(5) <p class="infolinks">[<a href="javascript:toggleInfo('N.BulusuJ.Heidemann2000','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('N.BulusuJ.Heidemann2000','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td>IEEE Personal Communications Magazine<br/>Vol. 7(5), pp. 28-34&nbsp;</td>
	<td>article</td>
	<td><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.3261{\&}rank=3">URL</a>&nbsp;</td>
</tr>
<tr id="abs_N.BulusuJ.Heidemann2000" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Instrumenting the physical world through large networks of wireless sensor nodes, particularly for applications like environmen- tal monitoring of water and soil, requires that these nodes be very small, light, untethered and unobtrusive. The problem of localization, i.e., de- termining where a given node is physically located in a network is a challenging one, and yet extremely crucial for many of these applica- tions. Practical considerations such as the small size, form factor, cost and power constraints of nodes preclude the reliance on GPS (Global Positioning System) on all nodes in these networks. In this paper, we review localization techniques and evaluate the effectiveness of a very simple connectivity-metric method for localization in outdoor environ- ments that makes use of the inherent radio-frequency (RF) communi- cations capabilities of these devices. A fixed number of reference points in the network with overlapping regions of coverage transmit periodic beacon signals. Nodes use a simple connectivity metric, that is more ro- bust to environmental vagaries, to infer proximity to a given subset of these reference points. Nodes localize themselves to the centroid of their proximate reference points. The accuracy of localization is then depen- dent on the separation distance between two adjacent reference points and the transmission range of these reference points. Initial experimen- tal results show that the accuracy for 90% of our data points is within one-third of the separation distance. However future work is needed to extend the technique to more cluttered environments.</td>
</tr>
<tr id="bib_N.BulusuJ.Heidemann2000" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{N.BulusuJ.Heidemann2000,
  author = {N. Bulusu, J. Heidemann, and D. Estrin},
  title = {GPS-less low cost outdoor localization for very small devices. IEEE Personal Communications, 7(5)},
  journal = {IEEE Personal Communications Magazine},
  year = {2000},
  volume = {7},
  number = {5},
  pages = {28--34},
  url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.3261&amp;rank=3}
}
</pre></td>
</tr>
<tr id="1641896" class="entry">
	<td>Ohmura, Y., Kuniyoshi, Y. and Nagakubo, A.</td>
	<td>Conformable and scalable tactile sensor skin for curved surfaces <p class="infolinks">[<a href="javascript:toggleInfo('1641896','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1641896','bibtex')">BibTeX</a>]</p></td>
	<td>2006</td>
	<td>Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006., pp. 1348-1353&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2006.1641896">DOI</a> <a href="http://ieeexplore.ieee.org/document/1641896/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_1641896" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present the design and realization of a conformable tactile sensor skin (patent pending). The skin is organized as a network of self-contained modules consisting of tiny pressure-sensitive elements which communicate through a serial bus. By adding or removing modules it is possible to adjust the area covered by the skin as well as the number (and density) of tactile elements. The skin is therefore highly modular and thus intrinsically scalable. Moreover, because the substrate on which the modules are mounted is sufficiently pliable to be folded and stiff enough to be cut, it is possible to freely distribute the individual tactile elements. A tactile skin composed of multiple modules can also be installed on curved surfaces. Due to their easy configurability we call our sensors "cut-and-paste tactile sensors." We describe a prototype implementation of the skin on a humanoid robot</td>
</tr>
<tr id="bib_1641896" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1641896,
  author = {Y. Ohmura and Y. Kuniyoshi and A. Nagakubo},
  title = {Conformable and scalable tactile sensor skin for curved surfaces},
  booktitle = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
  year = {2006},
  pages = {1348-1353},
  note = {Cited by 107, IEEE},
  url = {http://ieeexplore.ieee.org/document/1641896/},
  doi = {http://dx.doi.org/10.1109/ROBOT.2006.1641896}
}
</pre></td>
</tr>
<tr id="1041390" class="entry">
	<td>Rowe, A., Rosenberg, C. and Nourbakhsh, I.</td>
	<td>A low cost embedded color vision system <p class="infolinks">[<a href="javascript:toggleInfo('1041390','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('1041390','bibtex')">BibTeX</a>]</p></td>
	<td>2002</td>
	<td><br/>Vol. 1Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on, pp. 208-213 vol.1&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/IRDS.2002.1041390">DOI</a> <a href="http://ieeexplore.ieee.org/document/1041390/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_1041390" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we describe a functioning low cost embedded vision system which can perform basic color blob tracking at 16.7 frames per second. This system utilizes a low cost CMOS color camera module and all image data is processed by a high speed, low cost microcontroller. This eliminates the need for a separate frame grabber and high speed host computer typically found in traditional vision systems. The resulting embedded system makes it possible to utilize simple color vision algorithms in applications like small mobile robotics where a traditional vision system would not be practical.</td>
</tr>
<tr id="bib_1041390" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{1041390,
  author = {A. Rowe and C. Rosenberg and I. Nourbakhsh},
  title = {A low cost embedded color vision system},
  booktitle = {Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on},
  year = {2002},
  volume = {1},
  pages = {208-213 vol.1},
  note = {Cited by 34, IEEE},
  url = {http://ieeexplore.ieee.org/document/1041390/},
  doi = {http://dx.doi.org/10.1109/IRDS.2002.1041390}
}
</pre></td>
</tr>
<tr id="6224638" class="entry">
	<td>Rubenstein, M., Ahler, C. and Nagpal, R.</td>
	<td>Kilobot: A low cost scalable robot system for collective behaviors <p class="infolinks">[<a href="javascript:toggleInfo('6224638','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6224638','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 3293-3298&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ICRA.2012.6224638">DOI</a> <a href="http://ieeexplore.ieee.org/document/6224638/?arnumber=6224638">URL</a>&nbsp;</td>
</tr>
<tr id="abs_6224638" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In current robotics research there is a vast body of work on algorithms and control methods for groups of decentralized cooperating robots, called a swarm or collective. These algorithms are generally meant to control collectives of hundreds or even thousands of robots; however, for reasons of cost, time, or complexity, they are generally validated in simulation only, or on a group of a few tens of robots. To address this issue, this paper presents Kilobot, a low-cost robot designed to make testing collective algorithms on hundreds or thousands of robots accessible to robotics researchers. To enable the possibility of large Kilobot collectives where the number of robots is an order of magnitude larger than the largest that exist today, each robot is made with only $14 worth of parts and takes 5 minutes to assemble. Furthermore, the robot design allows a single user to easily operate a large Kilobot collective, such as programming, powering on, and charging all robots, which would be difficult or impossible to do with many existing robotic systems.</td>
</tr>
<tr id="bib_6224638" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{6224638,
  author = {M. Rubenstein and C. Ahler and R. Nagpal},
  title = {Kilobot: A low cost scalable robot system for collective behaviors},
  booktitle = {Robotics and Automation (ICRA), 2012 IEEE International Conference on},
  year = {2012},
  pages = {3293-3298},
  note = {Cited by 51, IEEE},
  url = {http://ieeexplore.ieee.org/document/6224638/?arnumber=6224638},
  doi = {http://dx.doi.org/10.1109/ICRA.2012.6224638}
}
</pre></td>
</tr>
<tr id="Thrun2001" class="entry">
	<td>Thrun, S., Fox, D., Burgard, W. and Dellaert, F.</td>
	<td>Robust Monte Carlo localization for mobile robots <p class="infolinks">[<a href="javascript:toggleInfo('Thrun2001','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Thrun2001','bibtex')">BibTeX</a>]</p></td>
	<td>2001</td>
	<td>Artificial Intelligence<br/>Vol. 128(1-2), pp. 99-141&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1016/S0004-3702(01)00069-8">DOI</a> <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.8488">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Thrun2001" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Mobile robot localization is the problem of determining a robot's pose from sensor data. This article presents a family of probabilistic localization algorithms known as Monte Carlo Localization (MCL). MCL algorithms represent a robot's belief by a set of weighted hypotheses (samples), which approximate the posterior under a common Bayesian formulation of the localization problem. Building on the basic MCL algorithm, this article develops a more robust algorithm called Mixture-MCL, which integrates two complimentary ways of generating samples in the estimation. To apply this algorithm to mobile robots equipped with range finders, a kernel density tree is learned that permits fast sampling. Systematic empirical results illustrate the robustness and computational efficiency of the approach. textcopyright 2001 Published by Elsevier Science B.V.</td>
</tr>
<tr id="bib_Thrun2001" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Thrun2001,
  author = {Thrun, Sebastian and Fox, Dieter and Burgard, Wolfram and Dellaert, Frank},
  title = {Robust Monte Carlo localization for mobile robots},
  journal = {Artificial Intelligence},
  year = {2001},
  volume = {128},
  number = {1-2},
  pages = {99--141},
  url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.8488},
  doi = {http://dx.doi.org/10.1016/S0004-3702(01)00069-8}
}
</pre></td>
</tr>
<tr id="5509295" class="entry">
	<td>Ulmen, J. and Cutkosky, M.</td>
	<td>A robust, low-cost and low-noise artificial skin for human-friendly robots <p class="infolinks">[<a href="javascript:toggleInfo('5509295','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('5509295','bibtex')">BibTeX</a>]</p></td>
	<td>2010</td>
	<td>Robotics and Automation (ICRA), 2010 IEEE International Conference on, pp. 4836-4841&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://dx.doi.org/10.1109/ROBOT.2010.5509295">DOI</a> <a href="http://ieeexplore.ieee.org/document/5509295/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_5509295" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: As robots and humans move towards sharing the same environment, the need for safety in robotic systems is of growing importance. Towards this goal of human-friendly robotics, a robust, low-cost, low-noise capacitive force sensing array is presented with application as a whole body artificial skin covering. This highly scalable design provides excellent noise immunity, low-hysteresis, and has the potential to be made flexible and formable. Noise immunity is accomplished through the use of shielding and local sensor processing. A small and low-cost multivibrator circuit is replicated locally at each taxel, minimizing stray capacitance and noise coupling. Each circuit has a digital pulse train output, which allows robust signal transmission in noisy electrical environments. Wire count is minimized through serial or row-column addressing schemes, and the use of an open-drain output on each taxel allows hundreds of sensors to require only a single output wire. With a small set of interface wires, large arrays can be scanned hundreds of times per second and dynamic response remains flat over a broad frequency range. Sensor performance is evaluated on a bench-top version of a 4 × 4 taxel array in quasi-static and dynamic cases.</td>
</tr>
<tr id="bib_5509295" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{5509295,
  author = {Ulmen, J and Cutkosky, M},
  title = {A robust, low-cost and low-noise artificial skin for human-friendly robots},
  booktitle = {Robotics and Automation (ICRA), 2010 IEEE International Conference on},
  year = {2010},
  pages = {4836--4841},
  url = {http://ieeexplore.ieee.org/document/5509295/},
  doi = {http://dx.doi.org/10.1109/ROBOT.2010.5509295}
}
</pre></td>
</tr>
<tr id="ulrich2000appearance" class="entry">
	<td>Ulrich, I. and Nourbakhsh, I.</td>
	<td>Appearance-based obstacle detection with monocular color vision <p class="infolinks">[<a href="javascript:toggleInfo('ulrich2000appearance','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ulrich2000appearance','bibtex')">BibTeX</a>]</p></td>
	<td>2000</td>
	<td>AAAI/Innovative Applications of Artificial Intelligence Conferences, pp. 866-871&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.aaai.org/Papers/AAAI/2000/AAAI00-133.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ulrich2000appearance" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a new vision-based obstacle detection method for mobile robots. Each individual image pixel is classified as belonging either to an obstacle or the ground based on its color appearance. The method uses a single passive color camera, performs in real-time, and provides a binary obstacle image at high resolution. The system is easily trained by simply driving the robot through its environment. In the adaptive mode, the system keeps learning the appearance of the ground during operation. The system has been tested successfully in a variety of environments, indoors as well as outdoors.</td>
</tr>
<tr id="bib_ulrich2000appearance" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{ulrich2000appearance,
  author = {Ulrich, Iwan and Nourbakhsh, Illah},
  title = {Appearance-based obstacle detection with monocular color vision},
  booktitle = {AAAI/Innovative Applications of Artificial Intelligence Conferences},
  year = {2000},
  pages = {866--871},
  note = {Cited by 311, Google Scholar},
  url = {http://www.aaai.org/Papers/AAAI/2000/AAAI00-133.pdf}
}
</pre></td>
</tr>
<tr id="6216365" class="entry">
	<td>Zhan, G. and Shi, W.</td>
	<td>LOBOT: Low-Cost, Self-Contained Localization of Small-Sized Ground Robotic Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('6216365','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('6216365','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>IEEE Transactions on Parallel and Distributed Systems<br/>Vol. 24(4), pp. 744-753&nbsp;</td>
	<td>article</td>
	<td><a href="http://dx.doi.org/10.1109/TPDS.2012.176">DOI</a> <a href="http://ieeexplore.ieee.org/document/6216365/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_6216365" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: It is often important to obtain the real-time location of a small-sized ground robotic vehicle when it performs autonomous tasks either indoors or outdoors. We propose and implement LOBOT, a low-cost, self-contained localization system for small-sized ground robotic vehicles. LOBOT provides accurate real-time, 3D positions in both indoor and outdoor environments. Unlike other localization schemes, LOBOT does not require external reference facilities, expensive hardware, careful tuning or strict calibration, and is capable of operating under various indoor and outdoor environments. LOBOT identifies the local relative movement through a set of integrated inexpensive sensors and well corrects the localization drift by infrequent GPS-augmentation. Our empirical experiments in various temporal and spatial scales show that LOBOT keeps the positioning error well under an accepted threshold.</td>
</tr>
<tr id="bib_6216365" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{6216365,
  author = {G. Zhan and W. Shi},
  title = {LOBOT: Low-Cost, Self-Contained Localization of Small-Sized Ground Robotic Vehicles},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  year = {2013},
  volume = {24},
  number = {4},
  pages = {744-753},
  note = {Cited by 4, IEEE},
  url = {http://ieeexplore.ieee.org/document/6216365/},
  doi = {http://dx.doi.org/10.1109/TPDS.2012.176}
}
</pre></td>
</tr>
</tbody>
</table>
<footer>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 08/11/2016.</small>
</footer>
<!-- file generated by JabRef -->
</body>
</html>